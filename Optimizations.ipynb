{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RIx_kSKwEMeD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Gradient_descent:\n",
        "  def __init__(self,param,lr=0.001):\n",
        "    self.lr = lr\n",
        "    self.param = list(param)\n",
        "\n",
        "  def zero_grad(self):\n",
        "    for par in self.param:\n",
        "      if par.grad is not None:\n",
        "        par.grad.zero_()\n",
        "\n",
        "  def step(self):\n",
        "    with torch.no_grad():\n",
        "      for par in self.param:\n",
        "        if par.grad is not None:\n",
        "          par -= self.lr*par.grad\n",
        "\n"
      ],
      "metadata": {
        "id": "RYrCp-9pEQ_m"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MomentumGradientDescent:\n",
        "  def __init__(self,param,lr=0.01,momentum=0.9):\n",
        "    self.lr = lr\n",
        "    self.param = list(param)\n",
        "    self.velocities = [torch.zeros_like(p) for p in self.param]\n",
        "    self.momentum = momentum\n",
        "  def zero_grad(self):\n",
        "    for par in self.param:\n",
        "      if par.grad is not None:\n",
        "        par.grad.zero_()\n",
        "  def step(self):\n",
        "    with torch.no_grad():\n",
        "      for i,par in enumerate(self.param):\n",
        "        if par.grad is not None:\n",
        "          self.velocities[i] = self.momentum*self.velocities[i] + par.grad\n",
        "          par -= self.lr * self.velocities[i]\n"
      ],
      "metadata": {
        "id": "9ci8AJ6NMg7I"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nesterov Accelerated Gradient->\n",
        "   ~ updates the velocity based on where this (current) gradient is about to take us\n",
        "   ~ steps :\n",
        "          look_ahead(Θ`) = Θt - B*V(t-1)\n",
        "          Gradient_at_Θ` (gt) = L(Θ`)\n",
        "          velocity = B*V(t-1) + n*gt\n",
        "          Θt+1 = Θt - velocity\n"
      ],
      "metadata": {
        "id": "ecLlZkgHPL90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NAGD:\n",
        "  def __init__(self,param,lr=0.001,momentum=0.9):\n",
        "    self.lr = lr\n",
        "    self.param = list(param)\n",
        "    self.momentum = momentum\n",
        "    self.velocities = [torch.zeros_like(p) for p in self.param]\n",
        "  def zero_grad(self):\n",
        "    for par in self.param:\n",
        "      if par.grad is not None:\n",
        "        par.grad.zero_()\n",
        "  def step(self):\n",
        "    with torch.no_grad():\n",
        "      for i,par in enumerate(self.param):\n",
        "        if par.grad is not None:\n",
        "          v_prev =self.velocities[i]\n",
        "          self.velocities[i] = self.momentum*self.velocities[i] + self.lr*par.grad\n",
        "\n",
        "          par -= self.momentum*v_prev + self.velocities[i]"
      ],
      "metadata": {
        "id": "ShxmM7ieNyMP"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We dont take Gradient at Look ahead because its computationally expensive to compute the forward pass again with the look_ahead term ,"
      ],
      "metadata": {
        "id": "q7Vrme7dVrRj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adaptive Learning Rate Methods\n",
        "\n",
        "\n",
        "1.   accumulates sum of square Gradients\n",
        "2.   Uses this to control learning rate , e.g.;  if Gradients grow faster , the effective learning rate becomes smaller (learning_rate/sum_squared_gradeints) , and if gradient are too small , then effective lr is high.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "221nI47AXtYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdaGrad:\n",
        "  def __init__(self,param,lr=0.01,eps=1e-6):\n",
        "    self.lr = lr\n",
        "    self.param = list(param)\n",
        "    self.eps = eps\n",
        "    self.G = [torch.zeros_like(p) for p in self.param]\n",
        "\n",
        "  def zero_grad(self):\n",
        "    for par in self.param:\n",
        "      if par.grad is not None:\n",
        "        par.grad.zero_()\n",
        "\n",
        "  def step(self):\n",
        "    with torch.no_grad():\n",
        "      for i,par in enumerate(self.param):\n",
        "        self.G[i] += par.grad**2\n",
        "        adaptive_learning = self.lr/(torch.sqrt(self.G[i]+self.eps))\n",
        "        par -= par.grad*adaptive_learning\n"
      ],
      "metadata": {
        "id": "Kpvi5oE8V4Cd"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RMSProp (Root Mean Squared Propogation)\n",
        "\n",
        "\n",
        "1.   Hadles the aggressive decay of params like in adagrad by taking exponential moving average of squared gradeints\n",
        "2.   EG = EG*b + (1-b)*(G**2)\n",
        "     effective_lr = lr/sqrt(EG) + eps\n",
        "     grad -= effective_lr*grad(i-1)\n",
        "\n"
      ],
      "metadata": {
        "id": "OdEKpy7basi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSProp:\n",
        "  def __init__(self,param,lr=0.001,beta=0.9,eps=1e-8):\n",
        "    self.param = list(param)\n",
        "    self.lr = lr\n",
        "    self.b = beta\n",
        "    self.eps = eps\n",
        "    self.G = [torch.zeros_like(p) for p in self.param]\n",
        "\n",
        "  def zero_grad(self):\n",
        "    for par in self.param:\n",
        "      if par.grad is not None:\n",
        "        par.grad.zero_()\n",
        "\n",
        "  def step(self):\n",
        "    with torch.no_grad():\n",
        "      for i,par in enumerate(self.param):\n",
        "        if par.grad is not None:\n",
        "          self.G[i] = self.G[i]*self.b + (1-self.b)*(par.grad**2)\n",
        "          adaptive_learning = self.lr/(torch.sqrt(self.G[i])+self.eps)\n",
        "          par -= adaptive_learning*par.grad\n"
      ],
      "metadata": {
        "id": "Q7VFB0V8Xsom"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AdaDelta Combines AdaGRad and RMSProp , with generating an effective learning rate through the ratio of exponential moving average of squared params to  squared gradeints\n",
        "\n",
        "\n",
        "1.   EG = rho*EG + (1-rho)*G^2\n",
        "2.   rms_param = sqrt(EP+eps)\n",
        "3.   rms_grad = sqrt(EG+eps)\n",
        "4.   lr = rms_param/rms_grad\n",
        "5.   delta = -lr*param.grad\n",
        "6.   param += delta\n",
        "EG - exponential moving average of gradeitns, EP - exponential moving average of params\n",
        "\n"
      ],
      "metadata": {
        "id": "_eWUA1ODhAJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdaDelta:\n",
        "  def __init__(self,param,rho=0.9,eps=1e-8):\n",
        "    self.rho = rho\n",
        "    self.param = list(param)\n",
        "    self.eps = eps\n",
        "    self.param_av = [torch.zeros_like(p) for p in self.param]\n",
        "    self.gradient_av = [torch.zeros_like(p) for p in self.param]\n",
        "\n",
        "  def zero_grad(self):\n",
        "    for par in self.param:\n",
        "      if par.grad is not None:\n",
        "        par.grad.zero_()\n",
        "\n",
        "  def step(self):\n",
        "    with torch.no_grad():\n",
        "      for i,par in enumerate(self.param):\n",
        "        if par.grad is not None:\n",
        "          self.gradient_av[i] = self.gradient_av[i]*self.rho + (1-self.rho)*(par.grad**2)\n",
        "          rms_par = torch.sqrt(self.param_av[i]+self.eps)\n",
        "          rms_grad= torch.sqrt(self.gradient_av[i]+self.eps)\n",
        "          effective_lr = rms_par/rms_grad\n",
        "          delta = -effective_lr*par.grad\n",
        "          self.param_av[i] = self.rho*self.param_av[i] + (1-self.rho)*(delta**2)\n",
        "          par += delta\n"
      ],
      "metadata": {
        "id": "1LiPb0MlZpwr"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adam\n",
        "adam combines the adaptive learning rate technique from RMS prop and Fast cponvergences of Moment based gradeints\n",
        "\n",
        "\n",
        "1.   m = b1*m + (1-b1)*grad\n",
        "2.   v = b2*v + (1-b2)*(grad**2)\n",
        "3.   m_hat = m/(1- b1^t)\n",
        "4.   v_hat = v/(1- b2^t)\n",
        "5.   lr = lr/sqrt(v_hat) + eps // effetive learning rate via RMS prop\n",
        "6.   par -= lr*m_hat\n"
      ],
      "metadata": {
        "id": "sVEErYaGnt_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Adam:\n",
        "  def __init__(self,param,lr=0.001,b1=0.9,b2=0.999,eps=1e-8):\n",
        "    self.param = list(param)\n",
        "    self.lr = lr\n",
        "    self.b1 = b1\n",
        "    self.b2 = b2\n",
        "    self.eps = eps\n",
        "    self.m = [torch.zeros_like(p) for p in self.param]\n",
        "    self.v = [torch.zeros_like(p) for p in self.param]\n",
        "    self.t = 0\n",
        "\n",
        "  def zero_grad(self):\n",
        "    for par in self.param:\n",
        "      if par.grad is not None:\n",
        "        par.grad.zero_()\n",
        "\n",
        "  def step(self):\n",
        "    self.t += 1\n",
        "    with torch.no_grad():\n",
        "      for i,par in enumerate(self.param):\n",
        "        if par.grad is not None:\n",
        "          self.m[i] = self.m[i]*self.b1 + (1-self.b1)*par.grad\n",
        "          self.v[i] = self.v[i]*self.b2 + (1-self.b2)*(par.grad**2)\n",
        "\n",
        "          m_c = self.m[i]/(1 - self.b1**self.t)\n",
        "          v_c = self.v[i]/(1-self.b2**self.t)\n",
        "          effective_lr = self.lr/(torch.sqrt(v_c)+self.eps)\n",
        "          par -= effective_lr*m_c\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5eExW6Log-1w"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdamW:\n",
        "  def __init__(self,param,lr=0.001,b1=0.9,b2=0.999,eps=1e-8,weight_decay=0.01):\n",
        "    self.param = param\n",
        "    self.lr = lr\n",
        "    self.b1 = b1\n",
        "    self.b2 = b2\n",
        "    self.eps = eps\n",
        "    self.m = [torch.zeros_like(p) for p in self.param]\n",
        "    self.v = [torch.zeros_like(p) for p in self.param]\n",
        "    self.t = 0\n",
        "    self.weight_decay = weight_decay\n",
        "\n",
        "  def zero_grad(self):\n",
        "    for par in self.param:\n",
        "      if par.grad is not None:\n",
        "        par.grad.zero_()\n",
        "\n",
        "  def step(self):\n",
        "    with torch.no_grad():\n",
        "      for i,par in enumerate(self.param):\n",
        "        self.m[i] = self.m[i]*self.b1 + (1-self.b1)*par.grad\n",
        "        self.v[i] = self.v[i]*self.b2 + (1-self.b2)*(par.grad**2)\n",
        "\n",
        "        m_c = self.m[i]/(1 - self.b1**self.t)\n",
        "        v_c = self.v[i]/(1-self.b2**self.t)\n",
        "        updative_update = m_c/(torch.sqrt(v_c)+self.eps)\n",
        "        par -= self.lr*(updative_update + self.weight_decay*par)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6r53rkzpnJFN"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_circles"
      ],
      "metadata": {
        "id": "7q6-SN-Cpsaq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X,y = make_circles()"
      ],
      "metadata": {
        "id": "0iF6iUBC-B8u"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmwJcBl5-EeO",
        "outputId": "69802fff-dbde-4888-ddbb-b392515dd70a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SF5DRBt-M5I",
        "outputId": "3038da86-c159-4b0f-ca2b-085d76cf981e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P92HC3PV-O2h",
        "outputId": "e1ba9b18-1c0f-4414-eec1-3afc151ada46"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
              "       1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
              "       1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n",
              "       0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,\n",
              "       0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test = X[:70,],X[70:,]\n",
        "Y_train,Y_test = y[:70],y[70:]"
      ],
      "metadata": {
        "id": "n_wLp0QF-P_l"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50Xx1ZcY-cyI",
        "outputId": "bcfffecb-24de-48a0-b8ea-dc5bc5ac16f8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(70, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = torch.tensor(X_train,dtype=torch.float)\n",
        "X_test = torch.tensor(X_test,dtype=torch.float)\n",
        "Y_train = torch.tensor(Y_train,dtype=torch.long)\n",
        "Y_test = torch.tensor(Y_test,dtype=torch.long)"
      ],
      "metadata": {
        "id": "bQoRmKro-feL"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(2,64),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm1d(64),\n",
        "        nn.Linear(64,32),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm1d(32),\n",
        "        nn.Linear(32,16),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm1d(16),\n",
        "        nn.Linear(16,2)\n",
        "\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    return self.layers(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "S41JyLGo-yen"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(opt,X_train,Y_train,X_test,Y_test,model):\n",
        "  loss_func = nn.CrossEntropyLoss()\n",
        "  epoch = 0\n",
        "  training_loss = []\n",
        "  test_loss = []\n",
        "  training_acc = []\n",
        "  test_acc =[]\n",
        "  while epoch <=100:\n",
        "    model.train()\n",
        "\n",
        "    outs  = model(X_train)\n",
        "    los = loss_func(outs,Y_train)\n",
        "    training_loss.append(los.item())\n",
        "    opt.zero_grad()\n",
        "    los.backward()\n",
        "    opt.step()\n",
        "    _,preds = torch.max(outs,1)\n",
        "    acc = (preds == Y_train).sum().item()/X_train.shape[0]\n",
        "    training_acc.append(acc)\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      outs = model(X_test)\n",
        "      _,preds = torch.max(outs,1)\n",
        "      los = loss_func(outs,Y_test)\n",
        "      test_loss.append(los.item())\n",
        "      ac2 = (preds == Y_test).sum().item()/X_test.shape[0]\n",
        "      test_acc.append(ac2)\n",
        "      epoch += 1\n",
        "  for i in range(1,6):\n",
        "\n",
        "    print(f'training loss at epoch {i*20} is {training_loss[i*20]} and accuracy is {training_acc[i*20]}',end=' ')\n",
        "    print(f' || testing loss at epoch {i*20} is {test_loss[i*20]} and accuracy is {test_acc[i*20]}')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hVkTC4K3_bJt"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NN()"
      ],
      "metadata": {
        "id": "OrADkXnCGtgh"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_optimizers = {\n",
        "    'Vanilla GD': Gradient_descent(model.parameters(),lr=0.01),\n",
        "    'Momentum': MomentumGradientDescent(model.parameters(),lr=0.01,momentum=0.9),\n",
        "    'Adam': Adam(model.parameters(),lr=0.03),\n",
        "    'AdamW': AdamW(model.parameters(),lr=0.03),\n",
        "    'RMSProp': RMSProp(model.parameters()),\n",
        "}"
      ],
      "metadata": {
        "id": "zLotEbBtF47A"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for d in list(nn_optimizers.keys()):\n",
        "  print(f' Results for optimizer {d}')\n",
        "  train_loop(nn_optimizers[d],X_train,Y_train,X_test,Y_test,model)\n",
        "  print('*********************************************************************************************************************************')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpkcB79BGwLY",
        "outputId": "339db2ff-007a-4aea-932b-27e1bfd271f0"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Results for optimizer Vanilla GD\n",
            "training loss at epoch 20 is 8.327617138093046e-07 and accuracy is 1.0  || testing loss at epoch 20 is 4.386856289784191e-06 and accuracy is 1.0\n",
            "training loss at epoch 40 is 8.327617138093046e-07 and accuracy is 1.0  || testing loss at epoch 40 is 4.363014795671916e-06 and accuracy is 1.0\n",
            "training loss at epoch 60 is 8.327617138093046e-07 and accuracy is 1.0  || testing loss at epoch 60 is 4.35904121331987e-06 and accuracy is 1.0\n",
            "training loss at epoch 80 is 8.327617138093046e-07 and accuracy is 1.0  || testing loss at epoch 80 is 4.35904121331987e-06 and accuracy is 1.0\n",
            "training loss at epoch 100 is 8.327617138093046e-07 and accuracy is 1.0  || testing loss at epoch 100 is 4.35904121331987e-06 and accuracy is 1.0\n",
            "*********************************************************************************************************************************\n",
            " Results for optimizer Momentum\n",
            "training loss at epoch 20 is 7.510182626901951e-07 and accuracy is 1.0  || testing loss at epoch 20 is 4.053075372212334e-06 and accuracy is 1.0\n",
            "training loss at epoch 40 is 7.459092898898234e-07 and accuracy is 1.0  || testing loss at epoch 40 is 4.03718104280415e-06 and accuracy is 1.0\n",
            "training loss at epoch 60 is 7.425032890751027e-07 and accuracy is 1.0  || testing loss at epoch 60 is 4.033207005704753e-06 and accuracy is 1.0\n",
            "training loss at epoch 80 is 7.425032890751027e-07 and accuracy is 1.0  || testing loss at epoch 80 is 4.033207005704753e-06 and accuracy is 1.0\n",
            "training loss at epoch 100 is 7.425032890751027e-07 and accuracy is 1.0  || testing loss at epoch 100 is 4.033207005704753e-06 and accuracy is 1.0\n",
            "*********************************************************************************************************************************\n",
            " Results for optimizer Adam\n",
            "training loss at epoch 20 is 6.794928140152479e-07 and accuracy is 1.0  || testing loss at epoch 20 is 3.6676412946690107e-06 and accuracy is 1.0\n",
            "training loss at epoch 40 is 6.658688675997837e-07 and accuracy is 1.0  || testing loss at epoch 40 is 3.619958988565486e-06 and accuracy is 1.0\n",
            "training loss at epoch 60 is 6.60759894799412e-07 and accuracy is 1.0  || testing loss at epoch 60 is 3.6239323435438564e-06 and accuracy is 1.0\n",
            "training loss at epoch 80 is 6.624629236284818e-07 and accuracy is 1.0  || testing loss at epoch 80 is 3.6398264455783647e-06 and accuracy is 1.0\n",
            "training loss at epoch 100 is 6.60759894799412e-07 and accuracy is 1.0  || testing loss at epoch 100 is 3.6398259908310138e-06 and accuracy is 1.0\n",
            "*********************************************************************************************************************************\n",
            " Results for optimizer AdamW\n",
            "training loss at epoch 20 is 6.60759894799412e-07 and accuracy is 1.0  || testing loss at epoch 20 is 3.6398262182046892e-06 and accuracy is 1.0\n",
            "training loss at epoch 40 is 6.60759894799412e-07 and accuracy is 1.0  || testing loss at epoch 40 is 3.6398262182046892e-06 and accuracy is 1.0\n",
            "training loss at epoch 60 is 6.60759894799412e-07 and accuracy is 1.0  || testing loss at epoch 60 is 3.6398262182046892e-06 and accuracy is 1.0\n",
            "training loss at epoch 80 is 6.60759894799412e-07 and accuracy is 1.0  || testing loss at epoch 80 is 3.6398262182046892e-06 and accuracy is 1.0\n",
            "training loss at epoch 100 is 6.60759894799412e-07 and accuracy is 1.0  || testing loss at epoch 100 is 3.6398262182046892e-06 and accuracy is 1.0\n",
            "*********************************************************************************************************************************\n",
            " Results for optimizer RMSProp\n",
            "training loss at epoch 20 is 3.678457858313777e-07 and accuracy is 1.0  || testing loss at epoch 20 is 2.638469368321239e-06 and accuracy is 1.0\n",
            "training loss at epoch 40 is 1.9073485191256623e-07 and accuracy is 1.0  || testing loss at epoch 40 is 1.3430852732199128e-06 and accuracy is 1.0\n",
            "training loss at epoch 60 is 1.1750628914342087e-07 and accuracy is 1.0  || testing loss at epoch 60 is 6.159139616102038e-07 and accuracy is 1.0\n",
            "training loss at epoch 80 is 4.4277733479702874e-08 and accuracy is 1.0  || testing loss at epoch 80 is 3.616011952090048e-07 and accuracy is 1.0\n",
            "training loss at epoch 100 is 0.0 and accuracy is 1.0  || testing loss at epoch 100 is 2.1855025522654614e-07 and accuracy is 1.0\n",
            "*********************************************************************************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UsX63dRJHUWt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}