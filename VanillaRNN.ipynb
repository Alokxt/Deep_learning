{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-Jec46pHsX02"
      },
      "outputs": [],
      "source": [
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "ItKNT3gCskLH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "upl = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "GV2x4u2SsdVt",
        "outputId": "1a82f163-7cb8-4730-8424-04fe121bb0cb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-fc3ed0ba-36ef-4af3-a8e1-c05b12467194\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-fc3ed0ba-36ef-4af3-a8e1-c05b12467194\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving train (1).csv to train (1).csv\n",
            "Saving test (1).csv to test (1).csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('train (1).csv')\n",
        "test = pd.read_csv('test (1).csv')"
      ],
      "metadata": {
        "id": "wuRkTfB3sf8J"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "VcLuSo3Kstmn",
        "outputId": "7f260c14-a604-4640-90a0-04ae39ade1bb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id                                             phrase  feature_1  \\\n",
              "0        0  It may as well be called `` Jar-Jar Binks : Th...       14.0   \n",
              "1        1                               You have to see it .        6.0   \n",
              "2        2  ... either you 're willing to go with this cla...       16.0   \n",
              "3        3  Watching Harris ham it up while physically and...       37.0   \n",
              "4        4  Pete 's screenplay manages to find that real n...       20.0   \n",
              "...    ...                                                ...        ...   \n",
              "6995  6995  A fantastic premise anchors this movie , but w...       41.0   \n",
              "6996  6996  A boring , pretentious muddle that uses a sens...       37.0   \n",
              "6997  6997     Boy , has this franchise ever run out of gas .       11.0   \n",
              "6998  6998  It 's hard to understand why anyone in his rig...       21.0   \n",
              "6999  6999  Perhaps the film should be seen as a conversat...       11.0   \n",
              "\n",
              "      feature_2  feature_3  sentiment  \n",
              "0           5.0        7.0          0  \n",
              "1           1.0        NaN          2  \n",
              "2           0.0        6.0          1  \n",
              "3           NaN        3.0          1  \n",
              "4           1.0        4.0          2  \n",
              "...         ...        ...        ...  \n",
              "6995        3.0        NaN          1  \n",
              "6996        2.0       11.0          0  \n",
              "6997        1.0        2.0          0  \n",
              "6998        1.0        2.0          0  \n",
              "6999        1.0        1.0          1  \n",
              "\n",
              "[7000 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8a8b78cd-5d01-4ece-9d86-8779097f97e2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>phrase</th>\n",
              "      <th>feature_1</th>\n",
              "      <th>feature_2</th>\n",
              "      <th>feature_3</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>It may as well be called `` Jar-Jar Binks : Th...</td>\n",
              "      <td>14.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>You have to see it .</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>... either you 're willing to go with this cla...</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Watching Harris ham it up while physically and...</td>\n",
              "      <td>37.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Pete 's screenplay manages to find that real n...</td>\n",
              "      <td>20.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6995</th>\n",
              "      <td>6995</td>\n",
              "      <td>A fantastic premise anchors this movie , but w...</td>\n",
              "      <td>41.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6996</th>\n",
              "      <td>6996</td>\n",
              "      <td>A boring , pretentious muddle that uses a sens...</td>\n",
              "      <td>37.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6997</th>\n",
              "      <td>6997</td>\n",
              "      <td>Boy , has this franchise ever run out of gas .</td>\n",
              "      <td>11.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6998</th>\n",
              "      <td>6998</td>\n",
              "      <td>It 's hard to understand why anyone in his rig...</td>\n",
              "      <td>21.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6999</th>\n",
              "      <td>6999</td>\n",
              "      <td>Perhaps the film should be seen as a conversat...</td>\n",
              "      <td>11.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7000 rows × 6 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8a8b78cd-5d01-4ece-9d86-8779097f97e2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8a8b78cd-5d01-4ece-9d86-8779097f97e2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8a8b78cd-5d01-4ece-9d86-8779097f97e2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-bca161e7-f9cf-4e5d-8234-8993a149f058\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bca161e7-f9cf-4e5d-8234-8993a149f058')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-bca161e7-f9cf-4e5d-8234-8993a149f058 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_5b0a5e33-6443-4a7c-915b-3701683ae71f\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('train')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_5b0a5e33-6443-4a7c-915b-3701683ae71f button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('train');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train",
              "summary": "{\n  \"name\": \"train\",\n  \"rows\": 7000,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2020,\n        \"min\": 0,\n        \"max\": 6999,\n        \"num_unique_values\": 7000,\n        \"samples\": [\n          6500,\n          2944,\n          2024\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"phrase\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6823,\n        \"samples\": [\n          \"It 's got all the familiar Bruckheimer elements , and Schumacher does probably as good a job as anyone at bringing off the Hopkins\\\\/Rock collision of acting styles and onscreen personas .\",\n          \"In questioning the election process , Payami graphically illustrates the problems of fledgling democracies , but also the strength and sense of freedom the Iranian people already possess , with or without access to the ballot box .\",\n          \"The performances of the children , untrained in acting , have an honesty and dignity that breaks your heart .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9.303562107439932,\n        \"min\": 1.0,\n        \"max\": 52.0,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          26.0,\n          2.0,\n          49.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.6348584123391023,\n        \"min\": 0.0,\n        \"max\": 19.0,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          6.0,\n          12.0,\n          5.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.321154025922672,\n        \"min\": 0.0,\n        \"max\": 19.0,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          7.0,\n          15.0,\n          13.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train['phrase'].iloc[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "f3APi1WCsyI3",
        "outputId": "c306e959-aa14-4910-b4a4-0abec09095eb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"It may as well be called `` Jar-Jar Binks : The Movie . ''\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "D = {}"
      ],
      "metadata": {
        "id": "0IEUQbdds3iZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "D['Unknown'] = 0"
      ],
      "metadata": {
        "id": "JB8XugRKwjQU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_data = train.iloc[:10]['phrase'].to_list()\n",
        "sents = train.iloc[:10]['sentiment']"
      ],
      "metadata": {
        "id": "s4Inj301vf-o"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "I1dijV3LvqTB",
        "outputId": "187a7165-23ae-4a15-9efa-dd0f89a24728"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0\n",
              "1    2\n",
              "2    1\n",
              "3    1\n",
              "4    2\n",
              "5    0\n",
              "6    0\n",
              "7    2\n",
              "8    2\n",
              "9    1\n",
              "Name: sentiment, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dict(x):\n",
        "  r = x.split(\" \")\n",
        "  for w in r:\n",
        "    if w.isalpha() and w.lower() not in D:\n",
        "      D[w.lower()] = len(D)\n",
        ""
      ],
      "metadata": {
        "id": "d8SfHGPHv5Qn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['phrase'].apply(lambda x:create_dict(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "vRNk2f4ajDQS",
        "outputId": "924e9104-0b4a-4bdf-d0d1-04a71d3ec3b5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       None\n",
              "1       None\n",
              "2       None\n",
              "3       None\n",
              "4       None\n",
              "        ... \n",
              "6995    None\n",
              "6996    None\n",
              "6997    None\n",
              "6998    None\n",
              "6999    None\n",
              "Name: phrase, Length: 7000, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>phrase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6995</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6996</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6997</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6998</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6999</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7000 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kKPnBHXyjMPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(D)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWqxScxwv-HD",
        "outputId": "a84b1939-339e-4cec-eaf1-5ac1ef32c6d3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13141"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embd_dim = 8\n",
        "embedd_mat = np.random.randn(len(D),embd_dim)*0.01"
      ],
      "metadata": {
        "id": "S3ussSMcwfGl"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_integer(x):\n",
        "  r = x.split(\" \")\n",
        "  l = []\n",
        "  for w in r:\n",
        "    if w.isalpha() and w.lower() in D:\n",
        "      l.append(D[w.lower()])\n",
        "    else:\n",
        "      l.append(0)\n",
        "\n",
        "\n",
        "  return l"
      ],
      "metadata": {
        "id": "lAQ4rzk2w_BM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convert_integer(\"Lagaan Bollywood\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfnzMafjxJCi",
        "outputId": "816c4607-0892-4510-8cbc-eaeadfc3c5e4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[90, 93]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = [convert_integer(r) for r in sample_data]"
      ],
      "metadata": {
        "id": "r3L8L6B3xNGa"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_embbeds(x):\n",
        "  return np.array([embedd_mat[i] for i in x])"
      ],
      "metadata": {
        "id": "LISoVZmNyB9h"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_dim = 16\n",
        "Wxh = np.random.randn(embd_dim,hidden_dim)*0.01\n",
        "Whh = np.random.randn(hidden_dim,hidden_dim)*0.01\n",
        "bh = np.random.randn(hidden_dim)\n",
        "Why = np.random.randn(hidden_dim,3)*0.01\n",
        "by = np.random.randn(3)"
      ],
      "metadata": {
        "id": "CeOFy3Le0X1c"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hidden_states(embbeds):\n",
        "  states = []\n",
        "  h_prev = np.zeros(hidden_dim)\n",
        "  for emb in embbeds:\n",
        "    h_prev = np.tanh(emb@Wxh + h_prev@Whh + bh)\n",
        "    states.append(h_prev)\n",
        "  return states"
      ],
      "metadata": {
        "id": "OMoxo7sKyIaC"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "  e = np.exp(x-np.max(x))\n",
        "  return e/np.sum(e)"
      ],
      "metadata": {
        "id": "ePfUzHWNyXqV"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def output(h):\n",
        "  logits = h@Why + by\n",
        "  return softmax(logits)"
      ],
      "metadata": {
        "id": "SAn2TD772WJ9"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eORnDL3P3C6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy(pred, target):\n",
        "    return -np.log(pred[target] + 1e-9)\n"
      ],
      "metadata": {
        "id": "bHmQse9D2ern"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(X,label):\n",
        "  embbeds = convert_embbeds(X)\n",
        "  hs = hidden_states(embbeds)\n",
        "  h_last = hs[-1]\n",
        "  pred = output(h_last)\n",
        "  loss = cross_entropy(pred, label)\n",
        "  return pred,hs,embbeds ,loss"
      ],
      "metadata": {
        "id": "Q-TEzDoj2tcG"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backprop(probs, label, hidden, embeds):\n",
        "    dWxh = np.zeros_like(Wxh)\n",
        "    dWhh = np.zeros_like(Whh)\n",
        "    dWhy = np.zeros_like(Why)\n",
        "    dbh  = np.zeros_like(bh)\n",
        "    dby  = np.zeros_like(by)\n",
        "\n",
        "    y_grad = probs.copy()\n",
        "    y_grad[label] -= 1\n",
        "\n",
        "    dWhy += np.outer(hidden[-1], y_grad)\n",
        "    dby  += y_grad\n",
        "\n",
        "    dh = y_grad @ Why.T\n",
        "    for t in reversed(range(len(hidden))):\n",
        "        h = hidden[t]\n",
        "        dz = (1 - h**2) * dh\n",
        "\n",
        "        dbh += dz\n",
        "        dWxh += np.outer(embeds[t], dz)\n",
        "\n",
        "        h_prev = hidden[t-1] if t > 0 else np.zeros(hidden_dim)\n",
        "        dWhh += np.outer(h_prev, dz)\n",
        "\n",
        "\n",
        "        dh = dz @ Whh.T\n",
        "\n",
        "    return dWxh, dWhh, dWhy, dbh, dby\n"
      ],
      "metadata": {
        "id": "6su1VUKK3Mz2"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 2000\n",
        "lr = 0.005"
      ],
      "metadata": {
        "id": "8KvHf6ikAmPI"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(epoch):\n",
        "  total_loss = 0\n",
        "  for label,encode in zip(sents,sequences):\n",
        "\n",
        "    pred,hs,embbeds ,loss = forward(encode,label)\n",
        "\n",
        "    dWxh, dWhh, dWhy, dbh, dby = backprop(pred,label,hs,embbeds)\n",
        "    Wxh -= lr*dWxh\n",
        "    Whh -= lr*dWhh\n",
        "    Why -= lr*dWhy\n",
        "    bh -= lr*dbh\n",
        "    by -= lr*dby\n",
        "    total_loss += loss\n",
        "  print(f'loss for all sentences in {i}th epoch is {total_loss}')\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvdUjf2WAo39",
        "outputId": "89f30f28-3db8-4683-f70c-bec55e2db972"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss for all sentences in 0th epoch is 16.79679719677856\n",
            "loss for all sentences in 1th epoch is 15.854205602253124\n",
            "loss for all sentences in 2th epoch is 15.024209341604394\n",
            "loss for all sentences in 3th epoch is 14.303899444123607\n",
            "loss for all sentences in 4th epoch is 13.688066241155664\n",
            "loss for all sentences in 5th epoch is 13.169259840253844\n",
            "loss for all sentences in 6th epoch is 12.738237638067938\n",
            "loss for all sentences in 7th epoch is 12.384654007350054\n",
            "loss for all sentences in 8th epoch is 12.097811099487458\n",
            "loss for all sentences in 9th epoch is 11.867318437395967\n",
            "loss for all sentences in 10th epoch is 11.683574528448647\n",
            "loss for all sentences in 11th epoch is 11.53805071140357\n",
            "loss for all sentences in 12th epoch is 11.42340347110764\n",
            "loss for all sentences in 13th epoch is 11.333461766364785\n",
            "loss for all sentences in 14th epoch is 11.263136941597585\n",
            "loss for all sentences in 15th epoch is 11.20829382194729\n",
            "loss for all sentences in 16th epoch is 11.165609879360606\n",
            "loss for all sentences in 17th epoch is 11.132438890050134\n",
            "loss for all sentences in 18th epoch is 11.10668770336602\n",
            "loss for all sentences in 19th epoch is 11.086709600474844\n",
            "loss for all sentences in 20th epoch is 11.07121466859697\n",
            "loss for all sentences in 21th epoch is 11.059196001735419\n",
            "loss for all sentences in 22th epoch is 11.049869825019307\n",
            "loss for all sentences in 23th epoch is 11.042627446533993\n",
            "loss for all sentences in 24th epoch is 11.036997022379083\n",
            "loss for all sentences in 25th epoch is 11.032613330013314\n",
            "loss for all sentences in 26th epoch is 11.029193998182315\n",
            "loss for all sentences in 27th epoch is 11.02652089474146\n",
            "loss for all sentences in 28th epoch is 11.02442560533886\n",
            "loss for all sentences in 29th epoch is 11.022778137900346\n",
            "loss for all sentences in 30th epoch is 11.02147815865421\n",
            "loss for all sentences in 31th epoch is 11.020448206905638\n",
            "loss for all sentences in 32th epoch is 11.01962845122572\n",
            "loss for all sentences in 33th epoch is 11.018972642894369\n",
            "loss for all sentences in 34th epoch is 11.01844499697116\n",
            "loss for all sentences in 35th epoch is 11.018017790568429\n",
            "loss for all sentences in 36th epoch is 11.01766951464319\n",
            "loss for all sentences in 37th epoch is 11.017383452345447\n",
            "loss for all sentences in 38th epoch is 11.017146585684102\n",
            "loss for all sentences in 39th epoch is 11.016948754656404\n",
            "loss for all sentences in 40th epoch is 11.016782010375154\n",
            "loss for all sentences in 41th epoch is 11.016640117197285\n",
            "loss for all sentences in 42th epoch is 11.016518169265842\n",
            "loss for all sentences in 43th epoch is 11.016412294903834\n",
            "loss for all sentences in 44th epoch is 11.01631942847713\n",
            "loss for all sentences in 45th epoch is 11.016237134092767\n",
            "loss for all sentences in 46th epoch is 11.016163469145113\n",
            "loss for all sentences in 47th epoch is 11.016096878518766\n",
            "loss for all sentences in 48th epoch is 11.016036112400355\n",
            "loss for all sentences in 49th epoch is 11.015980162293033\n",
            "loss for all sentences in 50th epoch is 11.015928211084873\n",
            "loss for all sentences in 51th epoch is 11.015879593984767\n",
            "loss for all sentences in 52th epoch is 11.015833767876853\n",
            "loss for all sentences in 53th epoch is 11.015790287208816\n",
            "loss for all sentences in 54th epoch is 11.015748784962382\n",
            "loss for all sentences in 55th epoch is 11.015708957585806\n",
            "loss for all sentences in 56th epoch is 11.015670553022865\n",
            "loss for all sentences in 57th epoch is 11.01563336116841\n",
            "loss for all sentences in 58th epoch is 11.015597206230801\n",
            "loss for all sentences in 59th epoch is 11.015561940597369\n",
            "loss for all sentences in 60th epoch is 11.01552743988832\n",
            "loss for all sentences in 61th epoch is 11.015493598953414\n",
            "loss for all sentences in 62th epoch is 11.015460328619016\n",
            "loss for all sentences in 63th epoch is 11.015427553034606\n",
            "loss for all sentences in 64th epoch is 11.015395207499816\n",
            "loss for all sentences in 65th epoch is 11.01536323667806\n",
            "loss for all sentences in 66th epoch is 11.015331593122479\n",
            "loss for all sentences in 67th epoch is 11.015300236054948\n",
            "loss for all sentences in 68th epoch is 11.01526913035111\n",
            "loss for all sentences in 69th epoch is 11.015238245693604\n",
            "loss for all sentences in 70th epoch is 11.015207555863249\n",
            "loss for all sentences in 71th epoch is 11.01517703814373\n",
            "loss for all sentences in 72th epoch is 11.015146672820002\n",
            "loss for all sentences in 73th epoch is 11.015116442754422\n",
            "loss for all sentences in 74th epoch is 11.015086333027497\n",
            "loss for all sentences in 75th epoch is 11.015056330632548\n",
            "loss for all sentences in 76th epoch is 11.015026424215556\n",
            "loss for all sentences in 77th epoch is 11.014996603852923\n",
            "loss for all sentences in 78th epoch is 11.014966860861199\n",
            "loss for all sentences in 79th epoch is 11.014937187633805\n",
            "loss for all sentences in 80th epoch is 11.01490757750062\n",
            "loss for all sentences in 81th epoch is 11.014878024607027\n",
            "loss for all sentences in 82th epoch is 11.014848523809489\n",
            "loss for all sentences in 83th epoch is 11.01481907058526\n",
            "loss for all sentences in 84th epoch is 11.014789660954197\n",
            "loss for all sentences in 85th epoch is 11.014760291410918\n",
            "loss for all sentences in 86th epoch is 11.014730958865877\n",
            "loss for all sentences in 87th epoch is 11.01470166059411\n",
            "loss for all sentences in 88th epoch is 11.014672394190578\n",
            "loss for all sentences in 89th epoch is 11.014643157531246\n",
            "loss for all sentences in 90th epoch is 11.014613948739074\n",
            "loss for all sentences in 91th epoch is 11.014584766154307\n",
            "loss for all sentences in 92th epoch is 11.014555608308486\n",
            "loss for all sentences in 93th epoch is 11.014526473901675\n",
            "loss for all sentences in 94th epoch is 11.014497361782507\n",
            "loss for all sentences in 95th epoch is 11.014468270930657\n",
            "loss for all sentences in 96th epoch is 11.014439200441508\n",
            "loss for all sentences in 97th epoch is 11.014410149512592\n",
            "loss for all sentences in 98th epoch is 11.014381117431743\n",
            "loss for all sentences in 99th epoch is 11.014352103566612\n",
            "loss for all sentences in 100th epoch is 11.01432310735548\n",
            "loss for all sentences in 101th epoch is 11.014294128299131\n",
            "loss for all sentences in 102th epoch is 11.014265165953695\n",
            "loss for all sentences in 103th epoch is 11.014236219924332\n",
            "loss for all sentences in 104th epoch is 11.01420728985966\n",
            "loss for all sentences in 105th epoch is 11.014178375446837\n",
            "loss for all sentences in 106th epoch is 11.014149476407184\n",
            "loss for all sentences in 107th epoch is 11.014120592492368\n",
            "loss for all sentences in 108th epoch is 11.014091723480972\n",
            "loss for all sentences in 109th epoch is 11.014062869175492\n",
            "loss for all sentences in 110th epoch is 11.01403402939967\n",
            "loss for all sentences in 111th epoch is 11.014005203996119\n",
            "loss for all sentences in 112th epoch is 11.013976392824242\n",
            "loss for all sentences in 113th epoch is 11.013947595758351\n",
            "loss for all sentences in 114th epoch is 11.01391881268605\n",
            "loss for all sentences in 115th epoch is 11.013890043506745\n",
            "loss for all sentences in 116th epoch is 11.013861288130357\n",
            "loss for all sentences in 117th epoch is 11.013832546476165\n",
            "loss for all sentences in 118th epoch is 11.013803818471795\n",
            "loss for all sentences in 119th epoch is 11.013775104052282\n",
            "loss for all sentences in 120th epoch is 11.013746403159294\n",
            "loss for all sentences in 121th epoch is 11.013717715740382\n",
            "loss for all sentences in 122th epoch is 11.013689041748368\n",
            "loss for all sentences in 123th epoch is 11.013660381140749\n",
            "loss for all sentences in 124th epoch is 11.013631733879208\n",
            "loss for all sentences in 125th epoch is 11.013603099929147\n",
            "loss for all sentences in 126th epoch is 11.013574479259319\n",
            "loss for all sentences in 127th epoch is 11.013545871841416\n",
            "loss for all sentences in 128th epoch is 11.013517277649804\n",
            "loss for all sentences in 129th epoch is 11.013488696661195\n",
            "loss for all sentences in 130th epoch is 11.013460128854433\n",
            "loss for all sentences in 131th epoch is 11.013431574210227\n",
            "loss for all sentences in 132th epoch is 11.013403032710983\n",
            "loss for all sentences in 133th epoch is 11.013374504340609\n",
            "loss for all sentences in 134th epoch is 11.013345989084351\n",
            "loss for all sentences in 135th epoch is 11.013317486928669\n",
            "loss for all sentences in 136th epoch is 11.013288997861078\n",
            "loss for all sentences in 137th epoch is 11.013260521870063\n",
            "loss for all sentences in 138th epoch is 11.013232058944961\n",
            "loss for all sentences in 139th epoch is 11.013203609075871\n",
            "loss for all sentences in 140th epoch is 11.013175172253588\n",
            "loss for all sentences in 141th epoch is 11.013146748469502\n",
            "loss for all sentences in 142th epoch is 11.013118337715563\n",
            "loss for all sentences in 143th epoch is 11.013089939984194\n",
            "loss for all sentences in 144th epoch is 11.01306155526827\n",
            "loss for all sentences in 145th epoch is 11.013033183561038\n",
            "loss for all sentences in 146th epoch is 11.013004824856115\n",
            "loss for all sentences in 147th epoch is 11.012976479147406\n",
            "loss for all sentences in 148th epoch is 11.012948146429105\n",
            "loss for all sentences in 149th epoch is 11.012919826695653\n",
            "loss for all sentences in 150th epoch is 11.01289151994171\n",
            "loss for all sentences in 151th epoch is 11.012863226162137\n",
            "loss for all sentences in 152th epoch is 11.012834945351965\n",
            "loss for all sentences in 153th epoch is 11.012806677506385\n",
            "loss for all sentences in 154th epoch is 11.012778422620727\n",
            "loss for all sentences in 155th epoch is 11.012750180690444\n",
            "loss for all sentences in 156th epoch is 11.012721951711105\n",
            "loss for all sentences in 157th epoch is 11.01269373567837\n",
            "loss for all sentences in 158th epoch is 11.012665532587995\n",
            "loss for all sentences in 159th epoch is 11.012637342435799\n",
            "loss for all sentences in 160th epoch is 11.012609165217686\n",
            "loss for all sentences in 161th epoch is 11.012581000929615\n",
            "loss for all sentences in 162th epoch is 11.012552849567594\n",
            "loss for all sentences in 163th epoch is 11.012524711127682\n",
            "loss for all sentences in 164th epoch is 11.012496585605973\n",
            "loss for all sentences in 165th epoch is 11.012468472998616\n",
            "loss for all sentences in 166th epoch is 11.012440373301773\n",
            "loss for all sentences in 167th epoch is 11.01241228651164\n",
            "loss for all sentences in 168th epoch is 11.01238421262444\n",
            "loss for all sentences in 169th epoch is 11.012356151636416\n",
            "loss for all sentences in 170th epoch is 11.012328103543819\n",
            "loss for all sentences in 171th epoch is 11.012300068342936\n",
            "loss for all sentences in 172th epoch is 11.012272046030045\n",
            "loss for all sentences in 173th epoch is 11.01224403660145\n",
            "loss for all sentences in 174th epoch is 11.012216040053456\n",
            "loss for all sentences in 175th epoch is 11.01218805638238\n",
            "loss for all sentences in 176th epoch is 11.01216008558454\n",
            "loss for all sentences in 177th epoch is 11.012132127656262\n",
            "loss for all sentences in 178th epoch is 11.012104182593871\n",
            "loss for all sentences in 179th epoch is 11.0120762503937\n",
            "loss for all sentences in 180th epoch is 11.01204833105208\n",
            "loss for all sentences in 181th epoch is 11.012020424565337\n",
            "loss for all sentences in 182th epoch is 11.011992530929806\n",
            "loss for all sentences in 183th epoch is 11.011964650141817\n",
            "loss for all sentences in 184th epoch is 11.01193678219769\n",
            "loss for all sentences in 185th epoch is 11.011908927093753\n",
            "loss for all sentences in 186th epoch is 11.011881084826328\n",
            "loss for all sentences in 187th epoch is 11.011853255391728\n",
            "loss for all sentences in 188th epoch is 11.011825438786271\n",
            "loss for all sentences in 189th epoch is 11.011797635006268\n",
            "loss for all sentences in 190th epoch is 11.011769844048015\n",
            "loss for all sentences in 191th epoch is 11.01174206590782\n",
            "loss for all sentences in 192th epoch is 11.011714300581975\n",
            "loss for all sentences in 193th epoch is 11.01168654806677\n",
            "loss for all sentences in 194th epoch is 11.011658808358487\n",
            "loss for all sentences in 195th epoch is 11.0116310814534\n",
            "loss for all sentences in 196th epoch is 11.011603367347787\n",
            "loss for all sentences in 197th epoch is 11.011575666037913\n",
            "loss for all sentences in 198th epoch is 11.011547977520037\n",
            "loss for all sentences in 199th epoch is 11.01152030179041\n",
            "loss for all sentences in 200th epoch is 11.011492638845283\n",
            "loss for all sentences in 201th epoch is 11.011464988680896\n",
            "loss for all sentences in 202th epoch is 11.011437351293477\n",
            "loss for all sentences in 203th epoch is 11.01140972667926\n",
            "loss for all sentences in 204th epoch is 11.01138211483446\n",
            "loss for all sentences in 205th epoch is 11.011354515755292\n",
            "loss for all sentences in 206th epoch is 11.01132692943797\n",
            "loss for all sentences in 207th epoch is 11.01129935587869\n",
            "loss for all sentences in 208th epoch is 11.01127179507364\n",
            "loss for all sentences in 209th epoch is 11.011244247019016\n",
            "loss for all sentences in 210th epoch is 11.011216711710993\n",
            "loss for all sentences in 211th epoch is 11.011189189145743\n",
            "loss for all sentences in 212th epoch is 11.011161679319434\n",
            "loss for all sentences in 213th epoch is 11.011134182228227\n",
            "loss for all sentences in 214th epoch is 11.01110669786827\n",
            "loss for all sentences in 215th epoch is 11.011079226235713\n",
            "loss for all sentences in 216th epoch is 11.011051767326693\n",
            "loss for all sentences in 217th epoch is 11.011024321137342\n",
            "loss for all sentences in 218th epoch is 11.010996887663783\n",
            "loss for all sentences in 219th epoch is 11.010969466902136\n",
            "loss for all sentences in 220th epoch is 11.010942058848512\n",
            "loss for all sentences in 221th epoch is 11.010914663499012\n",
            "loss for all sentences in 222th epoch is 11.010887280849737\n",
            "loss for all sentences in 223th epoch is 11.010859910896778\n",
            "loss for all sentences in 224th epoch is 11.010832553636217\n",
            "loss for all sentences in 225th epoch is 11.01080520906413\n",
            "loss for all sentences in 226th epoch is 11.010777877176592\n",
            "loss for all sentences in 227th epoch is 11.010750557969663\n",
            "loss for all sentences in 228th epoch is 11.010723251439398\n",
            "loss for all sentences in 229th epoch is 11.010695957581849\n",
            "loss for all sentences in 230th epoch is 11.010668676393056\n",
            "loss for all sentences in 231th epoch is 11.010641407869063\n",
            "loss for all sentences in 232th epoch is 11.01061415200589\n",
            "loss for all sentences in 233th epoch is 11.010586908799569\n",
            "loss for all sentences in 234th epoch is 11.010559678246112\n",
            "loss for all sentences in 235th epoch is 11.010532460341532\n",
            "loss for all sentences in 236th epoch is 11.010505255081826\n",
            "loss for all sentences in 237th epoch is 11.010478062462996\n",
            "loss for all sentences in 238th epoch is 11.010450882481031\n",
            "loss for all sentences in 239th epoch is 11.010423715131912\n",
            "loss for all sentences in 240th epoch is 11.010396560411621\n",
            "loss for all sentences in 241th epoch is 11.01036941831612\n",
            "loss for all sentences in 242th epoch is 11.010342288841384\n",
            "loss for all sentences in 243th epoch is 11.010315171983365\n",
            "loss for all sentences in 244th epoch is 11.01028806773801\n",
            "loss for all sentences in 245th epoch is 11.010260976101266\n",
            "loss for all sentences in 246th epoch is 11.010233897069075\n",
            "loss for all sentences in 247th epoch is 11.010206830637369\n",
            "loss for all sentences in 248th epoch is 11.010179776802069\n",
            "loss for all sentences in 249th epoch is 11.010152735559094\n",
            "loss for all sentences in 250th epoch is 11.010125706904361\n",
            "loss for all sentences in 251th epoch is 11.01009869083377\n",
            "loss for all sentences in 252th epoch is 11.010071687343228\n",
            "loss for all sentences in 253th epoch is 11.010044696428626\n",
            "loss for all sentences in 254th epoch is 11.010017718085848\n",
            "loss for all sentences in 255th epoch is 11.009990752310783\n",
            "loss for all sentences in 256th epoch is 11.009963799099301\n",
            "loss for all sentences in 257th epoch is 11.00993685844727\n",
            "loss for all sentences in 258th epoch is 11.009909930350556\n",
            "loss for all sentences in 259th epoch is 11.009883014805014\n",
            "loss for all sentences in 260th epoch is 11.009856111806497\n",
            "loss for all sentences in 261th epoch is 11.009829221350847\n",
            "loss for all sentences in 262th epoch is 11.0098023434339\n",
            "loss for all sentences in 263th epoch is 11.009775478051493\n",
            "loss for all sentences in 264th epoch is 11.009748625199453\n",
            "loss for all sentences in 265th epoch is 11.009721784873594\n",
            "loss for all sentences in 266th epoch is 11.009694957069735\n",
            "loss for all sentences in 267th epoch is 11.009668141783683\n",
            "loss for all sentences in 268th epoch is 11.00964133901124\n",
            "loss for all sentences in 269th epoch is 11.009614548748203\n",
            "loss for all sentences in 270th epoch is 11.009587770990365\n",
            "loss for all sentences in 271th epoch is 11.009561005733502\n",
            "loss for all sentences in 272th epoch is 11.009534252973404\n",
            "loss for all sentences in 273th epoch is 11.009507512705834\n",
            "loss for all sentences in 274th epoch is 11.009480784926566\n",
            "loss for all sentences in 275th epoch is 11.009454069631357\n",
            "loss for all sentences in 276th epoch is 11.009427366815967\n",
            "loss for all sentences in 277th epoch is 11.009400676476139\n",
            "loss for all sentences in 278th epoch is 11.009373998607618\n",
            "loss for all sentences in 279th epoch is 11.00934733320615\n",
            "loss for all sentences in 280th epoch is 11.009320680267457\n",
            "loss for all sentences in 281th epoch is 11.009294039787271\n",
            "loss for all sentences in 282th epoch is 11.009267411761318\n",
            "loss for all sentences in 283th epoch is 11.009240796185301\n",
            "loss for all sentences in 284th epoch is 11.009214193054941\n",
            "loss for all sentences in 285th epoch is 11.009187602365936\n",
            "loss for all sentences in 286th epoch is 11.009161024113986\n",
            "loss for all sentences in 287th epoch is 11.009134458294785\n",
            "loss for all sentences in 288th epoch is 11.009107904904019\n",
            "loss for all sentences in 289th epoch is 11.009081363937373\n",
            "loss for all sentences in 290th epoch is 11.009054835390517\n",
            "loss for all sentences in 291th epoch is 11.009028319259125\n",
            "loss for all sentences in 292th epoch is 11.009001815538866\n",
            "loss for all sentences in 293th epoch is 11.008975324225394\n",
            "loss for all sentences in 294th epoch is 11.00894884531437\n",
            "loss for all sentences in 295th epoch is 11.008922378801438\n",
            "loss for all sentences in 296th epoch is 11.008895924682243\n",
            "loss for all sentences in 297th epoch is 11.008869482952424\n",
            "loss for all sentences in 298th epoch is 11.008843053607613\n",
            "loss for all sentences in 299th epoch is 11.00881663664344\n",
            "loss for all sentences in 300th epoch is 11.008790232055523\n",
            "loss for all sentences in 301th epoch is 11.008763839839485\n",
            "loss for all sentences in 302th epoch is 11.00873745999093\n",
            "loss for all sentences in 303th epoch is 11.008711092505473\n",
            "loss for all sentences in 304th epoch is 11.00868473737871\n",
            "loss for all sentences in 305th epoch is 11.008658394606234\n",
            "loss for all sentences in 306th epoch is 11.008632064183645\n",
            "loss for all sentences in 307th epoch is 11.008605746106522\n",
            "loss for all sentences in 308th epoch is 11.008579440370445\n",
            "loss for all sentences in 309th epoch is 11.008553146970991\n",
            "loss for all sentences in 310th epoch is 11.008526865903734\n",
            "loss for all sentences in 311th epoch is 11.008500597164232\n",
            "loss for all sentences in 312th epoch is 11.00847434074805\n",
            "loss for all sentences in 313th epoch is 11.008448096650744\n",
            "loss for all sentences in 314th epoch is 11.00842186486786\n",
            "loss for all sentences in 315th epoch is 11.008395645394943\n",
            "loss for all sentences in 316th epoch is 11.008369438227534\n",
            "loss for all sentences in 317th epoch is 11.008343243361168\n",
            "loss for all sentences in 318th epoch is 11.008317060791377\n",
            "loss for all sentences in 319th epoch is 11.008290890513685\n",
            "loss for all sentences in 320th epoch is 11.00826473252361\n",
            "loss for all sentences in 321th epoch is 11.008238586816663\n",
            "loss for all sentences in 322th epoch is 11.008212453388365\n",
            "loss for all sentences in 323th epoch is 11.008186332234217\n",
            "loss for all sentences in 324th epoch is 11.008160223349712\n",
            "loss for all sentences in 325th epoch is 11.008134126730356\n",
            "loss for all sentences in 326th epoch is 11.008108042371635\n",
            "loss for all sentences in 327th epoch is 11.008081970269037\n",
            "loss for all sentences in 328th epoch is 11.008055910418038\n",
            "loss for all sentences in 329th epoch is 11.008029862814123\n",
            "loss for all sentences in 330th epoch is 11.008003827452756\n",
            "loss for all sentences in 331th epoch is 11.007977804329407\n",
            "loss for all sentences in 332th epoch is 11.007951793439538\n",
            "loss for all sentences in 333th epoch is 11.007925794778611\n",
            "loss for all sentences in 334th epoch is 11.00789980834207\n",
            "loss for all sentences in 335th epoch is 11.00787383412537\n",
            "loss for all sentences in 336th epoch is 11.007847872123952\n",
            "loss for all sentences in 337th epoch is 11.007821922333255\n",
            "loss for all sentences in 338th epoch is 11.007795984748716\n",
            "loss for all sentences in 339th epoch is 11.00777005936576\n",
            "loss for all sentences in 340th epoch is 11.007744146179819\n",
            "loss for all sentences in 341th epoch is 11.007718245186306\n",
            "loss for all sentences in 342th epoch is 11.007692356380643\n",
            "loss for all sentences in 343th epoch is 11.007666479758237\n",
            "loss for all sentences in 344th epoch is 11.0076406153145\n",
            "loss for all sentences in 345th epoch is 11.007614763044835\n",
            "loss for all sentences in 346th epoch is 11.007588922944633\n",
            "loss for all sentences in 347th epoch is 11.007563095009294\n",
            "loss for all sentences in 348th epoch is 11.007537279234207\n",
            "loss for all sentences in 349th epoch is 11.007511475614756\n",
            "loss for all sentences in 350th epoch is 11.007485684146321\n",
            "loss for all sentences in 351th epoch is 11.007459904824278\n",
            "loss for all sentences in 352th epoch is 11.007434137643997\n",
            "loss for all sentences in 353th epoch is 11.00740838260085\n",
            "loss for all sentences in 354th epoch is 11.0073826396902\n",
            "loss for all sentences in 355th epoch is 11.007356908907404\n",
            "loss for all sentences in 356th epoch is 11.007331190247815\n",
            "loss for all sentences in 357th epoch is 11.007305483706787\n",
            "loss for all sentences in 358th epoch is 11.00727978927966\n",
            "loss for all sentences in 359th epoch is 11.007254106961783\n",
            "loss for all sentences in 360th epoch is 11.007228436748491\n",
            "loss for all sentences in 361th epoch is 11.007202778635117\n",
            "loss for all sentences in 362th epoch is 11.007177132616992\n",
            "loss for all sentences in 363th epoch is 11.007151498689437\n",
            "loss for all sentences in 364th epoch is 11.007125876847775\n",
            "loss for all sentences in 365th epoch is 11.007100267087328\n",
            "loss for all sentences in 366th epoch is 11.007074669403401\n",
            "loss for all sentences in 367th epoch is 11.007049083791307\n",
            "loss for all sentences in 368th epoch is 11.007023510246349\n",
            "loss for all sentences in 369th epoch is 11.006997948763829\n",
            "loss for all sentences in 370th epoch is 11.006972399339043\n",
            "loss for all sentences in 371th epoch is 11.006946861967283\n",
            "loss for all sentences in 372th epoch is 11.006921336643835\n",
            "loss for all sentences in 373th epoch is 11.006895823363987\n",
            "loss for all sentences in 374th epoch is 11.00687032212302\n",
            "loss for all sentences in 375th epoch is 11.00684483291621\n",
            "loss for all sentences in 376th epoch is 11.006819355738825\n",
            "loss for all sentences in 377th epoch is 11.006793890586142\n",
            "loss for all sentences in 378th epoch is 11.00676843745342\n",
            "loss for all sentences in 379th epoch is 11.006742996335918\n",
            "loss for all sentences in 380th epoch is 11.0067175672289\n",
            "loss for all sentences in 381th epoch is 11.006692150127613\n",
            "loss for all sentences in 382th epoch is 11.006666745027308\n",
            "loss for all sentences in 383th epoch is 11.006641351923234\n",
            "loss for all sentences in 384th epoch is 11.006615970810628\n",
            "loss for all sentences in 385th epoch is 11.006590601684733\n",
            "loss for all sentences in 386th epoch is 11.006565244540775\n",
            "loss for all sentences in 387th epoch is 11.006539899373992\n",
            "loss for all sentences in 388th epoch is 11.006514566179609\n",
            "loss for all sentences in 389th epoch is 11.006489244952846\n",
            "loss for all sentences in 390th epoch is 11.006463935688922\n",
            "loss for all sentences in 391th epoch is 11.006438638383058\n",
            "loss for all sentences in 392th epoch is 11.00641335303046\n",
            "loss for all sentences in 393th epoch is 11.00638807962634\n",
            "loss for all sentences in 394th epoch is 11.0063628181659\n",
            "loss for all sentences in 395th epoch is 11.006337568644343\n",
            "loss for all sentences in 396th epoch is 11.006312331056865\n",
            "loss for all sentences in 397th epoch is 11.006287105398657\n",
            "loss for all sentences in 398th epoch is 11.006261891664915\n",
            "loss for all sentences in 399th epoch is 11.006236689850821\n",
            "loss for all sentences in 400th epoch is 11.006211499951558\n",
            "loss for all sentences in 401th epoch is 11.00618632196231\n",
            "loss for all sentences in 402th epoch is 11.00616115587825\n",
            "loss for all sentences in 403th epoch is 11.006136001694548\n",
            "loss for all sentences in 404th epoch is 11.006110859406375\n",
            "loss for all sentences in 405th epoch is 11.0060857290089\n",
            "loss for all sentences in 406th epoch is 11.006060610497276\n",
            "loss for all sentences in 407th epoch is 11.006035503866674\n",
            "loss for all sentences in 408th epoch is 11.006010409112237\n",
            "loss for all sentences in 409th epoch is 11.005985326229123\n",
            "loss for all sentences in 410th epoch is 11.00596025521248\n",
            "loss for all sentences in 411th epoch is 11.005935196057456\n",
            "loss for all sentences in 412th epoch is 11.005910148759183\n",
            "loss for all sentences in 413th epoch is 11.00588511331281\n",
            "loss for all sentences in 414th epoch is 11.005860089713465\n",
            "loss for all sentences in 415th epoch is 11.00583507795628\n",
            "loss for all sentences in 416th epoch is 11.00581007803639\n",
            "loss for all sentences in 417th epoch is 11.00578508994891\n",
            "loss for all sentences in 418th epoch is 11.005760113688972\n",
            "loss for all sentences in 419th epoch is 11.005735149251686\n",
            "loss for all sentences in 420th epoch is 11.005710196632172\n",
            "loss for all sentences in 421th epoch is 11.005685255825542\n",
            "loss for all sentences in 422th epoch is 11.0056603268269\n",
            "loss for all sentences in 423th epoch is 11.005635409631358\n",
            "loss for all sentences in 424th epoch is 11.005610504234014\n",
            "loss for all sentences in 425th epoch is 11.005585610629968\n",
            "loss for all sentences in 426th epoch is 11.00556072881432\n",
            "loss for all sentences in 427th epoch is 11.005535858782157\n",
            "loss for all sentences in 428th epoch is 11.005511000528573\n",
            "loss for all sentences in 429th epoch is 11.005486154048654\n",
            "loss for all sentences in 430th epoch is 11.005461319337481\n",
            "loss for all sentences in 431th epoch is 11.00543649639014\n",
            "loss for all sentences in 432th epoch is 11.005411685201702\n",
            "loss for all sentences in 433th epoch is 11.005386885767248\n",
            "loss for all sentences in 434th epoch is 11.005362098081845\n",
            "loss for all sentences in 435th epoch is 11.005337322140562\n",
            "loss for all sentences in 436th epoch is 11.005312557938465\n",
            "loss for all sentences in 437th epoch is 11.00528780547062\n",
            "loss for all sentences in 438th epoch is 11.005263064732077\n",
            "loss for all sentences in 439th epoch is 11.005238335717902\n",
            "loss for all sentences in 440th epoch is 11.005213618423145\n",
            "loss for all sentences in 441th epoch is 11.005188912842858\n",
            "loss for all sentences in 442th epoch is 11.005164218972084\n",
            "loss for all sentences in 443th epoch is 11.005139536805874\n",
            "loss for all sentences in 444th epoch is 11.005114866339266\n",
            "loss for all sentences in 445th epoch is 11.0050902075673\n",
            "loss for all sentences in 446th epoch is 11.005065560485011\n",
            "loss for all sentences in 447th epoch is 11.005040925087435\n",
            "loss for all sentences in 448th epoch is 11.005016301369599\n",
            "loss for all sentences in 449th epoch is 11.004991689326534\n",
            "loss for all sentences in 450th epoch is 11.004967088953263\n",
            "loss for all sentences in 451th epoch is 11.004942500244809\n",
            "loss for all sentences in 452th epoch is 11.004917923196192\n",
            "loss for all sentences in 453th epoch is 11.004893357802427\n",
            "loss for all sentences in 454th epoch is 11.004868804058527\n",
            "loss for all sentences in 455th epoch is 11.004844261959503\n",
            "loss for all sentences in 456th epoch is 11.004819731500366\n",
            "loss for all sentences in 457th epoch is 11.004795212676118\n",
            "loss for all sentences in 458th epoch is 11.004770705481766\n",
            "loss for all sentences in 459th epoch is 11.004746209912307\n",
            "loss for all sentences in 460th epoch is 11.004721725962739\n",
            "loss for all sentences in 461th epoch is 11.004697253628061\n",
            "loss for all sentences in 462th epoch is 11.004672792903255\n",
            "loss for all sentences in 463th epoch is 11.004648343783327\n",
            "loss for all sentences in 464th epoch is 11.004623906263246\n",
            "loss for all sentences in 465th epoch is 11.004599480338008\n",
            "loss for all sentences in 466th epoch is 11.004575066002593\n",
            "loss for all sentences in 467th epoch is 11.004550663251978\n",
            "loss for all sentences in 468th epoch is 11.004526272081138\n",
            "loss for all sentences in 469th epoch is 11.00450189248505\n",
            "loss for all sentences in 470th epoch is 11.00447752445869\n",
            "loss for all sentences in 471th epoch is 11.004453167997017\n",
            "loss for all sentences in 472th epoch is 11.004428823095004\n",
            "loss for all sentences in 473th epoch is 11.004404489747618\n",
            "loss for all sentences in 474th epoch is 11.004380167949813\n",
            "loss for all sentences in 475th epoch is 11.004355857696552\n",
            "loss for all sentences in 476th epoch is 11.004331558982791\n",
            "loss for all sentences in 477th epoch is 11.004307271803487\n",
            "loss for all sentences in 478th epoch is 11.004282996153588\n",
            "loss for all sentences in 479th epoch is 11.004258732028044\n",
            "loss for all sentences in 480th epoch is 11.004234479421804\n",
            "loss for all sentences in 481th epoch is 11.00421023832981\n",
            "loss for all sentences in 482th epoch is 11.004186008747004\n",
            "loss for all sentences in 483th epoch is 11.00416179066833\n",
            "loss for all sentences in 484th epoch is 11.004137584088722\n",
            "loss for all sentences in 485th epoch is 11.004113389003116\n",
            "loss for all sentences in 486th epoch is 11.004089205406448\n",
            "loss for all sentences in 487th epoch is 11.00406503329364\n",
            "loss for all sentences in 488th epoch is 11.00404087265963\n",
            "loss for all sentences in 489th epoch is 11.004016723499335\n",
            "loss for all sentences in 490th epoch is 11.003992585807683\n",
            "loss for all sentences in 491th epoch is 11.003968459579601\n",
            "loss for all sentences in 492th epoch is 11.003944344809998\n",
            "loss for all sentences in 493th epoch is 11.003920241493796\n",
            "loss for all sentences in 494th epoch is 11.003896149625911\n",
            "loss for all sentences in 495th epoch is 11.00387206920125\n",
            "loss for all sentences in 496th epoch is 11.003848000214727\n",
            "loss for all sentences in 497th epoch is 11.003823942661251\n",
            "loss for all sentences in 498th epoch is 11.003799896535726\n",
            "loss for all sentences in 499th epoch is 11.003775861833057\n",
            "loss for all sentences in 500th epoch is 11.003751838548148\n",
            "loss for all sentences in 501th epoch is 11.003727826675888\n",
            "loss for all sentences in 502th epoch is 11.003703826211185\n",
            "loss for all sentences in 503th epoch is 11.00367983714893\n",
            "loss for all sentences in 504th epoch is 11.003655859484017\n",
            "loss for all sentences in 505th epoch is 11.003631893211335\n",
            "loss for all sentences in 506th epoch is 11.003607938325775\n",
            "loss for all sentences in 507th epoch is 11.003583994822224\n",
            "loss for all sentences in 508th epoch is 11.003560062695566\n",
            "loss for all sentences in 509th epoch is 11.00353614194068\n",
            "loss for all sentences in 510th epoch is 11.003512232552453\n",
            "loss for all sentences in 511th epoch is 11.003488334525763\n",
            "loss for all sentences in 512th epoch is 11.00346444785548\n",
            "loss for all sentences in 513th epoch is 11.003440572536483\n",
            "loss for all sentences in 514th epoch is 11.003416708563645\n",
            "loss for all sentences in 515th epoch is 11.003392855931837\n",
            "loss for all sentences in 516th epoch is 11.003369014635929\n",
            "loss for all sentences in 517th epoch is 11.003345184670783\n",
            "loss for all sentences in 518th epoch is 11.003321366031265\n",
            "loss for all sentences in 519th epoch is 11.00329755871224\n",
            "loss for all sentences in 520th epoch is 11.003273762708568\n",
            "loss for all sentences in 521th epoch is 11.00324997801511\n",
            "loss for all sentences in 522th epoch is 11.003226204626719\n",
            "loss for all sentences in 523th epoch is 11.003202442538255\n",
            "loss for all sentences in 524th epoch is 11.003178691744562\n",
            "loss for all sentences in 525th epoch is 11.003154952240505\n",
            "loss for all sentences in 526th epoch is 11.003131224020926\n",
            "loss for all sentences in 527th epoch is 11.00310750708067\n",
            "loss for all sentences in 528th epoch is 11.003083801414592\n",
            "loss for all sentences in 529th epoch is 11.003060107017527\n",
            "loss for all sentences in 530th epoch is 11.003036423884321\n",
            "loss for all sentences in 531th epoch is 11.003012752009816\n",
            "loss for all sentences in 532th epoch is 11.00298909138885\n",
            "loss for all sentences in 533th epoch is 11.00296544201626\n",
            "loss for all sentences in 534th epoch is 11.002941803886879\n",
            "loss for all sentences in 535th epoch is 11.002918176995543\n",
            "loss for all sentences in 536th epoch is 11.00289456133708\n",
            "loss for all sentences in 537th epoch is 11.002870956906326\n",
            "loss for all sentences in 538th epoch is 11.002847363698105\n",
            "loss for all sentences in 539th epoch is 11.002823781707246\n",
            "loss for all sentences in 540th epoch is 11.002800210928573\n",
            "loss for all sentences in 541th epoch is 11.002776651356905\n",
            "loss for all sentences in 542th epoch is 11.002753102987073\n",
            "loss for all sentences in 543th epoch is 11.002729565813885\n",
            "loss for all sentences in 544th epoch is 11.00270603983217\n",
            "loss for all sentences in 545th epoch is 11.00268252503674\n",
            "loss for all sentences in 546th epoch is 11.002659021422405\n",
            "loss for all sentences in 547th epoch is 11.002635528983987\n",
            "loss for all sentences in 548th epoch is 11.002612047716292\n",
            "loss for all sentences in 549th epoch is 11.002588577614132\n",
            "loss for all sentences in 550th epoch is 11.002565118672315\n",
            "loss for all sentences in 551th epoch is 11.00254167088565\n",
            "loss for all sentences in 552th epoch is 11.002518234248939\n",
            "loss for all sentences in 553th epoch is 11.002494808756985\n",
            "loss for all sentences in 554th epoch is 11.002471394404598\n",
            "loss for all sentences in 555th epoch is 11.002447991186568\n",
            "loss for all sentences in 556th epoch is 11.002424599097704\n",
            "loss for all sentences in 557th epoch is 11.002401218132794\n",
            "loss for all sentences in 558th epoch is 11.002377848286642\n",
            "loss for all sentences in 559th epoch is 11.00235448955404\n",
            "loss for all sentences in 560th epoch is 11.002331141929778\n",
            "loss for all sentences in 561th epoch is 11.002307805408654\n",
            "loss for all sentences in 562th epoch is 11.002284479985452\n",
            "loss for all sentences in 563th epoch is 11.002261165654962\n",
            "loss for all sentences in 564th epoch is 11.002237862411974\n",
            "loss for all sentences in 565th epoch is 11.00221457025127\n",
            "loss for all sentences in 566th epoch is 11.002191289167639\n",
            "loss for all sentences in 567th epoch is 11.00216801915586\n",
            "loss for all sentences in 568th epoch is 11.002144760210719\n",
            "loss for all sentences in 569th epoch is 11.00212151232699\n",
            "loss for all sentences in 570th epoch is 11.002098275499456\n",
            "loss for all sentences in 571th epoch is 11.002075049722894\n",
            "loss for all sentences in 572th epoch is 11.002051834992079\n",
            "loss for all sentences in 573th epoch is 11.002028631301783\n",
            "loss for all sentences in 574th epoch is 11.002005438646787\n",
            "loss for all sentences in 575th epoch is 11.001982257021856\n",
            "loss for all sentences in 576th epoch is 11.001959086421762\n",
            "loss for all sentences in 577th epoch is 11.001935926841277\n",
            "loss for all sentences in 578th epoch is 11.001912778275166\n",
            "loss for all sentences in 579th epoch is 11.001889640718197\n",
            "loss for all sentences in 580th epoch is 11.001866514165132\n",
            "loss for all sentences in 581th epoch is 11.001843398610742\n",
            "loss for all sentences in 582th epoch is 11.00182029404978\n",
            "loss for all sentences in 583th epoch is 11.00179720047702\n",
            "loss for all sentences in 584th epoch is 11.001774117887212\n",
            "loss for all sentences in 585th epoch is 11.001751046275121\n",
            "loss for all sentences in 586th epoch is 11.0017279856355\n",
            "loss for all sentences in 587th epoch is 11.00170493596311\n",
            "loss for all sentences in 588th epoch is 11.0016818972527\n",
            "loss for all sentences in 589th epoch is 11.001658869499032\n",
            "loss for all sentences in 590th epoch is 11.001635852696857\n",
            "loss for all sentences in 591th epoch is 11.00161284684092\n",
            "loss for all sentences in 592th epoch is 11.001589851925981\n",
            "loss for all sentences in 593th epoch is 11.001566867946783\n",
            "loss for all sentences in 594th epoch is 11.001543894898075\n",
            "loss for all sentences in 595th epoch is 11.001520932774605\n",
            "loss for all sentences in 596th epoch is 11.00149798157112\n",
            "loss for all sentences in 597th epoch is 11.001475041282363\n",
            "loss for all sentences in 598th epoch is 11.001452111903077\n",
            "loss for all sentences in 599th epoch is 11.001429193428011\n",
            "loss for all sentences in 600th epoch is 11.001406285851894\n",
            "loss for all sentences in 601th epoch is 11.001383389169476\n",
            "loss for all sentences in 602th epoch is 11.001360503375492\n",
            "loss for all sentences in 603th epoch is 11.001337628464684\n",
            "loss for all sentences in 604th epoch is 11.001314764431786\n",
            "loss for all sentences in 605th epoch is 11.001291911271535\n",
            "loss for all sentences in 606th epoch is 11.001269068978662\n",
            "loss for all sentences in 607th epoch is 11.001246237547903\n",
            "loss for all sentences in 608th epoch is 11.001223416973993\n",
            "loss for all sentences in 609th epoch is 11.001200607251665\n",
            "loss for all sentences in 610th epoch is 11.001177808375646\n",
            "loss for all sentences in 611th epoch is 11.001155020340667\n",
            "loss for all sentences in 612th epoch is 11.001132243141452\n",
            "loss for all sentences in 613th epoch is 11.001109476772738\n",
            "loss for all sentences in 614th epoch is 11.001086721229242\n",
            "loss for all sentences in 615th epoch is 11.001063976505694\n",
            "loss for all sentences in 616th epoch is 11.001041242596822\n",
            "loss for all sentences in 617th epoch is 11.001018519497347\n",
            "loss for all sentences in 618th epoch is 11.000995807201988\n",
            "loss for all sentences in 619th epoch is 11.000973105705473\n",
            "loss for all sentences in 620th epoch is 11.000950415002519\n",
            "loss for all sentences in 621th epoch is 11.000927735087846\n",
            "loss for all sentences in 622th epoch is 11.000905065956172\n",
            "loss for all sentences in 623th epoch is 11.00088240760222\n",
            "loss for all sentences in 624th epoch is 11.0008597600207\n",
            "loss for all sentences in 625th epoch is 11.000837123206336\n",
            "loss for all sentences in 626th epoch is 11.000814497153836\n",
            "loss for all sentences in 627th epoch is 11.000791881857921\n",
            "loss for all sentences in 628th epoch is 11.000769277313296\n",
            "loss for all sentences in 629th epoch is 11.000746683514684\n",
            "loss for all sentences in 630th epoch is 11.000724100456791\n",
            "loss for all sentences in 631th epoch is 11.000701528134327\n",
            "loss for all sentences in 632th epoch is 11.000678966542004\n",
            "loss for all sentences in 633th epoch is 11.000656415674534\n",
            "loss for all sentences in 634th epoch is 11.000633875526617\n",
            "loss for all sentences in 635th epoch is 11.000611346092972\n",
            "loss for all sentences in 636th epoch is 11.000588827368295\n",
            "loss for all sentences in 637th epoch is 11.0005663193473\n",
            "loss for all sentences in 638th epoch is 11.000543822024687\n",
            "loss for all sentences in 639th epoch is 11.000521335395161\n",
            "loss for all sentences in 640th epoch is 11.000498859453428\n",
            "loss for all sentences in 641th epoch is 11.000476394194191\n",
            "loss for all sentences in 642th epoch is 11.000453939612148\n",
            "loss for all sentences in 643th epoch is 11.000431495702003\n",
            "loss for all sentences in 644th epoch is 11.00040906245846\n",
            "loss for all sentences in 645th epoch is 11.00038663987621\n",
            "loss for all sentences in 646th epoch is 11.000364227949957\n",
            "loss for all sentences in 647th epoch is 11.000341826674402\n",
            "loss for all sentences in 648th epoch is 11.000319436044236\n",
            "loss for all sentences in 649th epoch is 11.000297056054162\n",
            "loss for all sentences in 650th epoch is 11.000274686698875\n",
            "loss for all sentences in 651th epoch is 11.000252327973064\n",
            "loss for all sentences in 652th epoch is 11.000229979871431\n",
            "loss for all sentences in 653th epoch is 11.000207642388666\n",
            "loss for all sentences in 654th epoch is 11.000185315519465\n",
            "loss for all sentences in 655th epoch is 11.000162999258519\n",
            "loss for all sentences in 656th epoch is 11.000140693600517\n",
            "loss for all sentences in 657th epoch is 11.000118398540156\n",
            "loss for all sentences in 658th epoch is 11.000096114072122\n",
            "loss for all sentences in 659th epoch is 11.000073840191108\n",
            "loss for all sentences in 660th epoch is 11.0000515768918\n",
            "loss for all sentences in 661th epoch is 11.00002932416889\n",
            "loss for all sentences in 662th epoch is 11.000007082017067\n",
            "loss for all sentences in 663th epoch is 10.999984850431012\n",
            "loss for all sentences in 664th epoch is 10.999962629405417\n",
            "loss for all sentences in 665th epoch is 10.999940418934967\n",
            "loss for all sentences in 666th epoch is 10.999918219014349\n",
            "loss for all sentences in 667th epoch is 10.999896029638242\n",
            "loss for all sentences in 668th epoch is 10.999873850801336\n",
            "loss for all sentences in 669th epoch is 10.999851682498315\n",
            "loss for all sentences in 670th epoch is 10.99982952472386\n",
            "loss for all sentences in 671th epoch is 10.999807377472656\n",
            "loss for all sentences in 672th epoch is 10.999785240739381\n",
            "loss for all sentences in 673th epoch is 10.999763114518718\n",
            "loss for all sentences in 674th epoch is 10.99974099880535\n",
            "loss for all sentences in 675th epoch is 10.999718893593956\n",
            "loss for all sentences in 676th epoch is 10.999696798879215\n",
            "loss for all sentences in 677th epoch is 10.999674714655809\n",
            "loss for all sentences in 678th epoch is 10.999652640918413\n",
            "loss for all sentences in 679th epoch is 10.999630577661707\n",
            "loss for all sentences in 680th epoch is 10.99960852488037\n",
            "loss for all sentences in 681th epoch is 10.999586482569077\n",
            "loss for all sentences in 682th epoch is 10.999564450722506\n",
            "loss for all sentences in 683th epoch is 10.999542429335335\n",
            "loss for all sentences in 684th epoch is 10.999520418402238\n",
            "loss for all sentences in 685th epoch is 10.99949841791789\n",
            "loss for all sentences in 686th epoch is 10.999476427876965\n",
            "loss for all sentences in 687th epoch is 10.999454448274138\n",
            "loss for all sentences in 688th epoch is 10.999432479104083\n",
            "loss for all sentences in 689th epoch is 10.999410520361474\n",
            "loss for all sentences in 690th epoch is 10.999388572040985\n",
            "loss for all sentences in 691th epoch is 10.999366634137287\n",
            "loss for all sentences in 692th epoch is 10.99934470664505\n",
            "loss for all sentences in 693th epoch is 10.999322789558951\n",
            "loss for all sentences in 694th epoch is 10.999300882873657\n",
            "loss for all sentences in 695th epoch is 10.999278986583839\n",
            "loss for all sentences in 696th epoch is 10.999257100684169\n",
            "loss for all sentences in 697th epoch is 10.999235225169315\n",
            "loss for all sentences in 698th epoch is 10.999213360033949\n",
            "loss for all sentences in 699th epoch is 10.999191505272737\n",
            "loss for all sentences in 700th epoch is 10.999169660880353\n",
            "loss for all sentences in 701th epoch is 10.99914782685146\n",
            "loss for all sentences in 702th epoch is 10.99912600318073\n",
            "loss for all sentences in 703th epoch is 10.99910418986283\n",
            "loss for all sentences in 704th epoch is 10.999082386892425\n",
            "loss for all sentences in 705th epoch is 10.999060594264181\n",
            "loss for all sentences in 706th epoch is 10.999038811972767\n",
            "loss for all sentences in 707th epoch is 10.999017040012852\n",
            "loss for all sentences in 708th epoch is 10.998995278379097\n",
            "loss for all sentences in 709th epoch is 10.998973527066168\n",
            "loss for all sentences in 710th epoch is 10.998951786068732\n",
            "loss for all sentences in 711th epoch is 10.998930055381452\n",
            "loss for all sentences in 712th epoch is 10.998908334998998\n",
            "loss for all sentences in 713th epoch is 10.998886624916025\n",
            "loss for all sentences in 714th epoch is 10.998864925127206\n",
            "loss for all sentences in 715th epoch is 10.998843235627197\n",
            "loss for all sentences in 716th epoch is 10.998821556410668\n",
            "loss for all sentences in 717th epoch is 10.998799887472275\n",
            "loss for all sentences in 718th epoch is 10.998778228806687\n",
            "loss for all sentences in 719th epoch is 10.998756580408564\n",
            "loss for all sentences in 720th epoch is 10.998734942272568\n",
            "loss for all sentences in 721th epoch is 10.998713314393362\n",
            "loss for all sentences in 722th epoch is 10.998691696765606\n",
            "loss for all sentences in 723th epoch is 10.99867008938396\n",
            "loss for all sentences in 724th epoch is 10.99864849224309\n",
            "loss for all sentences in 725th epoch is 10.998626905337652\n",
            "loss for all sentences in 726th epoch is 10.99860532866231\n",
            "loss for all sentences in 727th epoch is 10.998583762211718\n",
            "loss for all sentences in 728th epoch is 10.998562205980544\n",
            "loss for all sentences in 729th epoch is 10.998540659963446\n",
            "loss for all sentences in 730th epoch is 10.99851912415508\n",
            "loss for all sentences in 731th epoch is 10.998497598550108\n",
            "loss for all sentences in 732th epoch is 10.998476083143188\n",
            "loss for all sentences in 733th epoch is 10.998454577928985\n",
            "loss for all sentences in 734th epoch is 10.99843308290215\n",
            "loss for all sentences in 735th epoch is 10.998411598057341\n",
            "loss for all sentences in 736th epoch is 10.998390123389223\n",
            "loss for all sentences in 737th epoch is 10.998368658892446\n",
            "loss for all sentences in 738th epoch is 10.998347204561679\n",
            "loss for all sentences in 739th epoch is 10.998325760391568\n",
            "loss for all sentences in 740th epoch is 10.998304326376783\n",
            "loss for all sentences in 741th epoch is 10.998282902511969\n",
            "loss for all sentences in 742th epoch is 10.998261488791789\n",
            "loss for all sentences in 743th epoch is 10.9982400852109\n",
            "loss for all sentences in 744th epoch is 10.998218691763961\n",
            "loss for all sentences in 745th epoch is 10.998197308445626\n",
            "loss for all sentences in 746th epoch is 10.998175935250552\n",
            "loss for all sentences in 747th epoch is 10.998154572173398\n",
            "loss for all sentences in 748th epoch is 10.998133219208816\n",
            "loss for all sentences in 749th epoch is 10.998111876351466\n",
            "loss for all sentences in 750th epoch is 10.998090543596001\n",
            "loss for all sentences in 751th epoch is 10.998069220937081\n",
            "loss for all sentences in 752th epoch is 10.998047908369358\n",
            "loss for all sentences in 753th epoch is 10.998026605887485\n",
            "loss for all sentences in 754th epoch is 10.998005313486129\n",
            "loss for all sentences in 755th epoch is 10.997984031159934\n",
            "loss for all sentences in 756th epoch is 10.99796275890356\n",
            "loss for all sentences in 757th epoch is 10.997941496711663\n",
            "loss for all sentences in 758th epoch is 10.997920244578896\n",
            "loss for all sentences in 759th epoch is 10.997899002499917\n",
            "loss for all sentences in 760th epoch is 10.997877770469378\n",
            "loss for all sentences in 761th epoch is 10.997856548481936\n",
            "loss for all sentences in 762th epoch is 10.997835336532246\n",
            "loss for all sentences in 763th epoch is 10.997814134614961\n",
            "loss for all sentences in 764th epoch is 10.997792942724738\n",
            "loss for all sentences in 765th epoch is 10.997771760856228\n",
            "loss for all sentences in 766th epoch is 10.997750589004088\n",
            "loss for all sentences in 767th epoch is 10.997729427162975\n",
            "loss for all sentences in 768th epoch is 10.99770827532754\n",
            "loss for all sentences in 769th epoch is 10.997687133492438\n",
            "loss for all sentences in 770th epoch is 10.997666001652325\n",
            "loss for all sentences in 771th epoch is 10.997644879801854\n",
            "loss for all sentences in 772th epoch is 10.997623767935675\n",
            "loss for all sentences in 773th epoch is 10.99760266604845\n",
            "loss for all sentences in 774th epoch is 10.997581574134836\n",
            "loss for all sentences in 775th epoch is 10.997560492189473\n",
            "loss for all sentences in 776th epoch is 10.997539420207026\n",
            "loss for all sentences in 777th epoch is 10.997518358182148\n",
            "loss for all sentences in 778th epoch is 10.997497306109489\n",
            "loss for all sentences in 779th epoch is 10.99747626398371\n",
            "loss for all sentences in 780th epoch is 10.99745523179946\n",
            "loss for all sentences in 781th epoch is 10.997434209551393\n",
            "loss for all sentences in 782th epoch is 10.997413197234167\n",
            "loss for all sentences in 783th epoch is 10.997392194842433\n",
            "loss for all sentences in 784th epoch is 10.997371202370843\n",
            "loss for all sentences in 785th epoch is 10.997350219814056\n",
            "loss for all sentences in 786th epoch is 10.997329247166727\n",
            "loss for all sentences in 787th epoch is 10.997308284423504\n",
            "loss for all sentences in 788th epoch is 10.997287331579045\n",
            "loss for all sentences in 789th epoch is 10.99726638862801\n",
            "loss for all sentences in 790th epoch is 10.997245455565043\n",
            "loss for all sentences in 791th epoch is 10.997224532384799\n",
            "loss for all sentences in 792th epoch is 10.997203619081942\n",
            "loss for all sentences in 793th epoch is 10.997182715651121\n",
            "loss for all sentences in 794th epoch is 10.997161822086989\n",
            "loss for all sentences in 795th epoch is 10.997140938384202\n",
            "loss for all sentences in 796th epoch is 10.997120064537416\n",
            "loss for all sentences in 797th epoch is 10.99709920054128\n",
            "loss for all sentences in 798th epoch is 10.997078346390458\n",
            "loss for all sentences in 799th epoch is 10.997057502079597\n",
            "loss for all sentences in 800th epoch is 10.997036667603354\n",
            "loss for all sentences in 801th epoch is 10.997015842956387\n",
            "loss for all sentences in 802th epoch is 10.99699502813335\n",
            "loss for all sentences in 803th epoch is 10.996974223128895\n",
            "loss for all sentences in 804th epoch is 10.996953427937676\n",
            "loss for all sentences in 805th epoch is 10.996932642554356\n",
            "loss for all sentences in 806th epoch is 10.996911866973582\n",
            "loss for all sentences in 807th epoch is 10.996891101190014\n",
            "loss for all sentences in 808th epoch is 10.996870345198307\n",
            "loss for all sentences in 809th epoch is 10.996849598993114\n",
            "loss for all sentences in 810th epoch is 10.996828862569096\n",
            "loss for all sentences in 811th epoch is 10.996808135920906\n",
            "loss for all sentences in 812th epoch is 10.9967874190432\n",
            "loss for all sentences in 813th epoch is 10.996766711930633\n",
            "loss for all sentences in 814th epoch is 10.996746014577862\n",
            "loss for all sentences in 815th epoch is 10.996725326979544\n",
            "loss for all sentences in 816th epoch is 10.996704649130335\n",
            "loss for all sentences in 817th epoch is 10.99668398102489\n",
            "loss for all sentences in 818th epoch is 10.99666332265787\n",
            "loss for all sentences in 819th epoch is 10.996642674023928\n",
            "loss for all sentences in 820th epoch is 10.996622035117719\n",
            "loss for all sentences in 821th epoch is 10.996601405933905\n",
            "loss for all sentences in 822th epoch is 10.996580786467144\n",
            "loss for all sentences in 823th epoch is 10.996560176712089\n",
            "loss for all sentences in 824th epoch is 10.9965395766634\n",
            "loss for all sentences in 825th epoch is 10.996518986315733\n",
            "loss for all sentences in 826th epoch is 10.996498405663745\n",
            "loss for all sentences in 827th epoch is 10.996477834702098\n",
            "loss for all sentences in 828th epoch is 10.996457273425449\n",
            "loss for all sentences in 829th epoch is 10.996436721828456\n",
            "loss for all sentences in 830th epoch is 10.996416179905772\n",
            "loss for all sentences in 831th epoch is 10.996395647652065\n",
            "loss for all sentences in 832th epoch is 10.996375125061988\n",
            "loss for all sentences in 833th epoch is 10.996354612130201\n",
            "loss for all sentences in 834th epoch is 10.996334108851366\n",
            "loss for all sentences in 835th epoch is 10.996313615220135\n",
            "loss for all sentences in 836th epoch is 10.996293131231178\n",
            "loss for all sentences in 837th epoch is 10.996272656879144\n",
            "loss for all sentences in 838th epoch is 10.9962521921587\n",
            "loss for all sentences in 839th epoch is 10.996231737064504\n",
            "loss for all sentences in 840th epoch is 10.996211291591218\n",
            "loss for all sentences in 841th epoch is 10.996190855733497\n",
            "loss for all sentences in 842th epoch is 10.996170429486009\n",
            "loss for all sentences in 843th epoch is 10.996150012843412\n",
            "loss for all sentences in 844th epoch is 10.996129605800363\n",
            "loss for all sentences in 845th epoch is 10.996109208351527\n",
            "loss for all sentences in 846th epoch is 10.996088820491568\n",
            "loss for all sentences in 847th epoch is 10.996068442215144\n",
            "loss for all sentences in 848th epoch is 10.996048073516919\n",
            "loss for all sentences in 849th epoch is 10.996027714391554\n",
            "loss for all sentences in 850th epoch is 10.996007364833709\n",
            "loss for all sentences in 851th epoch is 10.995987024838051\n",
            "loss for all sentences in 852th epoch is 10.99596669439924\n",
            "loss for all sentences in 853th epoch is 10.995946373511941\n",
            "loss for all sentences in 854th epoch is 10.995926062170813\n",
            "loss for all sentences in 855th epoch is 10.995905760370528\n",
            "loss for all sentences in 856th epoch is 10.995885468105742\n",
            "loss for all sentences in 857th epoch is 10.995865185371121\n",
            "loss for all sentences in 858th epoch is 10.99584491216133\n",
            "loss for all sentences in 859th epoch is 10.995824648471032\n",
            "loss for all sentences in 860th epoch is 10.995804394294895\n",
            "loss for all sentences in 861th epoch is 10.995784149627578\n",
            "loss for all sentences in 862th epoch is 10.995763914463751\n",
            "loss for all sentences in 863th epoch is 10.995743688798079\n",
            "loss for all sentences in 864th epoch is 10.995723472625226\n",
            "loss for all sentences in 865th epoch is 10.995703265939861\n",
            "loss for all sentences in 866th epoch is 10.995683068736646\n",
            "loss for all sentences in 867th epoch is 10.99566288101025\n",
            "loss for all sentences in 868th epoch is 10.995642702755342\n",
            "loss for all sentences in 869th epoch is 10.995622533966582\n",
            "loss for all sentences in 870th epoch is 10.995602374638644\n",
            "loss for all sentences in 871th epoch is 10.995582224766192\n",
            "loss for all sentences in 872th epoch is 10.995562084343893\n",
            "loss for all sentences in 873th epoch is 10.995541953366422\n",
            "loss for all sentences in 874th epoch is 10.995521831828436\n",
            "loss for all sentences in 875th epoch is 10.995501719724613\n",
            "loss for all sentences in 876th epoch is 10.995481617049618\n",
            "loss for all sentences in 877th epoch is 10.99546152379812\n",
            "loss for all sentences in 878th epoch is 10.995441439964791\n",
            "loss for all sentences in 879th epoch is 10.9954213655443\n",
            "loss for all sentences in 880th epoch is 10.995401300531313\n",
            "loss for all sentences in 881th epoch is 10.995381244920505\n",
            "loss for all sentences in 882th epoch is 10.995361198706545\n",
            "loss for all sentences in 883th epoch is 10.995341161884102\n",
            "loss for all sentences in 884th epoch is 10.99532113444785\n",
            "loss for all sentences in 885th epoch is 10.995301116392458\n",
            "loss for all sentences in 886th epoch is 10.995281107712604\n",
            "loss for all sentences in 887th epoch is 10.99526110840295\n",
            "loss for all sentences in 888th epoch is 10.995241118458175\n",
            "loss for all sentences in 889th epoch is 10.995221137872951\n",
            "loss for all sentences in 890th epoch is 10.995201166641952\n",
            "loss for all sentences in 891th epoch is 10.995181204759849\n",
            "loss for all sentences in 892th epoch is 10.995161252221315\n",
            "loss for all sentences in 893th epoch is 10.995141309021024\n",
            "loss for all sentences in 894th epoch is 10.995121375153653\n",
            "loss for all sentences in 895th epoch is 10.995101450613875\n",
            "loss for all sentences in 896th epoch is 10.995081535396366\n",
            "loss for all sentences in 897th epoch is 10.995061629495797\n",
            "loss for all sentences in 898th epoch is 10.995041732906849\n",
            "loss for all sentences in 899th epoch is 10.995021845624198\n",
            "loss for all sentences in 900th epoch is 10.995001967642512\n",
            "loss for all sentences in 901th epoch is 10.994982098956477\n",
            "loss for all sentences in 902th epoch is 10.994962239560762\n",
            "loss for all sentences in 903th epoch is 10.99494238945005\n",
            "loss for all sentences in 904th epoch is 10.994922548619018\n",
            "loss for all sentences in 905th epoch is 10.99490271706234\n",
            "loss for all sentences in 906th epoch is 10.994882894774697\n",
            "loss for all sentences in 907th epoch is 10.994863081750767\n",
            "loss for all sentences in 908th epoch is 10.994843277985225\n",
            "loss for all sentences in 909th epoch is 10.994823483472757\n",
            "loss for all sentences in 910th epoch is 10.99480369820804\n",
            "loss for all sentences in 911th epoch is 10.994783922185754\n",
            "loss for all sentences in 912th epoch is 10.994764155400572\n",
            "loss for all sentences in 913th epoch is 10.994744397847187\n",
            "loss for all sentences in 914th epoch is 10.99472464952027\n",
            "loss for all sentences in 915th epoch is 10.994704910414505\n",
            "loss for all sentences in 916th epoch is 10.994685180524575\n",
            "loss for all sentences in 917th epoch is 10.994665459845162\n",
            "loss for all sentences in 918th epoch is 10.994645748370948\n",
            "loss for all sentences in 919th epoch is 10.994626046096613\n",
            "loss for all sentences in 920th epoch is 10.994606353016845\n",
            "loss for all sentences in 921th epoch is 10.99458666912632\n",
            "loss for all sentences in 922th epoch is 10.99456699441973\n",
            "loss for all sentences in 923th epoch is 10.99454732889175\n",
            "loss for all sentences in 924th epoch is 10.994527672537071\n",
            "loss for all sentences in 925th epoch is 10.99450802535038\n",
            "loss for all sentences in 926th epoch is 10.994488387326353\n",
            "loss for all sentences in 927th epoch is 10.994468758459682\n",
            "loss for all sentences in 928th epoch is 10.994449138745052\n",
            "loss for all sentences in 929th epoch is 10.994429528177148\n",
            "loss for all sentences in 930th epoch is 10.994409926750661\n",
            "loss for all sentences in 931th epoch is 10.99439033446027\n",
            "loss for all sentences in 932th epoch is 10.994370751300668\n",
            "loss for all sentences in 933th epoch is 10.99435117726654\n",
            "loss for all sentences in 934th epoch is 10.994331612352576\n",
            "loss for all sentences in 935th epoch is 10.994312056553461\n",
            "loss for all sentences in 936th epoch is 10.99429250986389\n",
            "loss for all sentences in 937th epoch is 10.994272972278544\n",
            "loss for all sentences in 938th epoch is 10.994253443792122\n",
            "loss for all sentences in 939th epoch is 10.994233924399305\n",
            "loss for all sentences in 940th epoch is 10.994214414094788\n",
            "loss for all sentences in 941th epoch is 10.994194912873262\n",
            "loss for all sentences in 942th epoch is 10.994175420729416\n",
            "loss for all sentences in 943th epoch is 10.994155937657943\n",
            "loss for all sentences in 944th epoch is 10.994136463653534\n",
            "loss for all sentences in 945th epoch is 10.994116998710883\n",
            "loss for all sentences in 946th epoch is 10.99409754282468\n",
            "loss for all sentences in 947th epoch is 10.99407809598962\n",
            "loss for all sentences in 948th epoch is 10.994058658200393\n",
            "loss for all sentences in 949th epoch is 10.9940392294517\n",
            "loss for all sentences in 950th epoch is 10.994019809738226\n",
            "loss for all sentences in 951th epoch is 10.994000399054672\n",
            "loss for all sentences in 952th epoch is 10.99398099739573\n",
            "loss for all sentences in 953th epoch is 10.993961604756096\n",
            "loss for all sentences in 954th epoch is 10.993942221130466\n",
            "loss for all sentences in 955th epoch is 10.993922846513538\n",
            "loss for all sentences in 956th epoch is 10.993903480900004\n",
            "loss for all sentences in 957th epoch is 10.993884124284566\n",
            "loss for all sentences in 958th epoch is 10.993864776661916\n",
            "loss for all sentences in 959th epoch is 10.993845438026757\n",
            "loss for all sentences in 960th epoch is 10.99382610837378\n",
            "loss for all sentences in 961th epoch is 10.99380678769769\n",
            "loss for all sentences in 962th epoch is 10.993787475993187\n",
            "loss for all sentences in 963th epoch is 10.993768173254965\n",
            "loss for all sentences in 964th epoch is 10.993748879477728\n",
            "loss for all sentences in 965th epoch is 10.993729594656173\n",
            "loss for all sentences in 966th epoch is 10.993710318785002\n",
            "loss for all sentences in 967th epoch is 10.993691051858915\n",
            "loss for all sentences in 968th epoch is 10.993671793872615\n",
            "loss for all sentences in 969th epoch is 10.993652544820801\n",
            "loss for all sentences in 970th epoch is 10.993633304698177\n",
            "loss for all sentences in 971th epoch is 10.993614073499446\n",
            "loss for all sentences in 972th epoch is 10.993594851219308\n",
            "loss for all sentences in 973th epoch is 10.99357563785247\n",
            "loss for all sentences in 974th epoch is 10.993556433393634\n",
            "loss for all sentences in 975th epoch is 10.993537237837506\n",
            "loss for all sentences in 976th epoch is 10.993518051178789\n",
            "loss for all sentences in 977th epoch is 10.993498873412188\n",
            "loss for all sentences in 978th epoch is 10.993479704532406\n",
            "loss for all sentences in 979th epoch is 10.993460544534154\n",
            "loss for all sentences in 980th epoch is 10.993441393412134\n",
            "loss for all sentences in 981th epoch is 10.993422251161055\n",
            "loss for all sentences in 982th epoch is 10.993403117775623\n",
            "loss for all sentences in 983th epoch is 10.993383993250546\n",
            "loss for all sentences in 984th epoch is 10.993364877580532\n",
            "loss for all sentences in 985th epoch is 10.993345770760289\n",
            "loss for all sentences in 986th epoch is 10.993326672784525\n",
            "loss for all sentences in 987th epoch is 10.993307583647953\n",
            "loss for all sentences in 988th epoch is 10.993288503345275\n",
            "loss for all sentences in 989th epoch is 10.993269431871207\n",
            "loss for all sentences in 990th epoch is 10.993250369220458\n",
            "loss for all sentences in 991th epoch is 10.993231315387739\n",
            "loss for all sentences in 992th epoch is 10.99321227036776\n",
            "loss for all sentences in 993th epoch is 10.993193234155232\n",
            "loss for all sentences in 994th epoch is 10.993174206744873\n",
            "loss for all sentences in 995th epoch is 10.993155188131388\n",
            "loss for all sentences in 996th epoch is 10.993136178309495\n",
            "loss for all sentences in 997th epoch is 10.993117177273906\n",
            "loss for all sentences in 998th epoch is 10.993098185019331\n",
            "loss for all sentences in 999th epoch is 10.993079201540489\n",
            "loss for all sentences in 1000th epoch is 10.993060226832092\n",
            "loss for all sentences in 1001th epoch is 10.993041260888859\n",
            "loss for all sentences in 1002th epoch is 10.993022303705501\n",
            "loss for all sentences in 1003th epoch is 10.993003355276734\n",
            "loss for all sentences in 1004th epoch is 10.99298441559728\n",
            "loss for all sentences in 1005th epoch is 10.992965484661845\n",
            "loss for all sentences in 1006th epoch is 10.992946562465155\n",
            "loss for all sentences in 1007th epoch is 10.992927649001924\n",
            "loss for all sentences in 1008th epoch is 10.992908744266877\n",
            "loss for all sentences in 1009th epoch is 10.992889848254721\n",
            "loss for all sentences in 1010th epoch is 10.992870960960184\n",
            "loss for all sentences in 1011th epoch is 10.99285208237798\n",
            "loss for all sentences in 1012th epoch is 10.99283321250283\n",
            "loss for all sentences in 1013th epoch is 10.992814351329457\n",
            "loss for all sentences in 1014th epoch is 10.992795498852576\n",
            "loss for all sentences in 1015th epoch is 10.992776655066915\n",
            "loss for all sentences in 1016th epoch is 10.992757819967188\n",
            "loss for all sentences in 1017th epoch is 10.992738993548125\n",
            "loss for all sentences in 1018th epoch is 10.99272017580444\n",
            "loss for all sentences in 1019th epoch is 10.99270136673086\n",
            "loss for all sentences in 1020th epoch is 10.992682566322111\n",
            "loss for all sentences in 1021th epoch is 10.992663774572911\n",
            "loss for all sentences in 1022th epoch is 10.992644991477986\n",
            "loss for all sentences in 1023th epoch is 10.99262621703206\n",
            "loss for all sentences in 1024th epoch is 10.992607451229858\n",
            "loss for all sentences in 1025th epoch is 10.992588694066107\n",
            "loss for all sentences in 1026th epoch is 10.992569945535534\n",
            "loss for all sentences in 1027th epoch is 10.99255120563286\n",
            "loss for all sentences in 1028th epoch is 10.992532474352814\n",
            "loss for all sentences in 1029th epoch is 10.992513751690128\n",
            "loss for all sentences in 1030th epoch is 10.992495037639522\n",
            "loss for all sentences in 1031th epoch is 10.992476332195722\n",
            "loss for all sentences in 1032th epoch is 10.992457635353464\n",
            "loss for all sentences in 1033th epoch is 10.992438947107477\n",
            "loss for all sentences in 1034th epoch is 10.992420267452484\n",
            "loss for all sentences in 1035th epoch is 10.99240159638322\n",
            "loss for all sentences in 1036th epoch is 10.99238293389441\n",
            "loss for all sentences in 1037th epoch is 10.992364279980787\n",
            "loss for all sentences in 1038th epoch is 10.992345634637083\n",
            "loss for all sentences in 1039th epoch is 10.992326997858028\n",
            "loss for all sentences in 1040th epoch is 10.992308369638355\n",
            "loss for all sentences in 1041th epoch is 10.992289749972795\n",
            "loss for all sentences in 1042th epoch is 10.99227113885608\n",
            "loss for all sentences in 1043th epoch is 10.99225253628294\n",
            "loss for all sentences in 1044th epoch is 10.992233942248117\n",
            "loss for all sentences in 1045th epoch is 10.992215356746335\n",
            "loss for all sentences in 1046th epoch is 10.992196779772339\n",
            "loss for all sentences in 1047th epoch is 10.992178211320859\n",
            "loss for all sentences in 1048th epoch is 10.992159651386624\n",
            "loss for all sentences in 1049th epoch is 10.992141099964376\n",
            "loss for all sentences in 1050th epoch is 10.992122557048853\n",
            "loss for all sentences in 1051th epoch is 10.992104022634788\n",
            "loss for all sentences in 1052th epoch is 10.992085496716918\n",
            "loss for all sentences in 1053th epoch is 10.992066979289978\n",
            "loss for all sentences in 1054th epoch is 10.992048470348712\n",
            "loss for all sentences in 1055th epoch is 10.992029969887852\n",
            "loss for all sentences in 1056th epoch is 10.992011477902143\n",
            "loss for all sentences in 1057th epoch is 10.991992994386317\n",
            "loss for all sentences in 1058th epoch is 10.991974519335121\n",
            "loss for all sentences in 1059th epoch is 10.991956052743285\n",
            "loss for all sentences in 1060th epoch is 10.991937594605558\n",
            "loss for all sentences in 1061th epoch is 10.991919144916675\n",
            "loss for all sentences in 1062th epoch is 10.991900703671385\n",
            "loss for all sentences in 1063th epoch is 10.991882270864425\n",
            "loss for all sentences in 1064th epoch is 10.991863846490531\n",
            "loss for all sentences in 1065th epoch is 10.991845430544455\n",
            "loss for all sentences in 1066th epoch is 10.991827023020937\n",
            "loss for all sentences in 1067th epoch is 10.99180862391472\n",
            "loss for all sentences in 1068th epoch is 10.991790233220545\n",
            "loss for all sentences in 1069th epoch is 10.991771850933155\n",
            "loss for all sentences in 1070th epoch is 10.9917534770473\n",
            "loss for all sentences in 1071th epoch is 10.991735111557727\n",
            "loss for all sentences in 1072th epoch is 10.991716754459175\n",
            "loss for all sentences in 1073th epoch is 10.991698405746392\n",
            "loss for all sentences in 1074th epoch is 10.991680065414124\n",
            "loss for all sentences in 1075th epoch is 10.991661733457121\n",
            "loss for all sentences in 1076th epoch is 10.991643409870125\n",
            "loss for all sentences in 1077th epoch is 10.991625094647887\n",
            "loss for all sentences in 1078th epoch is 10.991606787785154\n",
            "loss for all sentences in 1079th epoch is 10.991588489276674\n",
            "loss for all sentences in 1080th epoch is 10.991570199117195\n",
            "loss for all sentences in 1081th epoch is 10.99155191730147\n",
            "loss for all sentences in 1082th epoch is 10.991533643824242\n",
            "loss for all sentences in 1083th epoch is 10.99151537868027\n",
            "loss for all sentences in 1084th epoch is 10.991497121864299\n",
            "loss for all sentences in 1085th epoch is 10.991478873371083\n",
            "loss for all sentences in 1086th epoch is 10.991460633195366\n",
            "loss for all sentences in 1087th epoch is 10.99144240133191\n",
            "loss for all sentences in 1088th epoch is 10.991424177775459\n",
            "loss for all sentences in 1089th epoch is 10.991405962520771\n",
            "loss for all sentences in 1090th epoch is 10.991387755562595\n",
            "loss for all sentences in 1091th epoch is 10.991369556895688\n",
            "loss for all sentences in 1092th epoch is 10.9913513665148\n",
            "loss for all sentences in 1093th epoch is 10.991333184414689\n",
            "loss for all sentences in 1094th epoch is 10.991315010590107\n",
            "loss for all sentences in 1095th epoch is 10.991296845035812\n",
            "loss for all sentences in 1096th epoch is 10.991278687746558\n",
            "loss for all sentences in 1097th epoch is 10.991260538717102\n",
            "loss for all sentences in 1098th epoch is 10.991242397942194\n",
            "loss for all sentences in 1099th epoch is 10.991224265416603\n",
            "loss for all sentences in 1100th epoch is 10.991206141135077\n",
            "loss for all sentences in 1101th epoch is 10.991188025092372\n",
            "loss for all sentences in 1102th epoch is 10.991169917283253\n",
            "loss for all sentences in 1103th epoch is 10.991151817702477\n",
            "loss for all sentences in 1104th epoch is 10.991133726344795\n",
            "loss for all sentences in 1105th epoch is 10.991115643204978\n",
            "loss for all sentences in 1106th epoch is 10.991097568277779\n",
            "loss for all sentences in 1107th epoch is 10.991079501557959\n",
            "loss for all sentences in 1108th epoch is 10.991061443040277\n",
            "loss for all sentences in 1109th epoch is 10.991043392719497\n",
            "loss for all sentences in 1110th epoch is 10.99102535059038\n",
            "loss for all sentences in 1111th epoch is 10.991007316647686\n",
            "loss for all sentences in 1112th epoch is 10.990989290886176\n",
            "loss for all sentences in 1113th epoch is 10.990971273300616\n",
            "loss for all sentences in 1114th epoch is 10.990953263885766\n",
            "loss for all sentences in 1115th epoch is 10.99093526263639\n",
            "loss for all sentences in 1116th epoch is 10.990917269547253\n",
            "loss for all sentences in 1117th epoch is 10.990899284613121\n",
            "loss for all sentences in 1118th epoch is 10.990881307828753\n",
            "loss for all sentences in 1119th epoch is 10.990863339188916\n",
            "loss for all sentences in 1120th epoch is 10.990845378688379\n",
            "loss for all sentences in 1121th epoch is 10.990827426321903\n",
            "loss for all sentences in 1122th epoch is 10.990809482084256\n",
            "loss for all sentences in 1123th epoch is 10.990791545970207\n",
            "loss for all sentences in 1124th epoch is 10.990773617974517\n",
            "loss for all sentences in 1125th epoch is 10.990755698091958\n",
            "loss for all sentences in 1126th epoch is 10.990737786317297\n",
            "loss for all sentences in 1127th epoch is 10.990719882645303\n",
            "loss for all sentences in 1128th epoch is 10.990701987070743\n",
            "loss for all sentences in 1129th epoch is 10.990684099588385\n",
            "loss for all sentences in 1130th epoch is 10.990666220193\n",
            "loss for all sentences in 1131th epoch is 10.990648348879358\n",
            "loss for all sentences in 1132th epoch is 10.990630485642228\n",
            "loss for all sentences in 1133th epoch is 10.990612630476383\n",
            "loss for all sentences in 1134th epoch is 10.990594783376588\n",
            "loss for all sentences in 1135th epoch is 10.99057694433762\n",
            "loss for all sentences in 1136th epoch is 10.99055911335425\n",
            "loss for all sentences in 1137th epoch is 10.990541290421248\n",
            "loss for all sentences in 1138th epoch is 10.990523475533386\n",
            "loss for all sentences in 1139th epoch is 10.99050566868544\n",
            "loss for all sentences in 1140th epoch is 10.99048786987218\n",
            "loss for all sentences in 1141th epoch is 10.99047007908838\n",
            "loss for all sentences in 1142th epoch is 10.990452296328815\n",
            "loss for all sentences in 1143th epoch is 10.990434521588263\n",
            "loss for all sentences in 1144th epoch is 10.990416754861494\n",
            "loss for all sentences in 1145th epoch is 10.990398996143282\n",
            "loss for all sentences in 1146th epoch is 10.990381245428406\n",
            "loss for all sentences in 1147th epoch is 10.990363502711642\n",
            "loss for all sentences in 1148th epoch is 10.99034576798776\n",
            "loss for all sentences in 1149th epoch is 10.990328041251546\n",
            "loss for all sentences in 1150th epoch is 10.990310322497773\n",
            "loss for all sentences in 1151th epoch is 10.990292611721216\n",
            "loss for all sentences in 1152th epoch is 10.990274908916657\n",
            "loss for all sentences in 1153th epoch is 10.99025721407887\n",
            "loss for all sentences in 1154th epoch is 10.990239527202634\n",
            "loss for all sentences in 1155th epoch is 10.990221848282735\n",
            "loss for all sentences in 1156th epoch is 10.990204177313942\n",
            "loss for all sentences in 1157th epoch is 10.990186514291041\n",
            "loss for all sentences in 1158th epoch is 10.990168859208811\n",
            "loss for all sentences in 1159th epoch is 10.990151212062033\n",
            "loss for all sentences in 1160th epoch is 10.990133572845483\n",
            "loss for all sentences in 1161th epoch is 10.99011594155395\n",
            "loss for all sentences in 1162th epoch is 10.99009831818221\n",
            "loss for all sentences in 1163th epoch is 10.990080702725047\n",
            "loss for all sentences in 1164th epoch is 10.990063095177241\n",
            "loss for all sentences in 1165th epoch is 10.990045495533577\n",
            "loss for all sentences in 1166th epoch is 10.990027903788837\n",
            "loss for all sentences in 1167th epoch is 10.990010319937804\n",
            "loss for all sentences in 1168th epoch is 10.989992743975261\n",
            "loss for all sentences in 1169th epoch is 10.989975175895994\n",
            "loss for all sentences in 1170th epoch is 10.989957615694786\n",
            "loss for all sentences in 1171th epoch is 10.989940063366426\n",
            "loss for all sentences in 1172th epoch is 10.989922518905692\n",
            "loss for all sentences in 1173th epoch is 10.989904982307374\n",
            "loss for all sentences in 1174th epoch is 10.989887453566258\n",
            "loss for all sentences in 1175th epoch is 10.989869932677129\n",
            "loss for all sentences in 1176th epoch is 10.989852419634769\n",
            "loss for all sentences in 1177th epoch is 10.989834914433976\n",
            "loss for all sentences in 1178th epoch is 10.989817417069528\n",
            "loss for all sentences in 1179th epoch is 10.989799927536213\n",
            "loss for all sentences in 1180th epoch is 10.989782445828823\n",
            "loss for all sentences in 1181th epoch is 10.989764971942144\n",
            "loss for all sentences in 1182th epoch is 10.989747505870966\n",
            "loss for all sentences in 1183th epoch is 10.989730047610077\n",
            "loss for all sentences in 1184th epoch is 10.989712597154268\n",
            "loss for all sentences in 1185th epoch is 10.98969515449833\n",
            "loss for all sentences in 1186th epoch is 10.989677719637045\n",
            "loss for all sentences in 1187th epoch is 10.98966029256521\n",
            "loss for all sentences in 1188th epoch is 10.989642873277614\n",
            "loss for all sentences in 1189th epoch is 10.98962546176905\n",
            "loss for all sentences in 1190th epoch is 10.989608058034303\n",
            "loss for all sentences in 1191th epoch is 10.989590662068176\n",
            "loss for all sentences in 1192th epoch is 10.98957327386545\n",
            "loss for all sentences in 1193th epoch is 10.989555893420926\n",
            "loss for all sentences in 1194th epoch is 10.98953852072939\n",
            "loss for all sentences in 1195th epoch is 10.989521155785638\n",
            "loss for all sentences in 1196th epoch is 10.989503798584465\n",
            "loss for all sentences in 1197th epoch is 10.989486449120662\n",
            "loss for all sentences in 1198th epoch is 10.989469107389022\n",
            "loss for all sentences in 1199th epoch is 10.989451773384344\n",
            "loss for all sentences in 1200th epoch is 10.989434447101418\n",
            "loss for all sentences in 1201th epoch is 10.989417128535042\n",
            "loss for all sentences in 1202th epoch is 10.98939981768001\n",
            "loss for all sentences in 1203th epoch is 10.989382514531119\n",
            "loss for all sentences in 1204th epoch is 10.989365219083163\n",
            "loss for all sentences in 1205th epoch is 10.989347931330943\n",
            "loss for all sentences in 1206th epoch is 10.989330651269247\n",
            "loss for all sentences in 1207th epoch is 10.98931337889288\n",
            "loss for all sentences in 1208th epoch is 10.989296114196637\n",
            "loss for all sentences in 1209th epoch is 10.989278857175314\n",
            "loss for all sentences in 1210th epoch is 10.989261607823709\n",
            "loss for all sentences in 1211th epoch is 10.989244366136623\n",
            "loss for all sentences in 1212th epoch is 10.989227132108851\n",
            "loss for all sentences in 1213th epoch is 10.989209905735192\n",
            "loss for all sentences in 1214th epoch is 10.989192687010451\n",
            "loss for all sentences in 1215th epoch is 10.98917547592942\n",
            "loss for all sentences in 1216th epoch is 10.989158272486902\n",
            "loss for all sentences in 1217th epoch is 10.989141076677697\n",
            "loss for all sentences in 1218th epoch is 10.989123888496607\n",
            "loss for all sentences in 1219th epoch is 10.989106707938431\n",
            "loss for all sentences in 1220th epoch is 10.989089534997966\n",
            "loss for all sentences in 1221th epoch is 10.989072369670023\n",
            "loss for all sentences in 1222th epoch is 10.989055211949394\n",
            "loss for all sentences in 1223th epoch is 10.989038061830888\n",
            "loss for all sentences in 1224th epoch is 10.9890209193093\n",
            "loss for all sentences in 1225th epoch is 10.989003784379438\n",
            "loss for all sentences in 1226th epoch is 10.988986657036106\n",
            "loss for all sentences in 1227th epoch is 10.988969537274102\n",
            "loss for all sentences in 1228th epoch is 10.988952425088232\n",
            "loss for all sentences in 1229th epoch is 10.988935320473301\n",
            "loss for all sentences in 1230th epoch is 10.98891822342411\n",
            "loss for all sentences in 1231th epoch is 10.988901133935464\n",
            "loss for all sentences in 1232th epoch is 10.988884052002167\n",
            "loss for all sentences in 1233th epoch is 10.98886697761903\n",
            "loss for all sentences in 1234th epoch is 10.988849910780846\n",
            "loss for all sentences in 1235th epoch is 10.988832851482433\n",
            "loss for all sentences in 1236th epoch is 10.988815799718587\n",
            "loss for all sentences in 1237th epoch is 10.988798755484122\n",
            "loss for all sentences in 1238th epoch is 10.988781718773836\n",
            "loss for all sentences in 1239th epoch is 10.988764689582545\n",
            "loss for all sentences in 1240th epoch is 10.988747667905047\n",
            "loss for all sentences in 1241th epoch is 10.988730653736152\n",
            "loss for all sentences in 1242th epoch is 10.98871364707067\n",
            "loss for all sentences in 1243th epoch is 10.988696647903403\n",
            "loss for all sentences in 1244th epoch is 10.988679656229163\n",
            "loss for all sentences in 1245th epoch is 10.98866267204276\n",
            "loss for all sentences in 1246th epoch is 10.988645695338999\n",
            "loss for all sentences in 1247th epoch is 10.988628726112688\n",
            "loss for all sentences in 1248th epoch is 10.98861176435864\n",
            "loss for all sentences in 1249th epoch is 10.988594810071659\n",
            "loss for all sentences in 1250th epoch is 10.988577863246558\n",
            "loss for all sentences in 1251th epoch is 10.988560923878147\n",
            "loss for all sentences in 1252th epoch is 10.988543991961233\n",
            "loss for all sentences in 1253th epoch is 10.988527067490626\n",
            "loss for all sentences in 1254th epoch is 10.988510150461142\n",
            "loss for all sentences in 1255th epoch is 10.988493240867587\n",
            "loss for all sentences in 1256th epoch is 10.988476338704773\n",
            "loss for all sentences in 1257th epoch is 10.98845944396751\n",
            "loss for all sentences in 1258th epoch is 10.988442556650611\n",
            "loss for all sentences in 1259th epoch is 10.98842567674889\n",
            "loss for all sentences in 1260th epoch is 10.988408804257157\n",
            "loss for all sentences in 1261th epoch is 10.988391939170222\n",
            "loss for all sentences in 1262th epoch is 10.988375081482896\n",
            "loss for all sentences in 1263th epoch is 10.988358231189999\n",
            "loss for all sentences in 1264th epoch is 10.98834138828634\n",
            "loss for all sentences in 1265th epoch is 10.988324552766732\n",
            "loss for all sentences in 1266th epoch is 10.988307724625988\n",
            "loss for all sentences in 1267th epoch is 10.988290903858925\n",
            "loss for all sentences in 1268th epoch is 10.98827409046035\n",
            "loss for all sentences in 1269th epoch is 10.988257284425083\n",
            "loss for all sentences in 1270th epoch is 10.988240485747935\n",
            "loss for all sentences in 1271th epoch is 10.988223694423725\n",
            "loss for all sentences in 1272th epoch is 10.988206910447264\n",
            "loss for all sentences in 1273th epoch is 10.988190133813369\n",
            "loss for all sentences in 1274th epoch is 10.988173364516856\n",
            "loss for all sentences in 1275th epoch is 10.988156602552534\n",
            "loss for all sentences in 1276th epoch is 10.98813984791523\n",
            "loss for all sentences in 1277th epoch is 10.988123100599749\n",
            "loss for all sentences in 1278th epoch is 10.988106360600915\n",
            "loss for all sentences in 1279th epoch is 10.988089627913538\n",
            "loss for all sentences in 1280th epoch is 10.98807290253244\n",
            "loss for all sentences in 1281th epoch is 10.988056184452436\n",
            "loss for all sentences in 1282th epoch is 10.988039473668344\n",
            "loss for all sentences in 1283th epoch is 10.988022770174975\n",
            "loss for all sentences in 1284th epoch is 10.988006073967156\n",
            "loss for all sentences in 1285th epoch is 10.9879893850397\n",
            "loss for all sentences in 1286th epoch is 10.987972703387427\n",
            "loss for all sentences in 1287th epoch is 10.987956029005147\n",
            "loss for all sentences in 1288th epoch is 10.987939361887689\n",
            "loss for all sentences in 1289th epoch is 10.987922702029866\n",
            "loss for all sentences in 1290th epoch is 10.987906049426499\n",
            "loss for all sentences in 1291th epoch is 10.987889404072403\n",
            "loss for all sentences in 1292th epoch is 10.9878727659624\n",
            "loss for all sentences in 1293th epoch is 10.98785613509131\n",
            "loss for all sentences in 1294th epoch is 10.987839511453952\n",
            "loss for all sentences in 1295th epoch is 10.987822895045142\n",
            "loss for all sentences in 1296th epoch is 10.987806285859707\n",
            "loss for all sentences in 1297th epoch is 10.98778968389246\n",
            "loss for all sentences in 1298th epoch is 10.987773089138226\n",
            "loss for all sentences in 1299th epoch is 10.987756501591821\n",
            "loss for all sentences in 1300th epoch is 10.987739921248071\n",
            "loss for all sentences in 1301th epoch is 10.987723348101792\n",
            "loss for all sentences in 1302th epoch is 10.987706782147809\n",
            "loss for all sentences in 1303th epoch is 10.987690223380937\n",
            "loss for all sentences in 1304th epoch is 10.987673671796006\n",
            "loss for all sentences in 1305th epoch is 10.987657127387829\n",
            "loss for all sentences in 1306th epoch is 10.987640590151237\n",
            "loss for all sentences in 1307th epoch is 10.987624060081043\n",
            "loss for all sentences in 1308th epoch is 10.987607537172073\n",
            "loss for all sentences in 1309th epoch is 10.987591021419147\n",
            "loss for all sentences in 1310th epoch is 10.98757451281709\n",
            "loss for all sentences in 1311th epoch is 10.987558011360724\n",
            "loss for all sentences in 1312th epoch is 10.987541517044871\n",
            "loss for all sentences in 1313th epoch is 10.987525029864354\n",
            "loss for all sentences in 1314th epoch is 10.987508549813999\n",
            "loss for all sentences in 1315th epoch is 10.987492076888621\n",
            "loss for all sentences in 1316th epoch is 10.987475611083052\n",
            "loss for all sentences in 1317th epoch is 10.98745915239211\n",
            "loss for all sentences in 1318th epoch is 10.98744270081062\n",
            "loss for all sentences in 1319th epoch is 10.98742625633341\n",
            "loss for all sentences in 1320th epoch is 10.9874098189553\n",
            "loss for all sentences in 1321th epoch is 10.98739338867111\n",
            "loss for all sentences in 1322th epoch is 10.987376965475674\n",
            "loss for all sentences in 1323th epoch is 10.987360549363805\n",
            "loss for all sentences in 1324th epoch is 10.987344140330338\n",
            "loss for all sentences in 1325th epoch is 10.987327738370091\n",
            "loss for all sentences in 1326th epoch is 10.987311343477893\n",
            "loss for all sentences in 1327th epoch is 10.987294955648565\n",
            "loss for all sentences in 1328th epoch is 10.987278574876935\n",
            "loss for all sentences in 1329th epoch is 10.987262201157826\n",
            "loss for all sentences in 1330th epoch is 10.987245834486066\n",
            "loss for all sentences in 1331th epoch is 10.987229474856475\n",
            "loss for all sentences in 1332th epoch is 10.987213122263887\n",
            "loss for all sentences in 1333th epoch is 10.987196776703119\n",
            "loss for all sentences in 1334th epoch is 10.987180438169002\n",
            "loss for all sentences in 1335th epoch is 10.987164106656365\n",
            "loss for all sentences in 1336th epoch is 10.987147782160026\n",
            "loss for all sentences in 1337th epoch is 10.987131464674816\n",
            "loss for all sentences in 1338th epoch is 10.987115154195559\n",
            "loss for all sentences in 1339th epoch is 10.987098850717086\n",
            "loss for all sentences in 1340th epoch is 10.987082554234217\n",
            "loss for all sentences in 1341th epoch is 10.987066264741784\n",
            "loss for all sentences in 1342th epoch is 10.98704998223461\n",
            "loss for all sentences in 1343th epoch is 10.987033706707527\n",
            "loss for all sentences in 1344th epoch is 10.987017438155355\n",
            "loss for all sentences in 1345th epoch is 10.987001176572926\n",
            "loss for all sentences in 1346th epoch is 10.986984921955067\n",
            "loss for all sentences in 1347th epoch is 10.986968674296602\n",
            "loss for all sentences in 1348th epoch is 10.98695243359236\n",
            "loss for all sentences in 1349th epoch is 10.98693619983717\n",
            "loss for all sentences in 1350th epoch is 10.986919973025861\n",
            "loss for all sentences in 1351th epoch is 10.986903753153257\n",
            "loss for all sentences in 1352th epoch is 10.986887540214184\n",
            "loss for all sentences in 1353th epoch is 10.986871334203475\n",
            "loss for all sentences in 1354th epoch is 10.986855135115952\n",
            "loss for all sentences in 1355th epoch is 10.986838942946452\n",
            "loss for all sentences in 1356th epoch is 10.986822757689794\n",
            "loss for all sentences in 1357th epoch is 10.986806579340811\n",
            "loss for all sentences in 1358th epoch is 10.98679040789433\n",
            "loss for all sentences in 1359th epoch is 10.98677424334518\n",
            "loss for all sentences in 1360th epoch is 10.986758085688187\n",
            "loss for all sentences in 1361th epoch is 10.986741934918182\n",
            "loss for all sentences in 1362th epoch is 10.986725791029995\n",
            "loss for all sentences in 1363th epoch is 10.986709654018446\n",
            "loss for all sentences in 1364th epoch is 10.986693523878373\n",
            "loss for all sentences in 1365th epoch is 10.986677400604604\n",
            "loss for all sentences in 1366th epoch is 10.986661284191962\n",
            "loss for all sentences in 1367th epoch is 10.986645174635285\n",
            "loss for all sentences in 1368th epoch is 10.986629071929391\n",
            "loss for all sentences in 1369th epoch is 10.986612976069114\n",
            "loss for all sentences in 1370th epoch is 10.986596887049288\n",
            "loss for all sentences in 1371th epoch is 10.98658080486473\n",
            "loss for all sentences in 1372th epoch is 10.986564729510281\n",
            "loss for all sentences in 1373th epoch is 10.986548660980764\n",
            "loss for all sentences in 1374th epoch is 10.98653259927101\n",
            "loss for all sentences in 1375th epoch is 10.98651654437585\n",
            "loss for all sentences in 1376th epoch is 10.986500496290109\n",
            "loss for all sentences in 1377th epoch is 10.986484455008618\n",
            "loss for all sentences in 1378th epoch is 10.986468420526208\n",
            "loss for all sentences in 1379th epoch is 10.986452392837707\n",
            "loss for all sentences in 1380th epoch is 10.986436371937947\n",
            "loss for all sentences in 1381th epoch is 10.986420357821755\n",
            "loss for all sentences in 1382th epoch is 10.986404350483959\n",
            "loss for all sentences in 1383th epoch is 10.986388349919391\n",
            "loss for all sentences in 1384th epoch is 10.986372356122878\n",
            "loss for all sentences in 1385th epoch is 10.986356369089254\n",
            "loss for all sentences in 1386th epoch is 10.986340388813346\n",
            "loss for all sentences in 1387th epoch is 10.986324415289982\n",
            "loss for all sentences in 1388th epoch is 10.986308448513993\n",
            "loss for all sentences in 1389th epoch is 10.986292488480212\n",
            "loss for all sentences in 1390th epoch is 10.986276535183462\n",
            "loss for all sentences in 1391th epoch is 10.98626058861858\n",
            "loss for all sentences in 1392th epoch is 10.986244648780389\n",
            "loss for all sentences in 1393th epoch is 10.98622871566372\n",
            "loss for all sentences in 1394th epoch is 10.986212789263408\n",
            "loss for all sentences in 1395th epoch is 10.98619686957428\n",
            "loss for all sentences in 1396th epoch is 10.986180956591163\n",
            "loss for all sentences in 1397th epoch is 10.986165050308887\n",
            "loss for all sentences in 1398th epoch is 10.986149150722287\n",
            "loss for all sentences in 1399th epoch is 10.986133257826184\n",
            "loss for all sentences in 1400th epoch is 10.986117371615414\n",
            "loss for all sentences in 1401th epoch is 10.986101492084806\n",
            "loss for all sentences in 1402th epoch is 10.986085619229188\n",
            "loss for all sentences in 1403th epoch is 10.98606975304339\n",
            "loss for all sentences in 1404th epoch is 10.986053893522243\n",
            "loss for all sentences in 1405th epoch is 10.986038040660574\n",
            "loss for all sentences in 1406th epoch is 10.986022194453216\n",
            "loss for all sentences in 1407th epoch is 10.986006354894995\n",
            "loss for all sentences in 1408th epoch is 10.985990521980744\n",
            "loss for all sentences in 1409th epoch is 10.98597469570529\n",
            "loss for all sentences in 1410th epoch is 10.985958876063462\n",
            "loss for all sentences in 1411th epoch is 10.98594306305009\n",
            "loss for all sentences in 1412th epoch is 10.985927256660005\n",
            "loss for all sentences in 1413th epoch is 10.985911456888036\n",
            "loss for all sentences in 1414th epoch is 10.985895663729012\n",
            "loss for all sentences in 1415th epoch is 10.985879877177759\n",
            "loss for all sentences in 1416th epoch is 10.985864097229113\n",
            "loss for all sentences in 1417th epoch is 10.985848323877896\n",
            "loss for all sentences in 1418th epoch is 10.985832557118941\n",
            "loss for all sentences in 1419th epoch is 10.985816796947075\n",
            "loss for all sentences in 1420th epoch is 10.985801043357133\n",
            "loss for all sentences in 1421th epoch is 10.985785296343936\n",
            "loss for all sentences in 1422th epoch is 10.985769555902314\n",
            "loss for all sentences in 1423th epoch is 10.9857538220271\n",
            "loss for all sentences in 1424th epoch is 10.985738094713122\n",
            "loss for all sentences in 1425th epoch is 10.985722373955207\n",
            "loss for all sentences in 1426th epoch is 10.985706659748182\n",
            "loss for all sentences in 1427th epoch is 10.985690952086882\n",
            "loss for all sentences in 1428th epoch is 10.985675250966127\n",
            "loss for all sentences in 1429th epoch is 10.985659556380751\n",
            "loss for all sentences in 1430th epoch is 10.985643868325582\n",
            "loss for all sentences in 1431th epoch is 10.985628186795445\n",
            "loss for all sentences in 1432th epoch is 10.985612511785174\n",
            "loss for all sentences in 1433th epoch is 10.985596843289592\n",
            "loss for all sentences in 1434th epoch is 10.985581181303527\n",
            "loss for all sentences in 1435th epoch is 10.98556552582181\n",
            "loss for all sentences in 1436th epoch is 10.985549876839263\n",
            "loss for all sentences in 1437th epoch is 10.985534234350721\n",
            "loss for all sentences in 1438th epoch is 10.98551859835101\n",
            "loss for all sentences in 1439th epoch is 10.985502968834956\n",
            "loss for all sentences in 1440th epoch is 10.985487345797385\n",
            "loss for all sentences in 1441th epoch is 10.985471729233126\n",
            "loss for all sentences in 1442th epoch is 10.985456119137005\n",
            "loss for all sentences in 1443th epoch is 10.98544051550385\n",
            "loss for all sentences in 1444th epoch is 10.98542491832849\n",
            "loss for all sentences in 1445th epoch is 10.98540932760575\n",
            "loss for all sentences in 1446th epoch is 10.98539374333046\n",
            "loss for all sentences in 1447th epoch is 10.985378165497439\n",
            "loss for all sentences in 1448th epoch is 10.985362594101517\n",
            "loss for all sentences in 1449th epoch is 10.985347029137522\n",
            "loss for all sentences in 1450th epoch is 10.985331470600283\n",
            "loss for all sentences in 1451th epoch is 10.985315918484618\n",
            "loss for all sentences in 1452th epoch is 10.985300372785362\n",
            "loss for all sentences in 1453th epoch is 10.985284833497335\n",
            "loss for all sentences in 1454th epoch is 10.985269300615364\n",
            "loss for all sentences in 1455th epoch is 10.985253774134275\n",
            "loss for all sentences in 1456th epoch is 10.985238254048895\n",
            "loss for all sentences in 1457th epoch is 10.985222740354045\n",
            "loss for all sentences in 1458th epoch is 10.985207233044555\n",
            "loss for all sentences in 1459th epoch is 10.985191732115247\n",
            "loss for all sentences in 1460th epoch is 10.985176237560948\n",
            "loss for all sentences in 1461th epoch is 10.98516074937648\n",
            "loss for all sentences in 1462th epoch is 10.985145267556671\n",
            "loss for all sentences in 1463th epoch is 10.985129792096341\n",
            "loss for all sentences in 1464th epoch is 10.98511432299032\n",
            "loss for all sentences in 1465th epoch is 10.985098860233427\n",
            "loss for all sentences in 1466th epoch is 10.985083403820488\n",
            "loss for all sentences in 1467th epoch is 10.985067953746325\n",
            "loss for all sentences in 1468th epoch is 10.985052510005765\n",
            "loss for all sentences in 1469th epoch is 10.98503707259363\n",
            "loss for all sentences in 1470th epoch is 10.98502164150474\n",
            "loss for all sentences in 1471th epoch is 10.98500621673392\n",
            "loss for all sentences in 1472th epoch is 10.984990798275996\n",
            "loss for all sentences in 1473th epoch is 10.98497538612579\n",
            "loss for all sentences in 1474th epoch is 10.984959980278118\n",
            "loss for all sentences in 1475th epoch is 10.98494458072781\n",
            "loss for all sentences in 1476th epoch is 10.984929187469682\n",
            "loss for all sentences in 1477th epoch is 10.984913800498562\n",
            "loss for all sentences in 1478th epoch is 10.984898419809266\n",
            "loss for all sentences in 1479th epoch is 10.98488304539662\n",
            "loss for all sentences in 1480th epoch is 10.984867677255446\n",
            "loss for all sentences in 1481th epoch is 10.984852315380556\n",
            "loss for all sentences in 1482th epoch is 10.984836959766783\n",
            "loss for all sentences in 1483th epoch is 10.984821610408938\n",
            "loss for all sentences in 1484th epoch is 10.984806267301847\n",
            "loss for all sentences in 1485th epoch is 10.984790930440326\n",
            "loss for all sentences in 1486th epoch is 10.984775599819201\n",
            "loss for all sentences in 1487th epoch is 10.984760275433286\n",
            "loss for all sentences in 1488th epoch is 10.984744957277403\n",
            "loss for all sentences in 1489th epoch is 10.984729645346373\n",
            "loss for all sentences in 1490th epoch is 10.984714339635014\n",
            "loss for all sentences in 1491th epoch is 10.984699040138143\n",
            "loss for all sentences in 1492th epoch is 10.984683746850578\n",
            "loss for all sentences in 1493th epoch is 10.984668459767141\n",
            "loss for all sentences in 1494th epoch is 10.984653178882649\n",
            "loss for all sentences in 1495th epoch is 10.984637904191915\n",
            "loss for all sentences in 1496th epoch is 10.984622635689766\n",
            "loss for all sentences in 1497th epoch is 10.984607373371013\n",
            "loss for all sentences in 1498th epoch is 10.984592117230472\n",
            "loss for all sentences in 1499th epoch is 10.984576867262964\n",
            "loss for all sentences in 1500th epoch is 10.984561623463302\n",
            "loss for all sentences in 1501th epoch is 10.984546385826306\n",
            "loss for all sentences in 1502th epoch is 10.984531154346788\n",
            "loss for all sentences in 1503th epoch is 10.984515929019569\n",
            "loss for all sentences in 1504th epoch is 10.984500709839459\n",
            "loss for all sentences in 1505th epoch is 10.984485496801279\n",
            "loss for all sentences in 1506th epoch is 10.98447028989984\n",
            "loss for all sentences in 1507th epoch is 10.984455089129955\n",
            "loss for all sentences in 1508th epoch is 10.984439894486444\n",
            "loss for all sentences in 1509th epoch is 10.984424705964114\n",
            "loss for all sentences in 1510th epoch is 10.984409523557787\n",
            "loss for all sentences in 1511th epoch is 10.98439434726227\n",
            "loss for all sentences in 1512th epoch is 10.984379177072384\n",
            "loss for all sentences in 1513th epoch is 10.98436401298293\n",
            "loss for all sentences in 1514th epoch is 10.984348854988731\n",
            "loss for all sentences in 1515th epoch is 10.984333703084596\n",
            "loss for all sentences in 1516th epoch is 10.984318557265336\n",
            "loss for all sentences in 1517th epoch is 10.984303417525764\n",
            "loss for all sentences in 1518th epoch is 10.984288283860689\n",
            "loss for all sentences in 1519th epoch is 10.984273156264926\n",
            "loss for all sentences in 1520th epoch is 10.984258034733283\n",
            "loss for all sentences in 1521th epoch is 10.984242919260575\n",
            "loss for all sentences in 1522th epoch is 10.984227809841604\n",
            "loss for all sentences in 1523th epoch is 10.984212706471187\n",
            "loss for all sentences in 1524th epoch is 10.984197609144132\n",
            "loss for all sentences in 1525th epoch is 10.984182517855245\n",
            "loss for all sentences in 1526th epoch is 10.984167432599335\n",
            "loss for all sentences in 1527th epoch is 10.984152353371217\n",
            "loss for all sentences in 1528th epoch is 10.984137280165692\n",
            "loss for all sentences in 1529th epoch is 10.984122212977573\n",
            "loss for all sentences in 1530th epoch is 10.984107151801663\n",
            "loss for all sentences in 1531th epoch is 10.984092096632768\n",
            "loss for all sentences in 1532th epoch is 10.984077047465702\n",
            "loss for all sentences in 1533th epoch is 10.984062004295268\n",
            "loss for all sentences in 1534th epoch is 10.984046967116267\n",
            "loss for all sentences in 1535th epoch is 10.98403193592351\n",
            "loss for all sentences in 1536th epoch is 10.9840169107118\n",
            "loss for all sentences in 1537th epoch is 10.984001891475943\n",
            "loss for all sentences in 1538th epoch is 10.983986878210747\n",
            "loss for all sentences in 1539th epoch is 10.983971870911006\n",
            "loss for all sentences in 1540th epoch is 10.983956869571536\n",
            "loss for all sentences in 1541th epoch is 10.983941874187135\n",
            "loss for all sentences in 1542th epoch is 10.983926884752604\n",
            "loss for all sentences in 1543th epoch is 10.983911901262747\n",
            "loss for all sentences in 1544th epoch is 10.983896923712367\n",
            "loss for all sentences in 1545th epoch is 10.983881952096265\n",
            "loss for all sentences in 1546th epoch is 10.983866986409245\n",
            "loss for all sentences in 1547th epoch is 10.983852026646101\n",
            "loss for all sentences in 1548th epoch is 10.983837072801641\n",
            "loss for all sentences in 1549th epoch is 10.983822124870667\n",
            "loss for all sentences in 1550th epoch is 10.983807182847968\n",
            "loss for all sentences in 1551th epoch is 10.983792246728353\n",
            "loss for all sentences in 1552th epoch is 10.983777316506618\n",
            "loss for all sentences in 1553th epoch is 10.983762392177562\n",
            "loss for all sentences in 1554th epoch is 10.98374747373598\n",
            "loss for all sentences in 1555th epoch is 10.983732561176675\n",
            "loss for all sentences in 1556th epoch is 10.983717654494441\n",
            "loss for all sentences in 1557th epoch is 10.983702753684073\n",
            "loss for all sentences in 1558th epoch is 10.983687858740373\n",
            "loss for all sentences in 1559th epoch is 10.983672969658132\n",
            "loss for all sentences in 1560th epoch is 10.983658086432147\n",
            "loss for all sentences in 1561th epoch is 10.983643209057213\n",
            "loss for all sentences in 1562th epoch is 10.983628337528124\n",
            "loss for all sentences in 1563th epoch is 10.983613471839677\n",
            "loss for all sentences in 1564th epoch is 10.98359861198666\n",
            "loss for all sentences in 1565th epoch is 10.983583757963874\n",
            "loss for all sentences in 1566th epoch is 10.983568909766104\n",
            "loss for all sentences in 1567th epoch is 10.983554067388146\n",
            "loss for all sentences in 1568th epoch is 10.983539230824793\n",
            "loss for all sentences in 1569th epoch is 10.983524400070834\n",
            "loss for all sentences in 1570th epoch is 10.98350957512106\n",
            "loss for all sentences in 1571th epoch is 10.983494755970264\n",
            "loss for all sentences in 1572th epoch is 10.983479942613233\n",
            "loss for all sentences in 1573th epoch is 10.983465135044757\n",
            "loss for all sentences in 1574th epoch is 10.983450333259626\n",
            "loss for all sentences in 1575th epoch is 10.983435537252623\n",
            "loss for all sentences in 1576th epoch is 10.983420747018544\n",
            "loss for all sentences in 1577th epoch is 10.983405962552172\n",
            "loss for all sentences in 1578th epoch is 10.983391183848294\n",
            "loss for all sentences in 1579th epoch is 10.983376410901698\n",
            "loss for all sentences in 1580th epoch is 10.98336164370717\n",
            "loss for all sentences in 1581th epoch is 10.98334688225949\n",
            "loss for all sentences in 1582th epoch is 10.983332126553451\n",
            "loss for all sentences in 1583th epoch is 10.98331737658383\n",
            "loss for all sentences in 1584th epoch is 10.983302632345417\n",
            "loss for all sentences in 1585th epoch is 10.98328789383299\n",
            "loss for all sentences in 1586th epoch is 10.983273161041334\n",
            "loss for all sentences in 1587th epoch is 10.983258433965233\n",
            "loss for all sentences in 1588th epoch is 10.983243712599462\n",
            "loss for all sentences in 1589th epoch is 10.98322899693881\n",
            "loss for all sentences in 1590th epoch is 10.983214286978054\n",
            "loss for all sentences in 1591th epoch is 10.983199582711968\n",
            "loss for all sentences in 1592th epoch is 10.983184884135344\n",
            "loss for all sentences in 1593th epoch is 10.983170191242953\n",
            "loss for all sentences in 1594th epoch is 10.983155504029572\n",
            "loss for all sentences in 1595th epoch is 10.983140822489982\n",
            "loss for all sentences in 1596th epoch is 10.98312614661896\n",
            "loss for all sentences in 1597th epoch is 10.983111476411283\n",
            "loss for all sentences in 1598th epoch is 10.983096811861722\n",
            "loss for all sentences in 1599th epoch is 10.983082152965057\n",
            "loss for all sentences in 1600th epoch is 10.98306749971606\n",
            "loss for all sentences in 1601th epoch is 10.983052852109509\n",
            "loss for all sentences in 1602th epoch is 10.983038210140174\n",
            "loss for all sentences in 1603th epoch is 10.98302357380283\n",
            "loss for all sentences in 1604th epoch is 10.98300894309225\n",
            "loss for all sentences in 1605th epoch is 10.9829943180032\n",
            "loss for all sentences in 1606th epoch is 10.98297969853046\n",
            "loss for all sentences in 1607th epoch is 10.982965084668793\n",
            "loss for all sentences in 1608th epoch is 10.982950476412976\n",
            "loss for all sentences in 1609th epoch is 10.982935873757771\n",
            "loss for all sentences in 1610th epoch is 10.98292127669795\n",
            "loss for all sentences in 1611th epoch is 10.982906685228281\n",
            "loss for all sentences in 1612th epoch is 10.98289209934353\n",
            "loss for all sentences in 1613th epoch is 10.982877519038468\n",
            "loss for all sentences in 1614th epoch is 10.982862944307854\n",
            "loss for all sentences in 1615th epoch is 10.98284837514646\n",
            "loss for all sentences in 1616th epoch is 10.98283381154905\n",
            "loss for all sentences in 1617th epoch is 10.982819253510385\n",
            "loss for all sentences in 1618th epoch is 10.98280470102523\n",
            "loss for all sentences in 1619th epoch is 10.982790154088347\n",
            "loss for all sentences in 1620th epoch is 10.9827756126945\n",
            "loss for all sentences in 1621th epoch is 10.98276107683845\n",
            "loss for all sentences in 1622th epoch is 10.982746546514955\n",
            "loss for all sentences in 1623th epoch is 10.982732021718778\n",
            "loss for all sentences in 1624th epoch is 10.982717502444679\n",
            "loss for all sentences in 1625th epoch is 10.982702988687414\n",
            "loss for all sentences in 1626th epoch is 10.982688480441745\n",
            "loss for all sentences in 1627th epoch is 10.982673977702426\n",
            "loss for all sentences in 1628th epoch is 10.982659480464214\n",
            "loss for all sentences in 1629th epoch is 10.982644988721866\n",
            "loss for all sentences in 1630th epoch is 10.982630502470135\n",
            "loss for all sentences in 1631th epoch is 10.982616021703778\n",
            "loss for all sentences in 1632th epoch is 10.982601546417552\n",
            "loss for all sentences in 1633th epoch is 10.982587076606201\n",
            "loss for all sentences in 1634th epoch is 10.982572612264487\n",
            "loss for all sentences in 1635th epoch is 10.982558153387156\n",
            "loss for all sentences in 1636th epoch is 10.982543699968959\n",
            "loss for all sentences in 1637th epoch is 10.98252925200465\n",
            "loss for all sentences in 1638th epoch is 10.982514809488974\n",
            "loss for all sentences in 1639th epoch is 10.982500372416682\n",
            "loss for all sentences in 1640th epoch is 10.982485940782523\n",
            "loss for all sentences in 1641th epoch is 10.982471514581242\n",
            "loss for all sentences in 1642th epoch is 10.982457093807586\n",
            "loss for all sentences in 1643th epoch is 10.982442678456298\n",
            "loss for all sentences in 1644th epoch is 10.982428268522128\n",
            "loss for all sentences in 1645th epoch is 10.98241386399982\n",
            "loss for all sentences in 1646th epoch is 10.98239946488411\n",
            "loss for all sentences in 1647th epoch is 10.98238507116975\n",
            "loss for all sentences in 1648th epoch is 10.982370682851476\n",
            "loss for all sentences in 1649th epoch is 10.982356299924032\n",
            "loss for all sentences in 1650th epoch is 10.982341922382156\n",
            "loss for all sentences in 1651th epoch is 10.982327550220585\n",
            "loss for all sentences in 1652th epoch is 10.982313183434064\n",
            "loss for all sentences in 1653th epoch is 10.982298822017325\n",
            "loss for all sentences in 1654th epoch is 10.98228446596511\n",
            "loss for all sentences in 1655th epoch is 10.982270115272152\n",
            "loss for all sentences in 1656th epoch is 10.98225576993319\n",
            "loss for all sentences in 1657th epoch is 10.98224142994295\n",
            "loss for all sentences in 1658th epoch is 10.982227095296173\n",
            "loss for all sentences in 1659th epoch is 10.98221276598759\n",
            "loss for all sentences in 1660th epoch is 10.982198442011935\n",
            "loss for all sentences in 1661th epoch is 10.982184123363936\n",
            "loss for all sentences in 1662th epoch is 10.982169810038323\n",
            "loss for all sentences in 1663th epoch is 10.982155502029826\n",
            "loss for all sentences in 1664th epoch is 10.98214119933318\n",
            "loss for all sentences in 1665th epoch is 10.982126901943102\n",
            "loss for all sentences in 1666th epoch is 10.982112609854328\n",
            "loss for all sentences in 1667th epoch is 10.982098323061578\n",
            "loss for all sentences in 1668th epoch is 10.98208404155958\n",
            "loss for all sentences in 1669th epoch is 10.982069765343057\n",
            "loss for all sentences in 1670th epoch is 10.982055494406733\n",
            "loss for all sentences in 1671th epoch is 10.98204122874533\n",
            "loss for all sentences in 1672th epoch is 10.982026968353571\n",
            "loss for all sentences in 1673th epoch is 10.982012713226174\n",
            "loss for all sentences in 1674th epoch is 10.981998463357865\n",
            "loss for all sentences in 1675th epoch is 10.981984218743355\n",
            "loss for all sentences in 1676th epoch is 10.981969979377364\n",
            "loss for all sentences in 1677th epoch is 10.981955745254611\n",
            "loss for all sentences in 1678th epoch is 10.981941516369812\n",
            "loss for all sentences in 1679th epoch is 10.981927292717682\n",
            "loss for all sentences in 1680th epoch is 10.981913074292935\n",
            "loss for all sentences in 1681th epoch is 10.981898861090285\n",
            "loss for all sentences in 1682th epoch is 10.981884653104443\n",
            "loss for all sentences in 1683th epoch is 10.98187045033012\n",
            "loss for all sentences in 1684th epoch is 10.981856252762029\n",
            "loss for all sentences in 1685th epoch is 10.981842060394877\n",
            "loss for all sentences in 1686th epoch is 10.981827873223374\n",
            "loss for all sentences in 1687th epoch is 10.981813691242225\n",
            "loss for all sentences in 1688th epoch is 10.98179951444614\n",
            "loss for all sentences in 1689th epoch is 10.981785342829827\n",
            "loss for all sentences in 1690th epoch is 10.981771176387984\n",
            "loss for all sentences in 1691th epoch is 10.981757015115315\n",
            "loss for all sentences in 1692th epoch is 10.981742859006529\n",
            "loss for all sentences in 1693th epoch is 10.981728708056323\n",
            "loss for all sentences in 1694th epoch is 10.9817145622594\n",
            "loss for all sentences in 1695th epoch is 10.981700421610459\n",
            "loss for all sentences in 1696th epoch is 10.981686286104194\n",
            "loss for all sentences in 1697th epoch is 10.981672155735312\n",
            "loss for all sentences in 1698th epoch is 10.981658030498501\n",
            "loss for all sentences in 1699th epoch is 10.98164391038846\n",
            "loss for all sentences in 1700th epoch is 10.981629795399884\n",
            "loss for all sentences in 1701th epoch is 10.981615685527467\n",
            "loss for all sentences in 1702th epoch is 10.9816015807659\n",
            "loss for all sentences in 1703th epoch is 10.981587481109875\n",
            "loss for all sentences in 1704th epoch is 10.98157338655408\n",
            "loss for all sentences in 1705th epoch is 10.981559297093211\n",
            "loss for all sentences in 1706th epoch is 10.981545212721946\n",
            "loss for all sentences in 1707th epoch is 10.981531133434983\n",
            "loss for all sentences in 1708th epoch is 10.981517059227\n",
            "loss for all sentences in 1709th epoch is 10.981502990092688\n",
            "loss for all sentences in 1710th epoch is 10.981488926026728\n",
            "loss for all sentences in 1711th epoch is 10.9814748670238\n",
            "loss for all sentences in 1712th epoch is 10.98146081307859\n",
            "loss for all sentences in 1713th epoch is 10.981446764185778\n",
            "loss for all sentences in 1714th epoch is 10.981432720340042\n",
            "loss for all sentences in 1715th epoch is 10.981418681536061\n",
            "loss for all sentences in 1716th epoch is 10.981404647768514\n",
            "loss for all sentences in 1717th epoch is 10.981390619032075\n",
            "loss for all sentences in 1718th epoch is 10.981376595321422\n",
            "loss for all sentences in 1719th epoch is 10.981362576631222\n",
            "loss for all sentences in 1720th epoch is 10.981348562956155\n",
            "loss for all sentences in 1721th epoch is 10.98133455429089\n",
            "loss for all sentences in 1722th epoch is 10.9813205506301\n",
            "loss for all sentences in 1723th epoch is 10.981306551968448\n",
            "loss for all sentences in 1724th epoch is 10.981292558300611\n",
            "loss for all sentences in 1725th epoch is 10.981278569621248\n",
            "loss for all sentences in 1726th epoch is 10.981264585925029\n",
            "loss for all sentences in 1727th epoch is 10.981250607206617\n",
            "loss for all sentences in 1728th epoch is 10.981236633460675\n",
            "loss for all sentences in 1729th epoch is 10.98122266468187\n",
            "loss for all sentences in 1730th epoch is 10.981208700864856\n",
            "loss for all sentences in 1731th epoch is 10.981194742004297\n",
            "loss for all sentences in 1732th epoch is 10.981180788094852\n",
            "loss for all sentences in 1733th epoch is 10.981166839131177\n",
            "loss for all sentences in 1734th epoch is 10.98115289510793\n",
            "loss for all sentences in 1735th epoch is 10.981138956019764\n",
            "loss for all sentences in 1736th epoch is 10.98112502186133\n",
            "loss for all sentences in 1737th epoch is 10.981111092627291\n",
            "loss for all sentences in 1738th epoch is 10.981097168312287\n",
            "loss for all sentences in 1739th epoch is 10.981083248910972\n",
            "loss for all sentences in 1740th epoch is 10.981069334417999\n",
            "loss for all sentences in 1741th epoch is 10.981055424828007\n",
            "loss for all sentences in 1742th epoch is 10.98104152013565\n",
            "loss for all sentences in 1743th epoch is 10.981027620335567\n",
            "loss for all sentences in 1744th epoch is 10.981013725422407\n",
            "loss for all sentences in 1745th epoch is 10.98099983539081\n",
            "loss for all sentences in 1746th epoch is 10.980985950235416\n",
            "loss for all sentences in 1747th epoch is 10.98097206995087\n",
            "loss for all sentences in 1748th epoch is 10.980958194531805\n",
            "loss for all sentences in 1749th epoch is 10.980944323972858\n",
            "loss for all sentences in 1750th epoch is 10.980930458268668\n",
            "loss for all sentences in 1751th epoch is 10.980916597413874\n",
            "loss for all sentences in 1752th epoch is 10.980902741403096\n",
            "loss for all sentences in 1753th epoch is 10.980888890230974\n",
            "loss for all sentences in 1754th epoch is 10.980875043892146\n",
            "loss for all sentences in 1755th epoch is 10.980861202381226\n",
            "loss for all sentences in 1756th epoch is 10.980847365692854\n",
            "loss for all sentences in 1757th epoch is 10.980833533821652\n",
            "loss for all sentences in 1758th epoch is 10.980819706762244\n",
            "loss for all sentences in 1759th epoch is 10.980805884509257\n",
            "loss for all sentences in 1760th epoch is 10.98079206705731\n",
            "loss for all sentences in 1761th epoch is 10.980778254401026\n",
            "loss for all sentences in 1762th epoch is 10.980764446535023\n",
            "loss for all sentences in 1763th epoch is 10.980750643453925\n",
            "loss for all sentences in 1764th epoch is 10.98073684515234\n",
            "loss for all sentences in 1765th epoch is 10.980723051624892\n",
            "loss for all sentences in 1766th epoch is 10.98070926286619\n",
            "loss for all sentences in 1767th epoch is 10.980695478870846\n",
            "loss for all sentences in 1768th epoch is 10.980681699633477\n",
            "loss for all sentences in 1769th epoch is 10.980667925148685\n",
            "loss for all sentences in 1770th epoch is 10.980654155411088\n",
            "loss for all sentences in 1771th epoch is 10.980640390415282\n",
            "loss for all sentences in 1772th epoch is 10.98062663015588\n",
            "loss for all sentences in 1773th epoch is 10.980612874627482\n",
            "loss for all sentences in 1774th epoch is 10.980599123824692\n",
            "loss for all sentences in 1775th epoch is 10.980585377742113\n",
            "loss for all sentences in 1776th epoch is 10.980571636374341\n",
            "loss for all sentences in 1777th epoch is 10.980557899715977\n",
            "loss for all sentences in 1778th epoch is 10.980544167761616\n",
            "loss for all sentences in 1779th epoch is 10.980530440505856\n",
            "loss for all sentences in 1780th epoch is 10.980516717943285\n",
            "loss for all sentences in 1781th epoch is 10.980503000068499\n",
            "loss for all sentences in 1782th epoch is 10.980489286876088\n",
            "loss for all sentences in 1783th epoch is 10.980475578360643\n",
            "loss for all sentences in 1784th epoch is 10.980461874516747\n",
            "loss for all sentences in 1785th epoch is 10.980448175338989\n",
            "loss for all sentences in 1786th epoch is 10.980434480821955\n",
            "loss for all sentences in 1787th epoch is 10.980420790960224\n",
            "loss for all sentences in 1788th epoch is 10.980407105748377\n",
            "loss for all sentences in 1789th epoch is 10.980393425181\n",
            "loss for all sentences in 1790th epoch is 10.980379749252661\n",
            "loss for all sentences in 1791th epoch is 10.980366077957948\n",
            "loss for all sentences in 1792th epoch is 10.980352411291427\n",
            "loss for all sentences in 1793th epoch is 10.980338749247679\n",
            "loss for all sentences in 1794th epoch is 10.980325091821268\n",
            "loss for all sentences in 1795th epoch is 10.980311439006769\n",
            "loss for all sentences in 1796th epoch is 10.980297790798751\n",
            "loss for all sentences in 1797th epoch is 10.980284147191778\n",
            "loss for all sentences in 1798th epoch is 10.980270508180416\n",
            "loss for all sentences in 1799th epoch is 10.980256873759233\n",
            "loss for all sentences in 1800th epoch is 10.980243243922782\n",
            "loss for all sentences in 1801th epoch is 10.980229618665632\n",
            "loss for all sentences in 1802th epoch is 10.980215997982338\n",
            "loss for all sentences in 1803th epoch is 10.980202381867459\n",
            "loss for all sentences in 1804th epoch is 10.980188770315548\n",
            "loss for all sentences in 1805th epoch is 10.980175163321157\n",
            "loss for all sentences in 1806th epoch is 10.980161560878845\n",
            "loss for all sentences in 1807th epoch is 10.980147962983155\n",
            "loss for all sentences in 1808th epoch is 10.98013436962864\n",
            "loss for all sentences in 1809th epoch is 10.980120780809848\n",
            "loss for all sentences in 1810th epoch is 10.98010719652132\n",
            "loss for all sentences in 1811th epoch is 10.9800936167576\n",
            "loss for all sentences in 1812th epoch is 10.980080041513233\n",
            "loss for all sentences in 1813th epoch is 10.980066470782756\n",
            "loss for all sentences in 1814th epoch is 10.98005290456071\n",
            "loss for all sentences in 1815th epoch is 10.980039342841627\n",
            "loss for all sentences in 1816th epoch is 10.980025785620047\n",
            "loss for all sentences in 1817th epoch is 10.980012232890497\n",
            "loss for all sentences in 1818th epoch is 10.979998684647516\n",
            "loss for all sentences in 1819th epoch is 10.979985140885626\n",
            "loss for all sentences in 1820th epoch is 10.97997160159936\n",
            "loss for all sentences in 1821th epoch is 10.97995806678324\n",
            "loss for all sentences in 1822th epoch is 10.979944536431791\n",
            "loss for all sentences in 1823th epoch is 10.979931010539536\n",
            "loss for all sentences in 1824th epoch is 10.979917489100997\n",
            "loss for all sentences in 1825th epoch is 10.979903972110687\n",
            "loss for all sentences in 1826th epoch is 10.97989045956313\n",
            "loss for all sentences in 1827th epoch is 10.979876951452834\n",
            "loss for all sentences in 1828th epoch is 10.979863447774317\n",
            "loss for all sentences in 1829th epoch is 10.979849948522086\n",
            "loss for all sentences in 1830th epoch is 10.979836453690654\n",
            "loss for all sentences in 1831th epoch is 10.979822963274527\n",
            "loss for all sentences in 1832th epoch is 10.979809477268208\n",
            "loss for all sentences in 1833th epoch is 10.979795995666207\n",
            "loss for all sentences in 1834th epoch is 10.979782518463015\n",
            "loss for all sentences in 1835th epoch is 10.979769045653143\n",
            "loss for all sentences in 1836th epoch is 10.979755577231082\n",
            "loss for all sentences in 1837th epoch is 10.979742113191334\n",
            "loss for all sentences in 1838th epoch is 10.979728653528381\n",
            "loss for all sentences in 1839th epoch is 10.979715198236729\n",
            "loss for all sentences in 1840th epoch is 10.97970174731086\n",
            "loss for all sentences in 1841th epoch is 10.979688300745265\n",
            "loss for all sentences in 1842th epoch is 10.97967485853443\n",
            "loss for all sentences in 1843th epoch is 10.979661420672839\n",
            "loss for all sentences in 1844th epoch is 10.97964798715497\n",
            "loss for all sentences in 1845th epoch is 10.979634557975311\n",
            "loss for all sentences in 1846th epoch is 10.979621133128333\n",
            "loss for all sentences in 1847th epoch is 10.979607712608516\n",
            "loss for all sentences in 1848th epoch is 10.979594296410333\n",
            "loss for all sentences in 1849th epoch is 10.97958088452826\n",
            "loss for all sentences in 1850th epoch is 10.979567476956758\n",
            "loss for all sentences in 1851th epoch is 10.979554073690304\n",
            "loss for all sentences in 1852th epoch is 10.979540674723363\n",
            "loss for all sentences in 1853th epoch is 10.979527280050394\n",
            "loss for all sentences in 1854th epoch is 10.979513889665858\n",
            "loss for all sentences in 1855th epoch is 10.979500503564223\n",
            "loss for all sentences in 1856th epoch is 10.979487121739941\n",
            "loss for all sentences in 1857th epoch is 10.979473744187466\n",
            "loss for all sentences in 1858th epoch is 10.979460370901258\n",
            "loss for all sentences in 1859th epoch is 10.979447001875764\n",
            "loss for all sentences in 1860th epoch is 10.979433637105428\n",
            "loss for all sentences in 1861th epoch is 10.979420276584708\n",
            "loss for all sentences in 1862th epoch is 10.979406920308046\n",
            "loss for all sentences in 1863th epoch is 10.979393568269877\n",
            "loss for all sentences in 1864th epoch is 10.97938022046465\n",
            "loss for all sentences in 1865th epoch is 10.9793668768868\n",
            "loss for all sentences in 1866th epoch is 10.979353537530764\n",
            "loss for all sentences in 1867th epoch is 10.979340202390977\n",
            "loss for all sentences in 1868th epoch is 10.979326871461872\n",
            "loss for all sentences in 1869th epoch is 10.979313544737874\n",
            "loss for all sentences in 1870th epoch is 10.979300222213414\n",
            "loss for all sentences in 1871th epoch is 10.979286903882915\n",
            "loss for all sentences in 1872th epoch is 10.979273589740805\n",
            "loss for all sentences in 1873th epoch is 10.979260279781503\n",
            "loss for all sentences in 1874th epoch is 10.979246973999423\n",
            "loss for all sentences in 1875th epoch is 10.979233672388986\n",
            "loss for all sentences in 1876th epoch is 10.979220374944607\n",
            "loss for all sentences in 1877th epoch is 10.97920708166069\n",
            "loss for all sentences in 1878th epoch is 10.979193792531653\n",
            "loss for all sentences in 1879th epoch is 10.979180507551899\n",
            "loss for all sentences in 1880th epoch is 10.979167226715838\n",
            "loss for all sentences in 1881th epoch is 10.979153950017862\n",
            "loss for all sentences in 1882th epoch is 10.979140677452383\n",
            "loss for all sentences in 1883th epoch is 10.979127409013795\n",
            "loss for all sentences in 1884th epoch is 10.979114144696487\n",
            "loss for all sentences in 1885th epoch is 10.979100884494862\n",
            "loss for all sentences in 1886th epoch is 10.979087628403306\n",
            "loss for all sentences in 1887th epoch is 10.979074376416207\n",
            "loss for all sentences in 1888th epoch is 10.97906112852795\n",
            "loss for all sentences in 1889th epoch is 10.979047884732925\n",
            "loss for all sentences in 1890th epoch is 10.979034645025505\n",
            "loss for all sentences in 1891th epoch is 10.979021409400078\n",
            "loss for all sentences in 1892th epoch is 10.97900817785101\n",
            "loss for all sentences in 1893th epoch is 10.978994950372686\n",
            "loss for all sentences in 1894th epoch is 10.978981726959468\n",
            "loss for all sentences in 1895th epoch is 10.978968507605732\n",
            "loss for all sentences in 1896th epoch is 10.978955292305837\n",
            "loss for all sentences in 1897th epoch is 10.978942081054157\n",
            "loss for all sentences in 1898th epoch is 10.978928873845048\n",
            "loss for all sentences in 1899th epoch is 10.978915670672869\n",
            "loss for all sentences in 1900th epoch is 10.978902471531978\n",
            "loss for all sentences in 1901th epoch is 10.97888927641673\n",
            "loss for all sentences in 1902th epoch is 10.978876085321476\n",
            "loss for all sentences in 1903th epoch is 10.978862898240562\n",
            "loss for all sentences in 1904th epoch is 10.978849715168344\n",
            "loss for all sentences in 1905th epoch is 10.978836536099156\n",
            "loss for all sentences in 1906th epoch is 10.978823361027343\n",
            "loss for all sentences in 1907th epoch is 10.978810189947245\n",
            "loss for all sentences in 1908th epoch is 10.978797022853199\n",
            "loss for all sentences in 1909th epoch is 10.978783859739533\n",
            "loss for all sentences in 1910th epoch is 10.978770700600585\n",
            "loss for all sentences in 1911th epoch is 10.978757545430678\n",
            "loss for all sentences in 1912th epoch is 10.978744394224146\n",
            "loss for all sentences in 1913th epoch is 10.978731246975308\n",
            "loss for all sentences in 1914th epoch is 10.978718103678478\n",
            "loss for all sentences in 1915th epoch is 10.978704964327985\n",
            "loss for all sentences in 1916th epoch is 10.978691828918135\n",
            "loss for all sentences in 1917th epoch is 10.97867869744325\n",
            "loss for all sentences in 1918th epoch is 10.978665569897629\n",
            "loss for all sentences in 1919th epoch is 10.978652446275587\n",
            "loss for all sentences in 1920th epoch is 10.978639326571427\n",
            "loss for all sentences in 1921th epoch is 10.978626210779453\n",
            "loss for all sentences in 1922th epoch is 10.97861309889396\n",
            "loss for all sentences in 1923th epoch is 10.978599990909245\n",
            "loss for all sentences in 1924th epoch is 10.978586886819599\n",
            "loss for all sentences in 1925th epoch is 10.97857378661932\n",
            "loss for all sentences in 1926th epoch is 10.978560690302695\n",
            "loss for all sentences in 1927th epoch is 10.978547597863999\n",
            "loss for all sentences in 1928th epoch is 10.978534509297528\n",
            "loss for all sentences in 1929th epoch is 10.97852142459755\n",
            "loss for all sentences in 1930th epoch is 10.978508343758353\n",
            "loss for all sentences in 1931th epoch is 10.978495266774203\n",
            "loss for all sentences in 1932th epoch is 10.97848219363937\n",
            "loss for all sentences in 1933th epoch is 10.97846912434813\n",
            "loss for all sentences in 1934th epoch is 10.978456058894745\n",
            "loss for all sentences in 1935th epoch is 10.978442997273474\n",
            "loss for all sentences in 1936th epoch is 10.978429939478579\n",
            "loss for all sentences in 1937th epoch is 10.978416885504316\n",
            "loss for all sentences in 1938th epoch is 10.978403835344942\n",
            "loss for all sentences in 1939th epoch is 10.978390788994703\n",
            "loss for all sentences in 1940th epoch is 10.978377746447855\n",
            "loss for all sentences in 1941th epoch is 10.978364707698631\n",
            "loss for all sentences in 1942th epoch is 10.978351672741283\n",
            "loss for all sentences in 1943th epoch is 10.978338641570048\n",
            "loss for all sentences in 1944th epoch is 10.978325614179159\n",
            "loss for all sentences in 1945th epoch is 10.978312590562851\n",
            "loss for all sentences in 1946th epoch is 10.978299570715356\n",
            "loss for all sentences in 1947th epoch is 10.978286554630895\n",
            "loss for all sentences in 1948th epoch is 10.978273542303702\n",
            "loss for all sentences in 1949th epoch is 10.978260533727985\n",
            "loss for all sentences in 1950th epoch is 10.978247528897974\n",
            "loss for all sentences in 1951th epoch is 10.978234527807878\n",
            "loss for all sentences in 1952th epoch is 10.978221530451908\n",
            "loss for all sentences in 1953th epoch is 10.978208536824276\n",
            "loss for all sentences in 1954th epoch is 10.978195546919185\n",
            "loss for all sentences in 1955th epoch is 10.97818256073084\n",
            "loss for all sentences in 1956th epoch is 10.97816957825344\n",
            "loss for all sentences in 1957th epoch is 10.978156599481178\n",
            "loss for all sentences in 1958th epoch is 10.978143624408245\n",
            "loss for all sentences in 1959th epoch is 10.978130653028842\n",
            "loss for all sentences in 1960th epoch is 10.978117685337144\n",
            "loss for all sentences in 1961th epoch is 10.978104721327343\n",
            "loss for all sentences in 1962th epoch is 10.97809176099361\n",
            "loss for all sentences in 1963th epoch is 10.978078804330133\n",
            "loss for all sentences in 1964th epoch is 10.97806585133108\n",
            "loss for all sentences in 1965th epoch is 10.978052901990617\n",
            "loss for all sentences in 1966th epoch is 10.978039956302924\n",
            "loss for all sentences in 1967th epoch is 10.978027014262153\n",
            "loss for all sentences in 1968th epoch is 10.978014075862468\n",
            "loss for all sentences in 1969th epoch is 10.978001141098032\n",
            "loss for all sentences in 1970th epoch is 10.97798820996299\n",
            "loss for all sentences in 1971th epoch is 10.977975282451503\n",
            "loss for all sentences in 1972th epoch is 10.97796235855771\n",
            "loss for all sentences in 1973th epoch is 10.977949438275761\n",
            "loss for all sentences in 1974th epoch is 10.977936521599794\n",
            "loss for all sentences in 1975th epoch is 10.977923608523948\n",
            "loss for all sentences in 1976th epoch is 10.977910699042356\n",
            "loss for all sentences in 1977th epoch is 10.977897793149147\n",
            "loss for all sentences in 1978th epoch is 10.977884890838453\n",
            "loss for all sentences in 1979th epoch is 10.977871992104392\n",
            "loss for all sentences in 1980th epoch is 10.977859096941092\n",
            "loss for all sentences in 1981th epoch is 10.977846205342663\n",
            "loss for all sentences in 1982th epoch is 10.97783331730322\n",
            "loss for all sentences in 1983th epoch is 10.977820432816877\n",
            "loss for all sentences in 1984th epoch is 10.977807551877733\n",
            "loss for all sentences in 1985th epoch is 10.977794674479902\n",
            "loss for all sentences in 1986th epoch is 10.977781800617471\n",
            "loss for all sentences in 1987th epoch is 10.977768930284547\n",
            "loss for all sentences in 1988th epoch is 10.977756063475216\n",
            "loss for all sentences in 1989th epoch is 10.977743200183571\n",
            "loss for all sentences in 1990th epoch is 10.977730340403692\n",
            "loss for all sentences in 1991th epoch is 10.977717484129665\n",
            "loss for all sentences in 1992th epoch is 10.977704631355564\n",
            "loss for all sentences in 1993th epoch is 10.97769178207547\n",
            "loss for all sentences in 1994th epoch is 10.977678936283446\n",
            "loss for all sentences in 1995th epoch is 10.977666093973566\n",
            "loss for all sentences in 1996th epoch is 10.977653255139892\n",
            "loss for all sentences in 1997th epoch is 10.97764041977648\n",
            "loss for all sentences in 1998th epoch is 10.97762758787739\n",
            "loss for all sentences in 1999th epoch is 10.977614759436673\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_test_Data = train.iloc[10:30]['phrase'].to_list()"
      ],
      "metadata": {
        "id": "_LfDoGWHA7U8"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_test_Data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FO49cIs0DKZY",
        "outputId": "e2a368a6-5d59-40bc-fb3c-a2f75ed21412"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Whenever you think you 've figured out Late Marriage , it throws you for a loop .\",\n",
              " 'Well-done supernatural thriller with keen insights into parapsychological phenomena and the soulful nuances of the grieving process .',\n",
              " 'A gratingly unfunny groaner littered with zero-dimensional , unlikable characters and hackneyed , threadbare comic setups .',\n",
              " \"I could n't recommend this film more .\",\n",
              " 'It remains to be seen whether Statham can move beyond the crime-land action genre , but then again , who says he has to ?',\n",
              " \"Take away the controversy , and it 's not much more watchable than a Mexican soap opera .\",\n",
              " 'But the second half of the movie really goes downhill .',\n",
              " 'There are plenty of scenes in Frida that do work , but rarely do they involve the title character herself .',\n",
              " \"You can taste it , but there 's no fizz .\",\n",
              " \"A movie that 's held captive by mediocrity .\",\n",
              " \"Just because A Walk to Remember is shrewd enough to activate girlish tear ducts does n't mean it 's good enough for our girls .\",\n",
              " \"Unlike Trey Parker , Sandler does n't understand that the idea of exploiting molestation for laughs is funny , not actually exploiting it yourself .\",\n",
              " 'Kids five and up will be delighted with the fast , funny , and even touching story .',\n",
              " 'There is more than one joke about putting the toilet seat down .',\n",
              " 'But once the falcon arrives in the skies above Manhattan , the adventure is on red alert .',\n",
              " 'Nearly every attempt at humor here is DOA .',\n",
              " 'In its treatment of the dehumanizing and ego-destroying process of unemployment , Time Out offers an exploration that is more accurate than anything I have seen in an American film .',\n",
              " 'Despite modest aspirations its occasional charms are not to be dismissed .',\n",
              " '... an eerily suspenseful , deeply absorbing piece that works as a treatise on spirituality as well as a solid sci-fi thriller .',\n",
              " \"An alternately raucous and sappy ethnic sitcom ... you 'd be wise to send your regrets .\"]"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sequences = [convert_integer(r) for r in sample_test_Data]"
      ],
      "metadata": {
        "id": "wQFyFCu8DMTH"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = []\n",
        "for s in test_sequences:\n",
        "  pred,_,_,_ = forward(s,0)\n",
        "  preds.append(pred)"
      ],
      "metadata": {
        "id": "swWAl1ShDZcH"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmax(preds,axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYxj08YvD3y5",
        "outputId": "6c72246b-c6a9-4aa0-ca18-ca2443cf1e92"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.iloc[10:16]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "dyrUbTkkD4ZV",
        "outputId": "4f57d57d-dd87-4020-9a70-91d66144ab75"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    id                                             phrase  feature_1  \\\n",
              "10  10  Whenever you think you 've figured out Late Ma...       17.0   \n",
              "11  11  Well-done supernatural thriller with keen insi...       18.0   \n",
              "12  12  A gratingly unfunny groaner littered with zero...       17.0   \n",
              "13  13             I could n't recommend this film more .        8.0   \n",
              "14  14  It remains to be seen whether Statham can move...       25.0   \n",
              "15  15  Take away the controversy , and it 's not much...       18.0   \n",
              "\n",
              "    feature_2  feature_3  sentiment  \n",
              "10        3.0        3.0          2  \n",
              "11        1.0        2.0          2  \n",
              "12        1.0        4.0          0  \n",
              "13        1.0        NaN          2  \n",
              "14        2.0        4.0          1  \n",
              "15        2.0        3.0          0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6c0daf13-500c-4134-b294-163c3af005c9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>phrase</th>\n",
              "      <th>feature_1</th>\n",
              "      <th>feature_2</th>\n",
              "      <th>feature_3</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>Whenever you think you 've figured out Late Ma...</td>\n",
              "      <td>17.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>Well-done supernatural thriller with keen insi...</td>\n",
              "      <td>18.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>A gratingly unfunny groaner littered with zero...</td>\n",
              "      <td>17.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>I could n't recommend this film more .</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>It remains to be seen whether Statham can move...</td>\n",
              "      <td>25.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>Take away the controversy , and it 's not much...</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6c0daf13-500c-4134-b294-163c3af005c9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6c0daf13-500c-4134-b294-163c3af005c9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6c0daf13-500c-4134-b294-163c3af005c9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-69e64d7b-bbeb-4775-83e5-1061c2f6e286\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-69e64d7b-bbeb-4775-83e5-1061c2f6e286')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-69e64d7b-bbeb-4775-83e5-1061c2f6e286 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"train\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 10,\n        \"max\": 15,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          10,\n          11,\n          15\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"phrase\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Whenever you think you 've figured out Late Marriage , it throws you for a loop .\",\n          \"Well-done supernatural thriller with keen insights into parapsychological phenomena and the soulful nuances of the grieving process .\",\n          \"Take away the controversy , and it 's not much more watchable than a Mexican soap opera .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.419102016632153,\n        \"min\": 8.0,\n        \"max\": 25.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          18.0,\n          25.0,\n          17.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.816496580927726,\n        \"min\": 1.0,\n        \"max\": 3.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3.0,\n          1.0,\n          2.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"feature_3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.8366600265340756,\n        \"min\": 2.0,\n        \"max\": 4.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3.0,\n          2.0,\n          4.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2,\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset,DataLoader\n",
        "import torch"
      ],
      "metadata": {
        "id": "Guc1a73mECPp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aBcRaaQ3II9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class customdata(Dataset):\n",
        "  def __init__(self,X,y):\n",
        "    seq = [convert_integer(t) for t in X]\n",
        "    mx = max(len(s) for s in seq)\n",
        "    pad = [ s + [0]*(mx-len(s)) for s in seq]\n",
        "    self.sequences  = torch.tensor(pad,dtype=torch.long)\n",
        "    self.label = torch.tensor(y,dtype=torch.long)\n",
        "  def __len__(self):\n",
        "    return len(self.sequences)\n",
        "  def __getitem__(self,index):\n",
        "    return self.sequences[index],self.label[index]\n"
      ],
      "metadata": {
        "id": "IdJ7DVhfGv3Z"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = train['phrase'].to_list()\n",
        "y = train['sentiment']"
      ],
      "metadata": {
        "id": "HtinopQhIXZd"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "rj9GegakIcWr"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,Y_train,Y_test = train_test_split(X,y,test_size=0.2)"
      ],
      "metadata": {
        "id": "0VIs7tUuIyOi"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "se = torch.tensor([convert_integer(r) for r in X_train])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "kD_LstE3JhCc",
        "outputId": "0644b24f-6b18-47b0-be3b-d4ca61e5989d"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "expected sequence of length 8 at dim 1 (got 14)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1553575226.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconvert_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: expected sequence of length 8 at dim 1 (got 14)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = customdata(X_train,Y_train.to_numpy())\n",
        "test_set = customdata(X_test,Y_test.to_numpy())"
      ],
      "metadata": {
        "id": "mTuxSnC7JKlO"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set.__getitem__(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Foq2TZlSLdBh",
        "outputId": "71b285c4-60f9-4c6c-f98d-04eeeca710ef"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([  18, 8756,  506,  518, 1548,  803,    0,   29,    1,  247, 8757,  224,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0]),\n",
              " tensor(2))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_load = DataLoader(train_set,batch_size=32,shuffle=True,pin_memory=True)\n",
        "test_load  = DataLoader(test_set,batch_size=32,shuffle=False,pin_memory=True)"
      ],
      "metadata": {
        "id": "gCLvC9tzLrT6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyRnn(nn.Module):\n",
        "  def __init__(self,hidden_dim,embbed_dim,vocab_size,cats):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embbedings =   nn.Embedding(vocab_size,embbed_dim)\n",
        "    self.rnn  =    nn.RNN(embbed_dim,hidden_dim,batch_first=True)\n",
        "    self.fc =   nn.Linear(hidden_dim,cats)\n",
        "  def forward(self,x):\n",
        "    emb = self.embbedings(x)\n",
        "    _,h = self.rnn(emb)\n",
        "    h_l = h.squeeze(0)\n",
        "    return self.fc(h_l)"
      ],
      "metadata": {
        "id": "KFNdx0gcMIEr"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyRnn(16,10,len(D),3)"
      ],
      "metadata": {
        "id": "vgulHapKOPVW"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y00d5GZ7Ok0x",
        "outputId": "f48a47c8-6c92-4413-ad02-3538e5e4aa5f"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "DlnoN7wdOnFK"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "UHtZTVKoOqsJ"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 500\n",
        "lr = 0.01"
      ],
      "metadata": {
        "id": "7q9k_AR_Ot8a"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fun = nn.CrossEntropyLoss()\n",
        "opt = torch.optim.Adagrad(model.parameters(),lr=0.03)"
      ],
      "metadata": {
        "id": "LXLnA3PrO-4_"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(epoch):\n",
        "  batch_loss =0\n",
        "  for batch,batch_label in train_load:\n",
        "    batch = batch.to(device)\n",
        "    batch_label = batch_label.to(device)\n",
        "    logits = model(batch)\n",
        "    loss = loss_fun(logits,batch_label)\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    batch_loss += loss.item()\n",
        "  print(f'Average batch loss in epoch{i} is {batch_loss/len(train_load)}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfgElcMpO5zV",
        "outputId": "8de71628-9f5c-4e1f-f744-b401a6c14eff"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average batch loss in epoch0 is 1.0497511063303266\n",
            "Average batch loss in epoch1 is 1.047859901700701\n",
            "Average batch loss in epoch2 is 1.0477984939302718\n",
            "Average batch loss in epoch3 is 1.0472133667128427\n",
            "Average batch loss in epoch4 is 1.047420915876116\n",
            "Average batch loss in epoch5 is 1.0467494320869446\n",
            "Average batch loss in epoch6 is 1.0472900322505405\n",
            "Average batch loss in epoch7 is 1.0470911724226815\n",
            "Average batch loss in epoch8 is 1.0472050632749286\n",
            "Average batch loss in epoch9 is 1.047230135372707\n",
            "Average batch loss in epoch10 is 1.0468078967503138\n",
            "Average batch loss in epoch11 is 1.0473878049850465\n",
            "Average batch loss in epoch12 is 1.0466180290494647\n",
            "Average batch loss in epoch13 is 1.0451447864941188\n",
            "Average batch loss in epoch14 is 1.0451415484292166\n",
            "Average batch loss in epoch15 is 1.0445384304864065\n",
            "Average batch loss in epoch16 is 1.0449730243001665\n",
            "Average batch loss in epoch17 is 1.03407566343035\n",
            "Average batch loss in epoch18 is 1.0229172430719649\n",
            "Average batch loss in epoch19 is 1.011986470903669\n",
            "Average batch loss in epoch20 is 0.995756230354309\n",
            "Average batch loss in epoch21 is 0.9785338766234262\n",
            "Average batch loss in epoch22 is 0.963303141934531\n",
            "Average batch loss in epoch23 is 0.9480666940552848\n",
            "Average batch loss in epoch24 is 0.9335834074020386\n",
            "Average batch loss in epoch25 is 0.9145813080242702\n",
            "Average batch loss in epoch26 is 0.9010325516973223\n",
            "Average batch loss in epoch27 is 0.8865294878823416\n",
            "Average batch loss in epoch28 is 0.8684309584753854\n",
            "Average batch loss in epoch29 is 0.8542707913262504\n",
            "Average batch loss in epoch30 is 0.8699450772149222\n",
            "Average batch loss in epoch31 is 0.8355773670332772\n",
            "Average batch loss in epoch32 is 0.8179106228692191\n",
            "Average batch loss in epoch33 is 0.8058453464508056\n",
            "Average batch loss in epoch34 is 0.7981193891593388\n",
            "Average batch loss in epoch35 is 0.7832189772810255\n",
            "Average batch loss in epoch36 is 0.7696883983271463\n",
            "Average batch loss in epoch37 is 0.7611039754322597\n",
            "Average batch loss in epoch38 is 0.7512060233524868\n",
            "Average batch loss in epoch39 is 0.7390937307902745\n",
            "Average batch loss in epoch40 is 0.7290741581576211\n",
            "Average batch loss in epoch41 is 0.7173323580196925\n",
            "Average batch loss in epoch42 is 0.7061896402495248\n",
            "Average batch loss in epoch43 is 0.6969990423747472\n",
            "Average batch loss in epoch44 is 0.6860395015989031\n",
            "Average batch loss in epoch45 is 0.6775401045594897\n",
            "Average batch loss in epoch46 is 0.664863475561142\n",
            "Average batch loss in epoch47 is 0.6583068714823042\n",
            "Average batch loss in epoch48 is 0.6487397171769823\n",
            "Average batch loss in epoch49 is 0.6374451049736568\n",
            "Average batch loss in epoch50 is 0.6278178972857339\n",
            "Average batch loss in epoch51 is 0.6193724226951599\n",
            "Average batch loss in epoch52 is 0.6128073714460646\n",
            "Average batch loss in epoch53 is 0.6044993419306619\n",
            "Average batch loss in epoch54 is 0.5935821006979262\n",
            "Average batch loss in epoch55 is 0.5854713548932756\n",
            "Average batch loss in epoch56 is 0.5785239926406316\n",
            "Average batch loss in epoch57 is 0.5693105936050415\n",
            "Average batch loss in epoch58 is 0.5638165737901415\n",
            "Average batch loss in epoch59 is 0.5557748906952994\n",
            "Average batch loss in epoch60 is 0.551016320841653\n",
            "Average batch loss in epoch61 is 0.5406061722551073\n",
            "Average batch loss in epoch62 is 0.5340233506475176\n",
            "Average batch loss in epoch63 is 0.5274591487646103\n",
            "Average batch loss in epoch64 is 0.5191472707475935\n",
            "Average batch loss in epoch65 is 0.53365570970944\n",
            "Average batch loss in epoch66 is 0.5262899366446904\n",
            "Average batch loss in epoch67 is 0.5078304136650903\n",
            "Average batch loss in epoch68 is 0.4996128763471331\n",
            "Average batch loss in epoch69 is 0.4940846658604486\n",
            "Average batch loss in epoch70 is 0.48794337817600797\n",
            "Average batch loss in epoch71 is 0.4828721610988889\n",
            "Average batch loss in epoch72 is 0.479176744222641\n",
            "Average batch loss in epoch73 is 0.47077390023640225\n",
            "Average batch loss in epoch74 is 0.4658378274100167\n",
            "Average batch loss in epoch75 is 0.4629739373070853\n",
            "Average batch loss in epoch76 is 0.45922884242875234\n",
            "Average batch loss in epoch77 is 0.4528865247113364\n",
            "Average batch loss in epoch78 is 0.4470197092635291\n",
            "Average batch loss in epoch79 is 0.4426846715382167\n",
            "Average batch loss in epoch80 is 0.43921188729149957\n",
            "Average batch loss in epoch81 is 0.4385284205845424\n",
            "Average batch loss in epoch82 is 0.4337576230083193\n",
            "Average batch loss in epoch83 is 0.4294117883699281\n",
            "Average batch loss in epoch84 is 0.4238815506867\n",
            "Average batch loss in epoch85 is 0.4177086403540203\n",
            "Average batch loss in epoch86 is 0.4161746392079762\n",
            "Average batch loss in epoch87 is 0.40871152639389036\n",
            "Average batch loss in epoch88 is 0.4065242013760975\n",
            "Average batch loss in epoch89 is 0.4027149667058672\n",
            "Average batch loss in epoch90 is 0.40053909616810934\n",
            "Average batch loss in epoch91 is 0.3976609471014568\n",
            "Average batch loss in epoch92 is 0.39478381727422984\n",
            "Average batch loss in epoch93 is 0.38848158419132234\n",
            "Average batch loss in epoch94 is 0.3890872905084065\n",
            "Average batch loss in epoch95 is 0.385870390193803\n",
            "Average batch loss in epoch96 is 0.3810606374059405\n",
            "Average batch loss in epoch97 is 0.37543339354651317\n",
            "Average batch loss in epoch98 is 0.37314532888787133\n",
            "Average batch loss in epoch99 is 0.3695359382459096\n",
            "Average batch loss in epoch100 is 0.36799263426235745\n",
            "Average batch loss in epoch101 is 0.36259313532284326\n",
            "Average batch loss in epoch102 is 0.35698207859482084\n",
            "Average batch loss in epoch103 is 0.3570759237664086\n",
            "Average batch loss in epoch104 is 0.35303954018013817\n",
            "Average batch loss in epoch105 is 0.3505862641334534\n",
            "Average batch loss in epoch106 is 0.34831349883760726\n",
            "Average batch loss in epoch107 is 0.34859472572803496\n",
            "Average batch loss in epoch108 is 0.3461792790038245\n",
            "Average batch loss in epoch109 is 0.33898659676313403\n",
            "Average batch loss in epoch110 is 0.33480595171451566\n",
            "Average batch loss in epoch111 is 0.3317121861236436\n",
            "Average batch loss in epoch112 is 0.3266596846495356\n",
            "Average batch loss in epoch113 is 0.324578403702804\n",
            "Average batch loss in epoch114 is 0.32328560918569565\n",
            "Average batch loss in epoch115 is 0.3189605823584965\n",
            "Average batch loss in epoch116 is 0.3179713330950056\n",
            "Average batch loss in epoch117 is 0.3164644967658179\n",
            "Average batch loss in epoch118 is 0.31346098107951026\n",
            "Average batch loss in epoch119 is 0.31086198908942086\n",
            "Average batch loss in epoch120 is 0.3073873224002974\n",
            "Average batch loss in epoch121 is 0.3028586074284145\n",
            "Average batch loss in epoch122 is 0.29904502608946393\n",
            "Average batch loss in epoch123 is 0.29544191692556654\n",
            "Average batch loss in epoch124 is 0.29248997649976183\n",
            "Average batch loss in epoch125 is 0.2892820219482694\n",
            "Average batch loss in epoch126 is 0.29821027691875185\n",
            "Average batch loss in epoch127 is 0.3016390501814229\n",
            "Average batch loss in epoch128 is 0.289222871533462\n",
            "Average batch loss in epoch129 is 0.29074261976139887\n",
            "Average batch loss in epoch130 is 0.2945089839611735\n",
            "Average batch loss in epoch131 is 0.28493213968617576\n",
            "Average batch loss in epoch132 is 0.2787460980245045\n",
            "Average batch loss in epoch133 is 0.277690797831331\n",
            "Average batch loss in epoch134 is 0.2759038908566747\n",
            "Average batch loss in epoch135 is 0.2766193344763347\n",
            "Average batch loss in epoch136 is 0.26756633232746807\n",
            "Average batch loss in epoch137 is 0.2647407871059009\n",
            "Average batch loss in epoch138 is 0.26369824298790523\n",
            "Average batch loss in epoch139 is 0.26131924446140015\n",
            "Average batch loss in epoch140 is 0.25842722445726396\n",
            "Average batch loss in epoch141 is 0.2622022217937878\n",
            "Average batch loss in epoch142 is 0.25936976385968075\n",
            "Average batch loss in epoch143 is 0.2547915400351797\n",
            "Average batch loss in epoch144 is 0.25240954739706856\n",
            "Average batch loss in epoch145 is 0.2506646240609033\n",
            "Average batch loss in epoch146 is 0.247439620750291\n",
            "Average batch loss in epoch147 is 0.24544095269271304\n",
            "Average batch loss in epoch148 is 0.24250785861696517\n",
            "Average batch loss in epoch149 is 0.2417988299046244\n",
            "Average batch loss in epoch150 is 0.24041545691234725\n",
            "Average batch loss in epoch151 is 0.24544249181236538\n",
            "Average batch loss in epoch152 is 0.24375182228428977\n",
            "Average batch loss in epoch153 is 0.2394300520100764\n",
            "Average batch loss in epoch154 is 0.2345770240681512\n",
            "Average batch loss in epoch155 is 0.23293245204857418\n",
            "Average batch loss in epoch156 is 0.24356033374156272\n",
            "Average batch loss in epoch157 is 0.2403529320870127\n",
            "Average batch loss in epoch158 is 0.23558418596429484\n",
            "Average batch loss in epoch159 is 0.2285512276632445\n",
            "Average batch loss in epoch160 is 0.22305617728403637\n",
            "Average batch loss in epoch161 is 0.2198539473755019\n",
            "Average batch loss in epoch162 is 0.21915743246674538\n",
            "Average batch loss in epoch163 is 0.21952910340258053\n",
            "Average batch loss in epoch164 is 0.21893593326210975\n",
            "Average batch loss in epoch165 is 0.21982425994106702\n",
            "Average batch loss in epoch166 is 0.2213408514218671\n",
            "Average batch loss in epoch167 is 0.22615231524620738\n",
            "Average batch loss in epoch168 is 0.2234462193293231\n",
            "Average batch loss in epoch169 is 0.22013756013342312\n",
            "Average batch loss in epoch170 is 0.21417350551911762\n",
            "Average batch loss in epoch171 is 0.20927788317203522\n",
            "Average batch loss in epoch172 is 0.20572602301836013\n",
            "Average batch loss in epoch173 is 0.20301950377013003\n",
            "Average batch loss in epoch174 is 0.20096427930252891\n",
            "Average batch loss in epoch175 is 0.19948077463677952\n",
            "Average batch loss in epoch176 is 0.1969408697102751\n",
            "Average batch loss in epoch177 is 0.19635877645441463\n",
            "Average batch loss in epoch178 is 0.3892156002989837\n",
            "Average batch loss in epoch179 is 0.28247887688023704\n",
            "Average batch loss in epoch180 is 0.24586447200604847\n",
            "Average batch loss in epoch181 is 0.22064010573284967\n",
            "Average batch loss in epoch182 is 0.20984058586614474\n",
            "Average batch loss in epoch183 is 0.21023013847214836\n",
            "Average batch loss in epoch184 is 0.202641935476235\n",
            "Average batch loss in epoch185 is 0.19821775561996868\n",
            "Average batch loss in epoch186 is 0.19528788116361415\n",
            "Average batch loss in epoch187 is 0.19180890519704138\n",
            "Average batch loss in epoch188 is 0.19048547299844878\n",
            "Average batch loss in epoch189 is 0.1900656943661826\n",
            "Average batch loss in epoch190 is 0.18753454336098263\n",
            "Average batch loss in epoch191 is 0.18543623028056963\n",
            "Average batch loss in epoch192 is 0.1862429126032761\n",
            "Average batch loss in epoch193 is 0.18792681785566467\n",
            "Average batch loss in epoch194 is 0.18453243345022202\n",
            "Average batch loss in epoch195 is 0.18127861993653432\n",
            "Average batch loss in epoch196 is 0.18067516138511044\n",
            "Average batch loss in epoch197 is 0.178456337377429\n",
            "Average batch loss in epoch198 is 0.17862722279770032\n",
            "Average batch loss in epoch199 is 0.17839964495173521\n",
            "Average batch loss in epoch200 is 0.17572583947862896\n",
            "Average batch loss in epoch201 is 0.1748955653182098\n",
            "Average batch loss in epoch202 is 0.173521433994174\n",
            "Average batch loss in epoch203 is 0.17255958148411343\n",
            "Average batch loss in epoch204 is 0.17077323925282273\n",
            "Average batch loss in epoch205 is 0.17118089555629662\n",
            "Average batch loss in epoch206 is 0.170997219607234\n",
            "Average batch loss in epoch207 is 0.17024497119443757\n",
            "Average batch loss in epoch208 is 0.169168236755899\n",
            "Average batch loss in epoch209 is 0.1697038634972913\n",
            "Average batch loss in epoch210 is 0.17189440711268356\n",
            "Average batch loss in epoch211 is 0.1712947403639555\n",
            "Average batch loss in epoch212 is 0.1666639870618071\n",
            "Average batch loss in epoch213 is 0.16594956967447486\n",
            "Average batch loss in epoch214 is 0.16299836984702518\n",
            "Average batch loss in epoch215 is 0.16158490635454656\n",
            "Average batch loss in epoch216 is 0.16168086361672196\n",
            "Average batch loss in epoch217 is 0.16418033489159176\n",
            "Average batch loss in epoch218 is 0.2267041853176696\n",
            "Average batch loss in epoch219 is 0.19164046924029077\n",
            "Average batch loss in epoch220 is 0.17967628308704922\n",
            "Average batch loss in epoch221 is 0.1717105738286461\n",
            "Average batch loss in epoch222 is 0.16711316052292075\n",
            "Average batch loss in epoch223 is 0.16233810194901058\n",
            "Average batch loss in epoch224 is 0.1601869944270168\n",
            "Average batch loss in epoch225 is 0.1595788404026202\n",
            "Average batch loss in epoch226 is 0.15765208567891803\n",
            "Average batch loss in epoch227 is 0.15594230083482605\n",
            "Average batch loss in epoch228 is 0.15415091549711568\n",
            "Average batch loss in epoch229 is 0.15396996045751232\n",
            "Average batch loss in epoch230 is 0.15274902729051454\n",
            "Average batch loss in epoch231 is 0.15662812648074967\n",
            "Average batch loss in epoch232 is 0.15753730375851904\n",
            "Average batch loss in epoch233 is 0.1603011057153344\n",
            "Average batch loss in epoch234 is 0.15920641725616796\n",
            "Average batch loss in epoch235 is 0.16850000131343093\n",
            "Average batch loss in epoch236 is 0.16841870217451027\n",
            "Average batch loss in epoch237 is 0.1852699122365032\n",
            "Average batch loss in epoch238 is 0.1671363074971097\n",
            "Average batch loss in epoch239 is 0.15796011604368687\n",
            "Average batch loss in epoch240 is 0.1552867548487016\n",
            "Average batch loss in epoch241 is 0.1520460108880486\n",
            "Average batch loss in epoch242 is 0.14963933568980012\n",
            "Average batch loss in epoch243 is 0.14745405635663442\n",
            "Average batch loss in epoch244 is 0.146679796608431\n",
            "Average batch loss in epoch245 is 0.14519468054175377\n",
            "Average batch loss in epoch246 is 0.14383581209927798\n",
            "Average batch loss in epoch247 is 0.14412930560963494\n",
            "Average batch loss in epoch248 is 0.14702376888266633\n",
            "Average batch loss in epoch249 is 0.1452961514783757\n",
            "Average batch loss in epoch250 is 0.1423862250255687\n",
            "Average batch loss in epoch251 is 0.14055237713669028\n",
            "Average batch loss in epoch252 is 0.1398657637089491\n",
            "Average batch loss in epoch253 is 0.13728623829782008\n",
            "Average batch loss in epoch254 is 0.13739595793187617\n",
            "Average batch loss in epoch255 is 0.13639660659645286\n",
            "Average batch loss in epoch256 is 0.1362035868849073\n",
            "Average batch loss in epoch257 is 0.13507323930838278\n",
            "Average batch loss in epoch258 is 0.13458194639001575\n",
            "Average batch loss in epoch259 is 0.1329962164589337\n",
            "Average batch loss in epoch260 is 0.13300952950758593\n",
            "Average batch loss in epoch261 is 0.13249606954199927\n",
            "Average batch loss in epoch262 is 0.13300757949905737\n",
            "Average batch loss in epoch263 is 0.13160163122628415\n",
            "Average batch loss in epoch264 is 0.17912479477269308\n",
            "Average batch loss in epoch265 is 0.17094209585871015\n",
            "Average batch loss in epoch266 is 0.16519467990313258\n",
            "Average batch loss in epoch267 is 0.15011943371168204\n",
            "Average batch loss in epoch268 is 0.14836672716907093\n",
            "Average batch loss in epoch269 is 0.1420252892321774\n",
            "Average batch loss in epoch270 is 0.13680887688483512\n",
            "Average batch loss in epoch271 is 0.13293013416230678\n",
            "Average batch loss in epoch272 is 0.13177313899355275\n",
            "Average batch loss in epoch273 is 0.1296952792576381\n",
            "Average batch loss in epoch274 is 0.12957590977528266\n",
            "Average batch loss in epoch275 is 0.12803624207420009\n",
            "Average batch loss in epoch276 is 0.12775867064084326\n",
            "Average batch loss in epoch277 is 0.12712248201348952\n",
            "Average batch loss in epoch278 is 0.12558194407394954\n",
            "Average batch loss in epoch279 is 0.1258663272006171\n",
            "Average batch loss in epoch280 is 0.1247550876598273\n",
            "Average batch loss in epoch281 is 0.1238909431121179\n",
            "Average batch loss in epoch282 is 0.12340676739279713\n",
            "Average batch loss in epoch283 is 0.12339503088167736\n",
            "Average batch loss in epoch284 is 0.12419583387672901\n",
            "Average batch loss in epoch285 is 0.12298103703984192\n",
            "Average batch loss in epoch286 is 0.12259723773492234\n",
            "Average batch loss in epoch287 is 0.12065177963780506\n",
            "Average batch loss in epoch288 is 0.12110440867287772\n",
            "Average batch loss in epoch289 is 0.12065975451043674\n",
            "Average batch loss in epoch290 is 0.12027752858187471\n",
            "Average batch loss in epoch291 is 0.12014821936509439\n",
            "Average batch loss in epoch292 is 0.11938826500305108\n",
            "Average batch loss in epoch293 is 0.11874930384968008\n",
            "Average batch loss in epoch294 is 0.11802228435873985\n",
            "Average batch loss in epoch295 is 0.11934374783188105\n",
            "Average batch loss in epoch296 is 0.12010285672332559\n",
            "Average batch loss in epoch297 is 0.1223825472540089\n",
            "Average batch loss in epoch298 is 0.12276973716914653\n",
            "Average batch loss in epoch299 is 0.12329015786094324\n",
            "Average batch loss in epoch300 is 0.11986875490418503\n",
            "Average batch loss in epoch301 is 0.12171348386577198\n",
            "Average batch loss in epoch302 is 0.12330692323190826\n",
            "Average batch loss in epoch303 is 0.12395154944487981\n",
            "Average batch loss in epoch304 is 0.12834195338189602\n",
            "Average batch loss in epoch305 is 0.12327086564685617\n",
            "Average batch loss in epoch306 is 0.12066139942301171\n",
            "Average batch loss in epoch307 is 0.11802763984139476\n",
            "Average batch loss in epoch308 is 0.11580104786370482\n",
            "Average batch loss in epoch309 is 0.13010898827442102\n",
            "Average batch loss in epoch310 is 0.12603115280824048\n",
            "Average batch loss in epoch311 is 0.121131186740739\n",
            "Average batch loss in epoch312 is 0.1188711045895304\n",
            "Average batch loss in epoch313 is 0.11695529603000199\n",
            "Average batch loss in epoch314 is 0.1149181379750371\n",
            "Average batch loss in epoch315 is 0.11432009450026921\n",
            "Average batch loss in epoch316 is 0.11413781337972198\n",
            "Average batch loss in epoch317 is 0.11362534715128797\n",
            "Average batch loss in epoch318 is 0.11313992945211274\n",
            "Average batch loss in epoch319 is 0.11235409118235111\n",
            "Average batch loss in epoch320 is 0.11171138849641596\n",
            "Average batch loss in epoch321 is 0.11344687294747148\n",
            "Average batch loss in epoch322 is 0.1119677215867809\n",
            "Average batch loss in epoch323 is 0.11177457986665623\n",
            "Average batch loss in epoch324 is 0.11243578747447049\n",
            "Average batch loss in epoch325 is 0.1112855730046119\n",
            "Average batch loss in epoch326 is 0.10983932315238884\n",
            "Average batch loss in epoch327 is 0.1101216662142958\n",
            "Average batch loss in epoch328 is 0.109550064557365\n",
            "Average batch loss in epoch329 is 0.11084842319201146\n",
            "Average batch loss in epoch330 is 0.11219472252364669\n",
            "Average batch loss in epoch331 is 0.12089258926255363\n",
            "Average batch loss in epoch332 is 0.1220680231122034\n",
            "Average batch loss in epoch333 is 0.12473823091281312\n",
            "Average batch loss in epoch334 is 0.12095077307628734\n",
            "Average batch loss in epoch335 is 0.12033896175878389\n",
            "Average batch loss in epoch336 is 0.1189189737662673\n",
            "Average batch loss in epoch337 is 0.11681811143245016\n",
            "Average batch loss in epoch338 is 0.1126053918206266\n",
            "Average batch loss in epoch339 is 0.11041989115732057\n",
            "Average batch loss in epoch340 is 0.10903762618878059\n",
            "Average batch loss in epoch341 is 0.10744300515523979\n",
            "Average batch loss in epoch342 is 0.10666411559496607\n",
            "Average batch loss in epoch343 is 0.10569958005100488\n",
            "Average batch loss in epoch344 is 0.1057645834716303\n",
            "Average batch loss in epoch345 is 0.10429799623255219\n",
            "Average batch loss in epoch346 is 0.10538406523742846\n",
            "Average batch loss in epoch347 is 0.10428071310477598\n",
            "Average batch loss in epoch348 is 0.10463906972536019\n",
            "Average batch loss in epoch349 is 0.11062225316252028\n",
            "Average batch loss in epoch350 is 0.12481813160436495\n",
            "Average batch loss in epoch351 is 0.13815241325646638\n",
            "Average batch loss in epoch352 is 0.14163354743804252\n",
            "Average batch loss in epoch353 is 0.12298171601125173\n",
            "Average batch loss in epoch354 is 0.11631923441376005\n",
            "Average batch loss in epoch355 is 0.11195234935198511\n",
            "Average batch loss in epoch356 is 0.10952993374850069\n",
            "Average batch loss in epoch357 is 0.10859427557992084\n",
            "Average batch loss in epoch358 is 0.1060138530869569\n",
            "Average batch loss in epoch359 is 0.1054450695855277\n",
            "Average batch loss in epoch360 is 0.10619287494570016\n",
            "Average batch loss in epoch361 is 0.10524738788604736\n",
            "Average batch loss in epoch362 is 0.10407244800989117\n",
            "Average batch loss in epoch363 is 0.10295876276812384\n",
            "Average batch loss in epoch364 is 0.10223886424941676\n",
            "Average batch loss in epoch365 is 0.10143085324870689\n",
            "Average batch loss in epoch366 is 0.10104869773345335\n",
            "Average batch loss in epoch367 is 0.10011079972343785\n",
            "Average batch loss in epoch368 is 0.10135043998914106\n",
            "Average batch loss in epoch369 is 0.0998508680877941\n",
            "Average batch loss in epoch370 is 0.09941309766577823\n",
            "Average batch loss in epoch371 is 0.09936313827655145\n",
            "Average batch loss in epoch372 is 0.09957770807934659\n",
            "Average batch loss in epoch373 is 0.09962665046964372\n",
            "Average batch loss in epoch374 is 0.0981204945913383\n",
            "Average batch loss in epoch375 is 0.09911518658910479\n",
            "Average batch loss in epoch376 is 0.09820768109389715\n",
            "Average batch loss in epoch377 is 0.09819561886468103\n",
            "Average batch loss in epoch378 is 0.09793663238840444\n",
            "Average batch loss in epoch379 is 0.09767406266182661\n",
            "Average batch loss in epoch380 is 0.0965248121161546\n",
            "Average batch loss in epoch381 is 0.09656860278121063\n",
            "Average batch loss in epoch382 is 0.09585941834641354\n",
            "Average batch loss in epoch383 is 0.09548673610602107\n",
            "Average batch loss in epoch384 is 0.09574449886700936\n",
            "Average batch loss in epoch385 is 0.0966590471885034\n",
            "Average batch loss in epoch386 is 0.09644342969038656\n",
            "Average batch loss in epoch387 is 0.0950134524969118\n",
            "Average batch loss in epoch388 is 0.09625819372811488\n",
            "Average batch loss in epoch389 is 0.09563071413763932\n",
            "Average batch loss in epoch390 is 0.09598043904240643\n",
            "Average batch loss in epoch391 is 0.09517404227384499\n",
            "Average batch loss in epoch392 is 0.09457389628248555\n",
            "Average batch loss in epoch393 is 0.0941540454540934\n",
            "Average batch loss in epoch394 is 0.09352301069136176\n",
            "Average batch loss in epoch395 is 0.09321464062003153\n",
            "Average batch loss in epoch396 is 0.09321873585028308\n",
            "Average batch loss in epoch397 is 0.09337200868342604\n",
            "Average batch loss in epoch398 is 0.09286446304592703\n",
            "Average batch loss in epoch399 is 0.09251784667372703\n",
            "Average batch loss in epoch400 is 0.09298461905813643\n",
            "Average batch loss in epoch401 is 0.09287622699780124\n",
            "Average batch loss in epoch402 is 0.0922771250616227\n",
            "Average batch loss in epoch403 is 0.09198232415531363\n",
            "Average batch loss in epoch404 is 0.09219520492745298\n",
            "Average batch loss in epoch405 is 0.09336643497326544\n",
            "Average batch loss in epoch406 is 0.09610236431338957\n",
            "Average batch loss in epoch407 is 0.09661720892680543\n",
            "Average batch loss in epoch408 is 0.09683153030595609\n",
            "Average batch loss in epoch409 is 0.12473915970219034\n",
            "Average batch loss in epoch410 is 0.22427219686763628\n",
            "Average batch loss in epoch411 is 0.20765084178319998\n",
            "Average batch loss in epoch412 is 0.16899757527879306\n",
            "Average batch loss in epoch413 is 0.1382283895409533\n",
            "Average batch loss in epoch414 is 0.12935170207704816\n",
            "Average batch loss in epoch415 is 0.14768244992409435\n",
            "Average batch loss in epoch416 is 0.11840077719518116\n",
            "Average batch loss in epoch417 is 0.11078879630193114\n",
            "Average batch loss in epoch418 is 0.10325780834470476\n",
            "Average batch loss in epoch419 is 0.0998419532978109\n",
            "Average batch loss in epoch420 is 0.0991369783585625\n",
            "Average batch loss in epoch421 is 0.09801577365824155\n",
            "Average batch loss in epoch422 is 0.0966265807779772\n",
            "Average batch loss in epoch423 is 0.09559589061353888\n",
            "Average batch loss in epoch424 is 0.09563606946063893\n",
            "Average batch loss in epoch425 is 0.09592597064163004\n",
            "Average batch loss in epoch426 is 0.0938690758762615\n",
            "Average batch loss in epoch427 is 0.09392026260495186\n",
            "Average batch loss in epoch428 is 0.09228337233353938\n",
            "Average batch loss in epoch429 is 0.09274920728589807\n",
            "Average batch loss in epoch430 is 0.09256751113172089\n",
            "Average batch loss in epoch431 is 0.09096878835665329\n",
            "Average batch loss in epoch432 is 0.09081120379003031\n",
            "Average batch loss in epoch433 is 0.09097561067768506\n",
            "Average batch loss in epoch434 is 0.08990313995097364\n",
            "Average batch loss in epoch435 is 0.09608463064368282\n",
            "Average batch loss in epoch436 is 0.0975465520897082\n",
            "Average batch loss in epoch437 is 0.09738474705389567\n",
            "Average batch loss in epoch438 is 0.09564520592934318\n",
            "Average batch loss in epoch439 is 0.09356180887403233\n",
            "Average batch loss in epoch440 is 0.09293027499956744\n",
            "Average batch loss in epoch441 is 0.09131376989451902\n",
            "Average batch loss in epoch442 is 0.09062699295048203\n",
            "Average batch loss in epoch443 is 0.09093166937253305\n",
            "Average batch loss in epoch444 is 0.09125003226633582\n",
            "Average batch loss in epoch445 is 0.08914657999628356\n",
            "Average batch loss in epoch446 is 0.08893405882907765\n",
            "Average batch loss in epoch447 is 0.0888115404333387\n",
            "Average batch loss in epoch448 is 0.09242082447345767\n",
            "Average batch loss in epoch449 is 0.0922933294730527\n",
            "Average batch loss in epoch450 is 0.09370783440502627\n",
            "Average batch loss in epoch451 is 0.09404107651008027\n",
            "Average batch loss in epoch452 is 0.09155093369739396\n",
            "Average batch loss in epoch453 is 0.09134935079408543\n",
            "Average batch loss in epoch454 is 0.08989900007311787\n",
            "Average batch loss in epoch455 is 0.08764010201873523\n",
            "Average batch loss in epoch456 is 0.0873426007692303\n",
            "Average batch loss in epoch457 is 0.08822674893108862\n",
            "Average batch loss in epoch458 is 0.0872632233319538\n",
            "Average batch loss in epoch459 is 0.08664269055905087\n",
            "Average batch loss in epoch460 is 0.08534226193491902\n",
            "Average batch loss in epoch461 is 0.10910309742870075\n",
            "Average batch loss in epoch462 is 0.09614645218742746\n",
            "Average batch loss in epoch463 is 0.09377398970403841\n",
            "Average batch loss in epoch464 is 0.09240353238369738\n",
            "Average batch loss in epoch465 is 0.08923692155629397\n",
            "Average batch loss in epoch466 is 0.08875051426036017\n",
            "Average batch loss in epoch467 is 0.08629162028697984\n",
            "Average batch loss in epoch468 is 0.08531799656472036\n",
            "Average batch loss in epoch469 is 0.08423383152378457\n",
            "Average batch loss in epoch470 is 0.08414191368967294\n",
            "Average batch loss in epoch471 is 0.08456438031579767\n",
            "Average batch loss in epoch472 is 0.08556490683129855\n",
            "Average batch loss in epoch473 is 0.08442074432703\n",
            "Average batch loss in epoch474 is 0.08304742465061801\n",
            "Average batch loss in epoch475 is 0.08250379685312509\n",
            "Average batch loss in epoch476 is 0.08216420218880688\n",
            "Average batch loss in epoch477 is 0.08165257795580796\n",
            "Average batch loss in epoch478 is 0.08165428682895644\n",
            "Average batch loss in epoch479 is 0.0825395749562553\n",
            "Average batch loss in epoch480 is 0.0839100048398333\n",
            "Average batch loss in epoch481 is 0.08526775703898498\n",
            "Average batch loss in epoch482 is 0.08229842067297016\n",
            "Average batch loss in epoch483 is 0.08156229208888752\n",
            "Average batch loss in epoch484 is 0.08053289530266608\n",
            "Average batch loss in epoch485 is 0.07999992552080325\n",
            "Average batch loss in epoch486 is 0.08051918179328953\n",
            "Average batch loss in epoch487 is 0.08043335847024406\n",
            "Average batch loss in epoch488 is 0.08102928407756346\n",
            "Average batch loss in epoch489 is 0.08048324483579823\n",
            "Average batch loss in epoch490 is 0.08390348228492907\n",
            "Average batch loss in epoch491 is 0.08212892571730274\n",
            "Average batch loss in epoch492 is 0.08158372605219483\n",
            "Average batch loss in epoch493 is 0.08001585592914905\n",
            "Average batch loss in epoch494 is 0.15712568264454604\n",
            "Average batch loss in epoch495 is 0.11753607010202749\n",
            "Average batch loss in epoch496 is 0.11123334109250989\n",
            "Average batch loss in epoch497 is 0.10099575477785298\n",
            "Average batch loss in epoch498 is 0.09743744189984033\n",
            "Average batch loss in epoch499 is 0.09903308330635939\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fc.weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ddKjY4Bownc",
        "outputId": "72ebe3b1-6afc-4e95-9285-fcb7385ed19a"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-2.8485e-01,  2.9835e-02,  4.4590e-01,  2.4673e-01, -1.8136e-01,\n",
              "         -2.4069e-01, -3.3705e-01, -2.6289e-01, -5.6413e-01, -1.8893e-01,\n",
              "          4.5167e-01,  2.1488e+00, -9.0250e-02, -2.4490e-01,  7.3689e-01,\n",
              "          1.6344e+00],\n",
              "        [ 9.3951e-05, -1.3747e-01, -9.2659e-01,  2.1089e-01, -5.6112e-01,\n",
              "          4.7368e-01, -4.8952e-01, -3.5842e-02, -4.7588e-01,  7.3461e-02,\n",
              "         -1.3295e+00, -2.4715e+00,  2.0336e-01, -2.3425e-01,  1.4892e+00,\n",
              "         -1.3769e+00],\n",
              "        [ 3.8911e-01,  7.2744e-02,  4.3962e-01, -5.4165e-01,  6.7231e-01,\n",
              "         -4.7414e-01,  5.1284e-01,  1.0875e-01,  8.4943e-01,  2.1064e-01,\n",
              "          5.3451e-01,  1.3609e-01, -8.0697e-03,  5.8111e-01, -2.4729e+00,\n",
              "         -4.5529e-01]], device='cuda:0', requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87iC9mwXk8nO",
        "outputId": "d621abd2-39ee-496b-96ce-35ba60218320"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyRnn(\n",
              "  (embbedings): Embedding(13141, 10)\n",
              "  (rnn): RNN(10, 16, batch_first=True)\n",
              "  (fc): Linear(in_features=16, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total = 0\n",
        "corr =0\n",
        "with torch.no_grad():\n",
        "  for batch,batch_label in test_load:\n",
        "    batch = batch.to(device)\n",
        "    batch_label = batch_label.to(device)\n",
        "    logits = model(batch)\n",
        "    preds = torch.argmax(logits,dim=1)\n",
        "    corr += (preds == batch_label).sum().item()\n",
        "    total += batch.shape[0]\n",
        "print(corr/total)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qo5tb800QTR9",
        "outputId": "20626fe8-117e-4894-995e-1e989eb9ee4d"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3335714285714286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = []\n",
        "with torch.no_grad():\n",
        "    for x, _ in test_load:\n",
        "      x = x.to(device)\n",
        "      preds.append(torch.argmax(model(x), dim=1))\n",
        "\n",
        "preds = torch.cat(preds)\n",
        "print(torch.bincount(preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4bvhr5YlrZF",
        "outputId": "15548eb5-252a-422d-9484-3cb7ac1904a8"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([546, 268, 586], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O7VkPirOpFSx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}